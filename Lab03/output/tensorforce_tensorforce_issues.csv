idtitlecreatedAtclosedAtclosed
MDU6SXNzdWUyMTU3MTY4MTI=API: allow update from external batch2017-03-21T12:10:51Z2017-03-21T13:28:22ZTrue
MDU6SXNzdWUyMTU3MjEyNDA=Optionally return model features2017-03-21T12:29:55Z2017-06-16T12:55:11ZTrue
MDU6SXNzdWUyMTY1MzgyMzI=Integrate internal states for LSTM policies2017-03-23T19:02:04Z2017-05-12T12:13:41ZTrue
MDU6SXNzdWUyMTY1Mzg2ODI=Generic distributed TF API2017-03-23T19:03:44Z2017-06-11T11:32:55ZTrue
MDU6SXNzdWUyMTY1Mzg3OTk=Docker container integration2017-03-23T19:04:08Z2017-07-03T08:05:23ZTrue
MDU6SXNzdWUyMTY3MDczMzc=Example runner for DeepMind lab2017-03-24T09:00:13Z2017-04-24T08:28:16ZTrue
MDU6SXNzdWUyMTY5NzA2Mjc=Write docs for distributed agent, explain different modes2017-03-25T12:19:55Z2017-06-16T12:51:15ZTrue
MDU6SXNzdWUyMTY5NzA2NTA=Add log level as config param2017-03-25T12:20:24Z2017-04-10T07:41:00ZTrue
MDU6SXNzdWUyMTc2MzY4NTM=Create default network configs for different environments2017-03-28T17:36:18Z2017-07-10T08:44:34ZTrue
MDU6SXNzdWUyMjE5MzcyMjY=Clean up dtype configuration2017-04-15T09:09:48Z2017-07-25T11:52:52ZTrue
MDU6SXNzdWUyMjIyMDQzNjE=Write unit test to verify every model runs until update2017-04-17T19:11:05Z2017-04-26T14:46:30ZTrue
MDU6SXNzdWUyMjM2Mjg5MjU=simple_q_agent example2017-04-23T11:17:01Z2017-04-26T07:44:36ZTrue
MDU6SXNzdWUyMjUyNDk5NTM=Default configuration locations2017-04-29T11:04:08Z2017-05-12T11:57:44ZTrue
MDU6SXNzdWUyMjgzMzE2NTM=Add check in replay memory to prevent batch_size > memory_size2017-05-12T15:43:48Z2017-05-29T09:11:20ZTrue
MDU6SXNzdWUyMzIwODM5MTM=Create new example + runner for universe2017-05-29T19:16:50Z2017-06-11T14:51:45ZTrue
MDU6SXNzdWUyMzY0NzQ5NjQ=update docs for 0.2 - new API and new parameter names2017-06-16T12:55:43Z2017-07-08T13:29:10ZTrue
MDU6SXNzdWUyMzY2NTE1MTQ=Adjust random agent to new state/action API2017-06-17T09:55:52Z2017-06-26T08:29:38ZTrue
MDU6SXNzdWUyNDI0NTUzMTk=Calling finalize() on the graph 2017-07-12T17:22:43Z2017-07-12T18:00:45ZTrue
MDU6SXNzdWUyNDI2Njk3OTQ=flake8: F821 undefined name in docs/m2r.py2017-07-13T11:27:38Z2017-07-13T11:43:15ZTrue
MDU6SXNzdWUyNDI3OTE1OTI=Test coverage for quick start example2017-07-13T18:33:54Z2017-07-15T10:35:02ZTrue
MDU6SXNzdWUyNDI4MjIzMjg=Investigate occasional NaN in TRPO2017-07-13T20:34:25Z2017-08-14T18:32:17ZTrue
MDU6SXNzdWUyNDI5MjU4Nzk=Implement generic natural policy gradient model2017-07-14T08:02:37Z2017-10-18T18:14:41ZTrue
MDU6SXNzdWUyNDMwNzY1NTU=Make Gaussian initial std configurable2017-07-14T18:33:47Z2017-07-15T13:31:05ZTrue
MDU6SXNzdWUyNDMxNTYzNzA=setup.py and tensorflow with/without gpu2017-07-15T06:15:10Z2017-07-15T13:04:12ZTrue
MDU6SXNzdWUyNDMxNzM5MTc=Logging consistency with external library.2017-07-15T12:59:09Z2017-07-29T09:19:38ZTrue
MDU6SXNzdWUyNDMyMjgxNDg=Check state type in act()2017-07-16T09:24:29Z2017-07-16T09:37:42ZTrue
MDU6SXNzdWUyNDM0NTYyNDE=Gaussian distribution parameters ignored2017-07-17T16:41:40Z2017-07-22T13:17:18ZTrue
MDU6SXNzdWUyNDM0NzI0OTY=result logging and policy saving2017-07-17T17:42:40Z2017-07-21T07:27:38ZTrue
MDU6SXNzdWUyNDM1MjUwNTA=Check TRPO with multiple (continuous) actions2017-07-17T20:56:23Z2017-07-25T20:09:23ZTrue
MDU6SXNzdWUyNDM1MzAwNTg=Allow custom distribution implementations2017-07-17T21:15:53Z2017-07-18T19:30:21ZTrue
MDU6SXNzdWUyNDM2NTk5NTE=Entropy regularization for policy gradient model2017-07-18T09:49:22Z2017-10-18T18:14:56ZTrue
MDU6SXNzdWUyNDM3OTc2MDA=Future plans and DDPG implementation2017-07-18T17:48:22Z2017-07-23T11:50:53ZTrue
MDU6SXNzdWUyNDM4NTk0NzM=Error running the example2017-07-18T21:38:17Z2017-07-19T06:50:42ZTrue
MDU6SXNzdWUyNDM5MzkwNDM=Always create tf.saver2017-07-19T06:50:20Z2017-07-19T07:14:50ZTrue
MDU6SXNzdWUyNDQ1MDYyMjI=Load and test a learned policy2017-07-20T21:47:50Z2017-07-21T16:16:28ZTrue
MDU6SXNzdWUyNDQ1ODI1MzI=(Example of) support for multi-valued Box actions?2017-07-21T07:07:13Z2017-07-25T20:08:46ZTrue
MDU6SXNzdWUyNDQ4MzAzNDk=Prioritized replay index out-of-range2017-07-22T07:20:58Z2017-07-22T13:03:11ZTrue
MDU6SXNzdWUyNDQ4MzM1MzM=ReplayMemory: add utility to set memory to array.2017-07-22T08:33:29Z2017-07-23T11:51:03ZTrue
MDU6SXNzdWUyNDUyMTA4MDk=Quick start example raises TypeError2017-07-24T21:12:20Z2017-07-24T22:02:16ZTrue
MDU6SXNzdWUyNDUyMTQ4OTY=Update quickstart docs 2017-07-24T21:29:47Z2017-07-24T21:38:05ZTrue
MDU6SXNzdWUyNDUyMjUyODc=TRPO struggling with CartPole-v0 from quick start2017-07-24T22:12:56Z2017-07-25T07:24:43ZTrue
MDU6SXNzdWUyNDUzMzYzODU=Issues with multiple continuous actions2017-07-25T09:18:41Z2017-07-25T20:08:49ZTrue
MDU6SXNzdWUyNDUzNjI0MzI=Incorrect number of columns computing lower triangular matrix in NAF agent2017-07-25T10:52:38Z2017-07-25T20:08:51ZTrue
MDU6SXNzdWUyNDUzNzU5OTM=Documentation for epsilon decay2017-07-25T11:51:41Z2017-07-26T09:45:57ZTrue
MDU6SXNzdWUyNDU1MTg5MzY=Configs should not change when passing to an agent2017-07-25T20:03:41Z2017-10-21T13:21:17ZTrue
MDU6SXNzdWUyNDU1MTkzODk=Min/max values for continuous actions2017-07-25T20:05:35Z2017-08-13T22:07:47ZTrue
MDU6SXNzdWUyNDU1NjkyNzQ=Exploration could be max until learning starts2017-07-26T00:04:35Z2017-07-29T17:29:41ZTrue
MDU6SXNzdWUyNDU1ODU2NzM=Cannot install2017-07-26T02:12:52Z2017-07-26T02:55:17ZTrue
MDU6SXNzdWUyNDU3MTIwMTY=Options of strategy about experience sampling at Replay.get_batch.2017-07-26T12:52:27Z2017-07-26T14:26:29ZTrue
MDU6SXNzdWUyNDU3MTI5ODk=Fix sampling instability in prioritised replay2017-07-26T12:56:18Z2017-07-26T15:57:58ZTrue
MDU6SXNzdWUyNDU5MDcwMDM=Dynamic discrete actions2017-07-27T02:19:15Z2017-07-27T12:41:16ZTrue
MDU6SXNzdWUyNDYwOTczNjU=TRPO does not work as the OpenAI baselines2017-07-27T16:26:54Z2017-08-14T18:24:17ZTrue
MDU6SXNzdWUyNDYxMTY1MDI=MLP baseline config2017-07-27T17:41:16Z2017-07-29T10:51:42ZTrue
MDU6SXNzdWUyNDYxMTc2MDQ=SavedModel Integration2017-07-27T17:45:38Z2019-09-08T14:03:03ZTrue
MDU6SXNzdWUyNDYxMTg0Njg=Configuration does not complain about extra parameters2017-07-27T17:48:51Z2017-07-29T16:26:29ZTrue
MDU6SXNzdWUyNDYxMTk1MDc=Handle next state semantics in case of random sampling2017-07-27T17:52:42Z2017-07-29T14:21:48ZTrue
MDU6SXNzdWUyNDYxMjgwNTY=Allow passing of a Model to an Agent2017-07-27T18:24:53Z2017-07-29T17:16:10ZTrue
MDU6SXNzdWUyNDYyNDMzNTI=Alternating act and observe2017-07-28T05:28:33Z2017-07-28T08:15:01ZTrue
MDU6SXNzdWUyNDY0Njc3MjY=Typo in constant.py2017-07-28T21:51:41Z2017-07-29T07:38:23ZTrue
MDU6SXNzdWUyNDY1MDI0MjU=A Distributional Perspective on Reinforcement Learning：code2017-07-29T06:06:15Z2017-07-30T09:49:17ZTrue
MDU6SXNzdWUyNDY1MTU3MzM=Baseline: Implement sampling over returns2017-07-29T11:15:42Z2017-07-30T09:48:58ZTrue
MDU6SXNzdWUyNDY3MDE4Mzk=tensorboard not showing values2017-07-31T09:47:41Z2017-08-12T07:56:17ZTrue
MDU6SXNzdWUyNDc5NzAyNTc=How to handle a warm start2017-08-04T10:46:51Z2017-08-08T09:37:10ZTrue
MDU6SXNzdWUyNDgyMDgwNDc=ValueError: Cannot feed value of shape (1,) for Tensor u'placeholder/state:0'2017-08-05T20:39:55Z2017-08-05T21:01:01ZTrue
MDU6SXNzdWUyNDk4NDU5OTA=QModel errors with LSTM2017-08-13T01:37:36Z2017-08-13T11:49:43ZTrue
MDU6SXNzdWUyNDk5MDQ5MDQ=Quickstart example doesn't work2017-08-13T23:02:03Z2017-08-14T08:59:26ZTrue
MDU6SXNzdWUyNTA0NjkxNTc=Multiple Continuous Actions2017-08-15T23:43:40Z2017-08-16T07:09:36ZTrue
MDU6SXNzdWUyNTEzMjg4NjU=Beta distribution: NaN issue in PPO2017-08-18T18:50:08Z2017-08-18T19:32:22ZTrue
MDU6SXNzdWUyNTEzMjkwMDM=TRPO runtime warning in line search2017-08-18T18:50:45Z2017-08-18T19:20:17ZTrue
MDU6SXNzdWUyNTE0MzA2MDg=Calculate correct KLdiv in TRPO (for diagnostics)2017-08-19T15:07:05Z2017-08-23T10:56:21ZTrue
MDU6SXNzdWUyNTE0MzEwNzA=Implement learning rate annealing2017-08-19T15:16:06Z2019-01-17T23:30:38ZTrue
MDU6SXNzdWUyNTIxNzA3OTI=KL divergence in PPOModel is always zero2017-08-23T06:12:35Z2017-08-23T10:56:04ZTrue
MDU6SXNzdWUyNTIyNDg0NzM=Incorrectness in surrogate loss2017-08-23T11:26:06Z2017-08-23T14:36:54ZTrue
MDU6SXNzdWUyNTI0Mjg0ODI=KLdiv in PPOModel2017-08-23T21:56:13Z2017-08-26T09:15:04ZTrue
MDU6SXNzdWUyNTMxMTcxOTM=layered_network_builder can't have multiple 'lstm'2017-08-26T19:20:18Z2017-08-29T09:48:13ZTrue
MDU6SXNzdWUyNTMxODIyODM=Baseline compatibility with multiple states/actions2017-08-27T18:58:59Z2017-09-02T10:50:41ZTrue
MDU6SXNzdWUyNTQ2MDYxOTY=Distributed NAF agent fails to initialize2017-09-01T10:54:13Z2017-09-02T10:52:39ZTrue
MDU6SXNzdWUyNTQ4MTA5NjE=Nan in Beta distribusion2017-09-02T13:13:12Z2017-09-02T13:26:47ZTrue
MDU6SXNzdWUyNTQ4OTcxNTU=Idea for baseline2017-09-03T20:02:15Z2017-10-21T12:31:52ZTrue
MDU6SXNzdWUyNTYwNjE0Nzk=Async w/ DQNAgent: 'NoneType' object has no attribute 'run' (self.session)2017-09-07T20:31:32Z2017-09-07T22:20:59ZTrue
MDU6SXNzdWUyNTY4NzEwMzY=Optimise Prioritised replay sampling 2017-09-11T23:25:45Z2017-10-22T14:35:56ZTrue
MDU6SXNzdWUyNTc2NTM4ODY=Additional tf summaries2017-09-14T09:31:37Z2017-09-14T11:39:43ZTrue
MDU6SXNzdWUyNTgwMDA5MDI=Is GPU being used at all?2017-09-15T10:50:32Z2017-09-15T10:56:49ZTrue
MDU6SXNzdWUyNTgxNzkyMjQ=LSTM and internal state handling potentially not correct2017-09-15T22:21:27Z2019-09-08T13:42:08ZTrue
MDU6SXNzdWUyNTg0NTEzMjI=Offline PPO?2017-09-18T11:45:22Z2019-09-08T13:47:21ZTrue
MDU6SXNzdWUyNTg5MDM0MjA=config.network is not JSON serializable2017-09-19T17:34:49Z2017-11-03T17:19:23ZTrue
MDU6SXNzdWUyNTg5MTEzMjk=Configuration refactoring - thoughts and suggestions welcome!2017-09-19T18:02:30Z2017-11-11T19:05:17ZTrue
MDU6SXNzdWUyNTg5MjYwOTA=config.tf_saver never actually used, still in docs2017-09-19T18:53:48Z2017-09-19T20:20:47ZTrue
MDU6SXNzdWUyNTg5MjY4MTY=clip_gradients no longer exists?2017-09-19T18:56:18Z2017-09-19T20:16:24ZTrue
MDU6SXNzdWUyNTk5NjgwMDU=Add optional batch normalization layer2017-09-23T00:30:07Z2017-09-25T16:38:35ZTrue
MDU6SXNzdWUyNjAwNjExMjk=Question about PolicyGradientModel.reward_estimation2017-09-24T06:06:41Z2019-09-08T13:47:09ZTrue
MDU6SXNzdWUyNjAxOTEyODU=Batch actions?2017-09-25T08:23:19Z2017-10-29T13:44:29ZTrue
MDU6SXNzdWUyNjAyMjQ2OTE=Replay.get_batch may get wrong next_states when sample the last observation2017-09-25T10:22:53Z2017-09-28T07:41:33ZTrue
MDU6SXNzdWUyNjA3NzE5MzE=Huber Loss calculation incorrect?2017-09-26T21:12:40Z2017-09-28T07:41:12ZTrue
MDU6SXNzdWUyNjEyMTg1MjU= multiple actions did not work!2017-09-28T07:27:56Z2017-09-28T07:36:02ZTrue
MDU6SXNzdWUyNjE2OTI1NzY=agent.load_model(path=path) NotFoundError2017-09-29T16:13:35Z2017-09-29T20:55:20ZTrue
MDU6SXNzdWUyNjE3NDIxNTI=agent.save_model issue!!2017-09-29T19:22:59Z2017-09-29T21:03:42ZTrue
MDU6SXNzdWUyNjE3NTExMDc=Easier Tensorboard Customization2017-09-29T19:45:36Z2017-11-07T22:27:22ZTrue
MDU6SXNzdWUyNjE4MjU2MDc=Update agent model export API to include global step2017-09-30T07:30:31Z2017-10-21T15:15:46ZTrue
MDU6SXNzdWUyNjIzODQ2MjE=Enable non-square kernels for conv2d in layered_network_builder2017-10-03T10:55:10Z2017-10-21T15:40:13ZTrue
MDU6SXNzdWUyNjI0MTE0NTQ=Vector as an action2017-10-03T12:39:13Z2017-10-03T15:19:11ZTrue
MDU6SXNzdWUyNjM2NDMzMzM=Can't use double_dqn=True w/ DQNNstepAgent2017-10-07T13:59:09Z2017-10-21T18:17:22ZTrue
MDU6SXNzdWUyNjM5Mjg5Mzg=Continuing training2017-10-09T15:08:04Z2017-10-09T21:16:17ZTrue
MDU6SXNzdWUyNjQwMzY1ODk=softmax activation in the last layer2017-10-09T21:48:46Z2017-10-09T23:06:51ZTrue
MDU6SXNzdWUyNjQzMjM4OTM=Test/Validation Runner2017-10-10T18:07:16Z2017-10-21T16:49:15ZTrue
MDU6SXNzdWUyNjQzNzc4NzY=Feature Request: Direct Future Prediction (DFP)2017-10-10T21:11:26Z2019-09-08T14:04:13ZTrue
MDU6SXNzdWUyNjQ3NDUyMTE=Gaussian log_stddev clipping - min > max; leads to constant log_stddev2017-10-11T21:40:21Z2017-10-11T21:44:24ZTrue
MDU6SXNzdWUyNjUxNzQzMjA=vpg_agent_visual baseline size error2017-10-13T05:32:18Z2017-10-22T14:35:24ZTrue
MDU6SXNzdWUyNjU0NjYzNDI=add acktr?2017-10-14T05:32:54Z2019-09-08T14:04:40ZTrue
MDU6SXNzdWUyNjU0Njk0NzI=Gym Discrete() observation_space rank error2017-10-14T06:39:40Z2017-10-23T07:35:04ZTrue
MDU6SXNzdWUyNjU2MDI5NTA=Code style editor configuration files2017-10-15T20:51:47Z2019-09-08T13:46:42ZTrue
MDU6SXNzdWUyNjY1ODg5MjI=Not possible to create custom TensorFlow network in #dev?2017-10-18T18:07:14Z2017-10-19T01:11:57ZTrue
MDU6SXNzdWUyNjY2MDcyODU=DQNAgent: AttributeError: 'Beta' object has no attribute 'state_action_value'2017-10-18T19:05:37Z2017-10-18T19:19:39ZTrue
MDU6SXNzdWUyNjY2OTAyMzI=Possible memory leak in DQNAgent2017-10-19T01:11:21Z2017-10-19T16:25:08ZTrue
MDU6SXNzdWUyNjcyNDE3ODU=Should entropy of different actions be reduced by sum?2017-10-20T17:06:19Z2017-10-22T06:27:02ZTrue
MDU6SXNzdWUyNjc0NDI4NDQ=DQN with prioritized experience replay2017-10-22T07:23:42Z2017-10-22T07:25:54ZTrue
MDU6SXNzdWUyNjc1NTI3MDc=Feature Request: NoisyNet for exploration2017-10-23T06:19:45Z2018-07-24T20:43:51ZTrue
MDU6SXNzdWUyNjc1NjgwMzE=Implement KFAC-optimizer2017-10-23T07:35:21Z2018-09-18T20:34:40ZTrue
MDU6SXNzdWUyNjc2NzA3MDU=Enforce alpha/beta > 1 in Beta distribution (ensure unimodal + concave)2017-10-23T13:36:26Z2017-10-23T14:08:30ZTrue
MDU6SXNzdWUyNjc5Mjg4NDA=Update categorical DQN to new API2017-10-24T07:42:30Z2019-09-08T14:04:56ZTrue
MDU6SXNzdWUyNjgwNjU5NDE=FEATURE REQUEST: Rainbow RL DeepMinds2017-10-24T14:57:21Z2019-09-08T14:05:20ZTrue
MDU6SXNzdWUyNjg0MDIwMjg=A3C status/example?2017-10-25T13:40:53Z2017-10-25T16:37:29ZTrue
MDU6SXNzdWUyNjg1NDQ2Mzc=Preprocessor Nomenclature Standardize & Normalize2017-10-25T20:49:59Z2017-10-29T13:43:54ZTrue
MDU6SXNzdWUyNjg2NTI4NDc=Prioritized Replay Memory Bugs2017-10-26T07:17:25Z2017-10-28T11:57:04ZTrue
MDU6SXNzdWUyNjg3OTkzNzU=exploration by NAF2017-10-26T15:09:47Z2017-10-26T15:18:24ZTrue
MDU6SXNzdWUyNjg5NjY2NDI=Double DNQ double_q_model =True removed2017-10-27T02:09:52Z2017-10-27T02:12:34ZTrue
MDU6SXNzdWUyNjkxMDQ2NTk=Tensorforce fresh installation2017-10-27T13:20:12Z2017-10-27T14:12:56ZTrue
MDU6SXNzdWUyNjkyMTM1MTM=Upgrade examples/threaded_ale.py to 0.3.02017-10-27T19:38:08Z2017-11-01T10:08:35ZTrue
MDU6SXNzdWUyNjkzMDg1MjY=error after update to 0.3.02017-10-28T10:56:14Z2017-11-07T15:06:44ZTrue
MDU6SXNzdWUyNjkzMTIwMTk=Memories with random instance order (e.g. prio replay) do not work properly2017-10-28T11:58:53Z2017-11-11T21:17:30ZTrue
MDU6SXNzdWUyNjkzNDUyODc="Documentation clarification on ""States"""2017-10-28T19:55:41Z2017-10-28T20:05:29ZTrue
MDU6SXNzdWUyNjkzNjQzOTU=Network Structure for multiple inputs2017-10-29T01:56:15Z2017-10-29T22:49:42ZTrue
MDU6SXNzdWUyNjk0NDgxODc=Document new example2017-10-29T23:33:15Z2017-10-30T09:11:18ZTrue
MDU6SXNzdWUyNjk5Mjg2MzQ=Explicit agent saving2017-10-31T11:44:34Z2017-11-07T15:14:08ZTrue
MDU6SXNzdWUyNjk5Mjk5MTM=Returning False from episode_finnished should stop training. Exception is raised but control is not returned to program2017-10-31T11:49:32Z2017-11-07T15:17:19ZTrue
MDU6SXNzdWUyNzA0NDc0MDk=Summary/Model Path settings fails at HEAD2017-11-01T20:48:50Z2017-11-07T15:02:18ZTrue
MDU6SXNzdWUyNzA0ODQzMDU=Possible bug with the DQFD agent (v0.3.0)2017-11-01T23:21:36Z2017-11-02T09:00:14ZTrue
MDU6SXNzdWUyNzA1NzQwODU=EWC integration?2017-11-02T08:46:34Z2018-07-24T20:44:03ZTrue
MDU6SXNzdWUyNzA2MDI5NDg=Create FAQ for new users2017-11-02T10:29:32Z2017-12-03T15:53:43ZTrue
MDU6SXNzdWUyNzA2OTI3NDA="Are all ""tensorforce/tests"" unittest modules supposed to pass?"2017-11-02T15:17:46Z2017-11-02T16:45:02ZTrue
MDU6SXNzdWUyNzExMDk3NzE="Example openai_universe.py  ""cannot import name create_agent"""2017-11-03T20:29:32Z2017-11-03T21:20:54ZTrue
MDU6SXNzdWUyNzExOTY0MDQ=Distributed issues2017-11-04T14:34:29Z2018-01-20T09:08:49ZTrue
MDU6SXNzdWUyNzE4ODMyMzU=Summaries are not written correctly when specified via summary_spec2017-11-07T15:54:32Z2017-11-07T22:21:19ZTrue
MDU6SXNzdWUyNzI2NDg1OTY=Add some examples/*.json w new optimizers (natural_gradient, evolutionary, etc)2017-11-09T17:19:43Z2019-09-08T13:46:09ZTrue
MDU6SXNzdWUyNzI3NTgzNDk=Possible issue with Distribution_model entropy_regularization2017-11-09T22:59:08Z2017-11-11T09:46:52ZTrue
MDU6SXNzdWUyNzMxMjEzNzI=Epsilon Annealing bug2017-11-11T05:35:21Z2017-11-11T10:01:59ZTrue
MDU6SXNzdWUyNzM1MjU5MjU=DataLossError and agent.restore_model2017-11-13T18:11:46Z2017-11-16T08:36:08ZTrue
MDU6SXNzdWUyNzM2MzgxMzk=FEATURE REQUEST: learning rate decay2017-11-14T01:35:38Z2017-11-18T18:23:18ZTrue
MDU6SXNzdWUyNzM3MDc0ODM=exploration in agent.py changes type of action2017-11-14T08:37:16Z2017-11-18T09:45:50ZTrue
MDU6SXNzdWUyNzQwMjMwMDA=Bug in model.restore function ?2017-11-15T03:29:23Z2017-11-16T08:35:02ZTrue
MDU6SXNzdWUyNzQzNDUwNTY=Restoring Saved Agent2017-11-15T23:43:51Z2017-11-16T08:35:20ZTrue
MDU6SXNzdWUyNzQzNTc1MzY=CNN Max Pooling2017-11-16T00:51:15Z2017-11-17T10:53:37ZTrue
MDU6SXNzdWUyNzQ5NjUwMDg=updated to 0.3.2 NAFAgent still does not work2017-11-17T19:05:08Z2017-12-13T21:10:27ZTrue
MDU6SXNzdWUyNzUxNDgyMzM=Invalid input rank for conv2d layer: 2, must be 42017-11-19T10:26:23Z2017-11-19T10:28:46ZTrue
MDU6SXNzdWUyNzUxOTE1ODk=Question about tensorboard loss calculation2017-11-19T20:54:51Z2017-11-20T09:22:39ZTrue
MDU6SXNzdWUyNzY0MDYyNjI=SelfPlay scenario2017-11-23T15:08:43Z2017-11-30T08:34:46ZTrue
MDU6SXNzdWUyNzcwMTAyMzQ=Minor bug in threaded_ale.py ?2017-11-27T13:00:11Z2017-12-03T10:45:04ZTrue
MDU6SXNzdWUyNzc1Njc3MjE=3.2 version no config.py file2017-11-28T22:54:54Z2017-11-29T00:55:27ZTrue
MDU6SXNzdWUyNzc4OTUwMDQ=Not sure how to use distributed training2017-11-29T20:26:11Z2017-11-30T15:45:54ZTrue
MDU6SXNzdWUyNzg4MzY4ODY=examples/openai_gym_async.py broken2017-12-04T00:06:33Z2017-12-08T20:45:22ZTrue
MDU6SXNzdWUyNzkxNjg1MzM=TypeError: conv1d() got an unexpected keyword argument 'input'2017-12-04T22:07:59Z2017-12-05T08:30:42ZTrue
MDU6SXNzdWUyNzk3MTA5MTA=No handlers could be found for logger error after running example code.2017-12-06T11:01:06Z2017-12-06T11:01:12ZTrue
MDU6SXNzdWUyNzk4MjQzMTA=Add example use multi-layer Lstm2017-12-06T16:45:06Z2017-12-06T17:52:46ZTrue
MDU6SXNzdWUyNzk5NjE4NDk=Any experience on performing multi agent learning?2017-12-07T00:57:35Z2017-12-07T08:32:25ZTrue
MDU6SXNzdWUyODAzMTgzNzQ=question: does tensorforce's network support combining two cnn networks?2017-12-07T23:38:33Z2018-01-29T14:22:57ZTrue
MDU6SXNzdWUyODA3NTI1OTA="TensorFlow 1.4.0 issue: ""At least two variables have the same name: ppo/.../Adam"""2017-12-09T20:34:06Z2018-01-16T20:32:05ZTrue
MDU6SXNzdWUyODA5MjE0MjU=Issue with running DQfD2017-12-11T07:42:28Z2017-12-11T08:37:12ZTrue
MDU6SXNzdWUyODEwMDE4Mjk=DQN not learning on Cartpole2017-12-11T12:38:42Z2017-12-11T15:04:10ZTrue
MDU6SXNzdWUyODE4NTY4Nzg=Which gym version do we need to run open_universe.py example ?2017-12-13T18:50:54Z2017-12-14T13:17:12ZTrue
MDU6SXNzdWUyODE5ODI1ODk=No importance sampling weights in prioritized replay buffer, will it be an issue?2017-12-14T05:02:44Z2017-12-15T14:39:23ZTrue
MDU6SXNzdWUyODIwMDI5Mzk=Explorations_spec is not recognized as the argument2017-12-14T07:14:59Z2017-12-14T19:35:43ZTrue
MDU6SXNzdWUyODIwNzk1MzQ=Surpport the value network and policy network share the same parameters in tensorforce?2017-12-14T12:10:58Z2017-12-15T02:31:27ZTrue
MDU6SXNzdWUyODI2NTg5Njg=question: does tensorforce allow you to process sequential data?2017-12-16T23:55:57Z2019-09-08T14:05:32ZTrue
MDU6SXNzdWUyODI2ODA1NzM=TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed in runner.py2017-12-17T09:16:03Z2018-02-02T22:02:47ZTrue
MDU6SXNzdWUyODMwOTg0MTQ=Request for summary lables2017-12-19T04:05:05Z2019-01-11T23:47:30ZTrue
MDU6SXNzdWUyODQwODU3ODU=Baseline regularization loss added to optimizer and baseline_optimizer?2017-12-22T06:50:35Z2017-12-22T07:06:10ZTrue
MDU6SXNzdWUyODQwOTAxNzY="What will be happen when I set baseline optimizer ""None"" but baseline model is not ""None""?"2017-12-22T07:16:58Z2017-12-24T05:19:26ZTrue
MDU6SXNzdWUyODQ0MDA1NDI=Support a PPO/TRPO example for continuous control problem2017-12-25T03:55:27Z2017-12-27T01:48:31ZTrue
MDU6SXNzdWUyODQ1ODY3NTk=Network.py Modification from_spec()2017-12-26T17:30:43Z2017-12-26T18:53:25ZTrue
MDU6SXNzdWUyODQ3NjUzNDY=Problems Saving Model with agent.save_model2017-12-27T17:34:21Z2017-12-28T17:52:33ZTrue
MDU6SXNzdWUyODQ5NzczMDA=Timeout for HalfCheetah-v1?2017-12-28T19:38:02Z2017-12-30T10:18:11ZTrue
MDU6SXNzdWUyODUxODE0Mzg=Utilizing GPU Capabilities2017-12-30T03:38:28Z2017-12-30T09:48:11ZTrue
MDU6SXNzdWUyODUyMTkwNjg=Activation for mlp2?2017-12-30T19:17:22Z2017-12-30T21:35:44ZTrue
MDU6SXNzdWUyODU0NDE4NTU=[Feature Request] Export Logits from network2018-01-02T14:26:25Z2018-02-02T22:04:08ZTrue
MDU6SXNzdWUyODYyNDI5OTI=Exploration out of bounds2018-01-05T10:02:23Z2018-01-05T10:57:26ZTrue
MDU6SXNzdWUyODY5MDY4MDI=Explorations config type casting error2018-01-08T22:18:42Z2018-01-08T22:27:29ZTrue
MDU6SXNzdWUyODg0MjQ3Njc=Segmentation fault (core dumped)2018-01-14T17:27:36Z2018-01-14T17:53:27ZTrue
MDU6SXNzdWUyODg3MTE5MzQ=Generative Adversarial Imitation Learning2018-01-15T20:45:59Z2019-09-08T14:08:28ZTrue
MDU6SXNzdWUyODk0ODA2OTA=OSError: dlopen: cannot load any more object with static TLS2018-01-18T02:14:30Z2018-01-22T08:47:38ZTrue
MDU6SXNzdWUyODk1MTA3OTE=Multi Step LSTM using PPO2018-01-18T05:40:32Z2019-09-08T13:45:49ZTrue
MDU6SXNzdWUyODk5MjQ3MDY=Exploration tied to action sampling for DQN agent2018-01-19T10:14:00Z2018-02-20T23:14:46ZTrue
MDU6SXNzdWUyOTAxMTg0NTE=Network Spec / Layers Documentation2018-01-19T21:53:40Z2019-09-08T13:45:28ZTrue
MDU6SXNzdWUyOTAyMzUzNDg=Intermittent missing index in prioritised reply2018-01-21T00:30:27Z2018-03-31T08:51:23ZTrue
MDU6SXNzdWUyOTAzMTY4NjU=AttributeError: 'NoneType' object has no attribute 'run'2018-01-21T23:33:48Z2018-02-02T22:03:50ZTrue
MDU6SXNzdWUyOTEwMzk5NDM=Possible bug: prob_ratio always 12018-01-24T00:12:55Z2018-02-02T22:20:01ZTrue
MDU6SXNzdWUyOTE2NzE5NzI=Save/load model manually2018-01-25T18:35:42Z2018-01-26T17:48:46ZTrue
MDU6SXNzdWUyOTE3MDIzNjI=[BUG] meta_parameter_recorder with Complex Network2018-01-25T20:19:20Z2019-01-17T23:30:09ZTrue
MDU6SXNzdWUyOTIxMTE4NDA=[FR] Add AlphaZero algorithm2018-01-27T11:00:13Z2018-02-02T21:52:34ZTrue
MDU6SXNzdWUyOTIxMTM3Mzc=How to use custom preprocessor in Agent?2018-01-27T11:34:03Z2018-01-27T14:43:55ZTrue
MDU6SXNzdWUyOTIxMzAzNjc=[BUG] The node 'ppo/..dropout..` has inputs from different frames. (memory branch)2018-01-27T15:50:04Z2018-08-03T22:23:14ZTrue
MDU6SXNzdWUyOTI1MTI5NTE=Fatal Python error: Segmentation fault2018-01-29T18:31:16Z2018-01-29T21:57:53ZTrue
MDU6SXNzdWUyOTI1NjAxODg=Multiple states 2018-01-29T21:05:55Z2018-01-30T22:09:11ZTrue
MDU6SXNzdWUyOTM0Njk2MDY=Fix model.py for distributed_spec runs2018-02-01T10:02:42Z2018-02-03T09:28:57ZTrue
MDU6SXNzdWUyOTM5NTIzMzI=Custom baseline network throws `object has no attribute 'internals_input'` (memory branch)2018-02-02T16:48:28Z2018-02-02T21:37:16ZTrue
MDU6SXNzdWUyOTQwMTY3NjI=Model for the Memory Agent2018-02-02T20:41:05Z2018-02-02T21:48:31ZTrue
MDU6SXNzdWUyOTQxNDgyMTI=Error trying to run a Runner twice2018-02-03T21:53:53Z2018-02-03T22:39:15ZTrue
MDU6SXNzdWUyOTQyODg3NTE=Setting Up DQN Agent2018-02-05T06:42:22Z2018-02-07T11:29:25ZTrue
MDU6SXNzdWUyOTQ1MzAxNjE=Large drop in performance when updating from 0.3.2 -> 0.3.5 (DQN,DDQN,PPO,TRPO)2018-02-05T20:23:54Z2018-03-31T09:06:54ZTrue
MDU6SXNzdWUyOTQ3OTcwOTA=ImportError: cannot import name 'Queue'2018-02-06T15:18:33Z2018-02-06T15:25:25ZTrue
MDU6SXNzdWUyOTUxNDA0ODI=Feature request: state dependent action space2018-02-07T13:38:23Z2019-09-08T14:06:02ZTrue
MDU6SXNzdWUyOTU2NjY2Njk=cannot import Configuration2018-02-08T21:10:49Z2018-02-08T21:14:48ZTrue
MDU6SXNzdWUyOTU4NTkzNjU=CI builds are breaking2018-02-09T12:46:08Z2018-02-24T08:46:53ZTrue
MDU6SXNzdWUyOTU4OTExMjI=tensorflow.python.framework.errors_impl.InvalidArgumentError: Must have updates.shape = indices.shape + params.shape[1:], got updates.shape [352], indices.shape [254], params.shape [100000]2018-02-09T14:36:21Z2018-02-25T09:20:27ZTrue
MDU6SXNzdWUyOTU5MzAwNzY=Maze Explorer in mode 0 TensorForceError: Invalid input rank for linear layer: 3, must be 2.2018-02-09T16:30:35Z2018-02-12T09:23:54ZTrue
MDU6SXNzdWUyOTc3NDU0MzA=Type error in ./tensorforce/environments/minimal_test.py2018-02-16T10:50:38Z2018-02-24T10:00:01ZTrue
MDU6SXNzdWUyOTg0NDA0Njg=PPO Agent with Multiple Action States2018-02-20T00:50:59Z2018-03-31T18:40:59ZTrue
MDU6SXNzdWUyOTg4MjM4MTc=Adjusting Entropy Regularization2018-02-21T02:45:15Z2018-07-24T20:39:16ZTrue
MDU6SXNzdWUyOTg5MTcwNjM=Document export/import API in faq2018-02-21T10:34:44Z2019-09-08T13:45:16ZTrue
MDU6SXNzdWUyOTg5NDY1NDY=Problem launching example openai_gym_async 2018-02-21T12:12:17Z2018-07-31T17:00:04ZTrue
MDU6SXNzdWUyOTk5MjAyNjE=Error in master branch: The node 'Merge/MergeSummary' has inputs from different frames.2018-02-24T07:26:54Z2018-07-31T16:58:28ZTrue
MDU6SXNzdWUzMDAwNzAzMDQ=__init__ got an unexpected argument 'states'2018-02-25T22:59:26Z2018-02-26T01:21:54ZTrue
MDU6SXNzdWUzMDAxMjA5MDg=A2C/A3C agent examples2018-02-26T06:12:25Z2018-03-03T10:20:01ZTrue
MDU6SXNzdWUzMDAxNjc1MjA=Dropped timesteps in replay memory lead to empty updates and crashes2018-02-26T09:40:25Z2018-03-05T08:38:54ZTrue
MDU6SXNzdWUzMDEwOTk5NTM=Pretraining networks for PPO2018-02-28T16:50:51Z2018-08-11T09:24:22ZTrue
MDU6SXNzdWUzMDE2MzYxNDA=How to preload weights for DQN agens2018-03-02T02:54:17Z2018-03-10T09:24:45ZTrue
MDU6SXNzdWUzMDI2MDIwNTY=Question about tf_actions_and_internals in DistributionModel2018-03-06T08:13:29Z2018-04-07T08:44:01ZTrue
MDU6SXNzdWUzMDMwMDEwOTI= TypeError: init() got an unexpected keyword argument 'cell_clip'2018-03-07T08:11:27Z2018-03-07T14:22:06ZTrue
MDU6SXNzdWUzMDMzNTQ2NTY=Tensorforce custom layer problem2018-03-08T04:43:43Z2018-03-27T14:53:04ZTrue
MDU6SXNzdWUzMDM3MjAzMjA=Some issues when using the latest version of tensorforce2018-03-09T04:25:25Z2018-03-14T07:14:19ZTrue
MDU6SXNzdWUzMDQwMjczNTI=Save and restore model components individually2018-03-10T01:15:58Z2018-03-17T08:56:58ZTrue
MDU6SXNzdWUzMDQxMTM4OTE=Problem while training multiple agents2018-03-10T23:10:35Z2018-03-11T00:00:25ZTrue
MDU6SXNzdWUzMDQxMTgyMDY=PPO training error with rewards other than 02018-03-11T00:24:53Z2018-03-27T14:53:25ZTrue
MDU6SXNzdWUzMDU2OTM4Mzk=Multiple agents with shared model in same environment2018-03-15T19:42:46Z2018-07-24T20:35:15ZTrue
MDU6SXNzdWUzMDU4Mzg4NTc=Agent setting for only taking action without model update2018-03-16T08:22:06Z2018-03-16T08:31:05ZTrue
MDU6SXNzdWUzMDYyNjA1NzI=Can't run quickstart example (OpenAI gym) when installing from pip2018-03-18T17:27:49Z2018-03-18T18:58:54ZTrue
MDU6SXNzdWUzMDYzNzU3MDc=How to set random seed2018-03-19T08:50:04Z2018-04-12T05:56:43ZTrue
MDU6SXNzdWUzMDY5MDI3MjM=Invalidad Argument Error : has inputs from different frames2018-03-20T14:57:58Z2018-03-27T14:52:30ZTrue
MDU6SXNzdWUzMDg0MjQ1NjY=TRPO throw exception2018-03-26T03:46:26Z2018-03-29T03:36:32ZTrue
MDU6SXNzdWUzMDg0NzkxOTE=unexpected keyword argument2018-03-26T08:38:16Z2018-03-26T17:09:17ZTrue
MDU6SXNzdWUzMDg1NTIxNjk=TypeError: while_loop() got an unexpected keyword argument 'maximum_iterations'2018-03-26T12:31:13Z2018-03-26T12:41:09ZTrue
MDU6SXNzdWUzMDg1Nzc5MzU=Maze_Runner Example not working?2018-03-26T13:43:26Z2018-04-16T21:07:14ZTrue
MDU6SXNzdWUzMTAyMTU3OTI=[BUG] ThreadedRunner is broken and its test is not covering this2018-03-31T06:02:01Z2019-09-08T13:45:05ZTrue
MDU6SXNzdWUzMTA1MDU5MzE=Bug: Model.start_server called twice in distributed mode2018-04-02T14:51:33Z2018-04-07T09:21:39ZTrue
MDU6SXNzdWUzMTE0Mjk3NTI=Large memory.capacity value causes tensorflow type issues2018-04-04T23:54:11Z2018-04-07T07:57:22ZTrue
MDU6SXNzdWUzMTE0NDEzNjg=Preprocessing operation like `divide` should be done after sampling experience2018-04-05T01:10:28Z2019-09-08T13:44:56ZTrue
MDU6SXNzdWUzMTE4NjQ0MDQ=InvalidArgumentError 2018-04-06T06:47:57Z2018-04-07T09:38:21ZTrue
MDU6SXNzdWUzMTIwMjQ4MjQ=Quickstart example get stuck [GPU]2018-04-06T15:43:55Z2018-07-28T19:40:33ZTrue
MDU6SXNzdWUzMTIzODQwODk=action space output to a single number from agent - Help!2018-04-09T03:52:50Z2018-07-31T17:04:28ZTrue
MDU6SXNzdWUzMTI0MDc4Njk=[BUG] run openai_gym_async.py throw exception2018-04-09T06:22:09Z2018-05-02T15:43:29ZTrue
MDU6SXNzdWUzMTI1ODM3OTI=summarizer error: loss-without-regularization as input to 'Merge/MergeSummary'2018-04-09T15:44:39Z2018-07-28T00:08:41ZTrue
MDU6SXNzdWUzMTI4NDEzODY=Episode reward summary in tensorboard for PPO agent2018-04-10T09:25:53Z2018-04-10T12:00:08ZTrue
MDU6SXNzdWUzMTM5NDY2NTg=TRPO failing after a few iterations of training2018-04-13T02:15:39Z2018-08-11T09:22:54ZTrue
MDU6SXNzdWUzMTQxODg5MzM=ddpg fails with multi-dimensional continuous input2018-04-13T17:24:50Z2018-09-01T15:21:02ZTrue
MDU6SXNzdWUzMTQxODkyNzI=Can you explain why the following DDPG code errors out.2018-04-13T17:25:58Z2018-04-13T19:02:59ZTrue
MDU6SXNzdWUzMTY1NTU2NjM=PPO Baseline with Complex Networks2018-04-22T07:17:27Z2018-05-14T18:18:45ZTrue
MDU6SXNzdWUzMTk2NzAyMDg=OpenAIGym Environment does not support Dict/nested spaces2018-05-02T18:50:19Z2018-05-04T07:45:47ZTrue
MDU6SXNzdWUzMjAyMDEwMDA=DQN not improving2018-05-04T09:04:47Z2018-05-04T15:04:39ZTrue
MDU6SXNzdWUzMjA0Mzg1MjY=Problem of the Example Code2018-05-04T21:40:44Z2018-05-05T09:23:03ZTrue
MDU6SXNzdWUzMjEyNDkyNjQ=TFLayer tf_apply not applicable to all predefined layers.2018-05-08T16:03:53Z2018-07-24T20:29:54ZTrue
MDU6SXNzdWUzMjE5MDU1ODQ= ImportError: cannot import name 'Configuration'2018-05-10T11:54:15Z2018-05-10T11:54:59ZTrue
MDU6SXNzdWUzMjI4MDAyOTU=How to set different actions for different states 2018-05-14T12:43:38Z2018-05-14T18:18:17ZTrue
MDU6SXNzdWUzMjMwNzI5NzI=A Pytorch Version?2018-05-15T05:38:08Z2018-07-24T16:40:32ZTrue
MDU6SXNzdWUzMjM1NzA3ODI=Summarizer is Broken2018-05-16T10:58:48Z2018-07-28T00:08:37ZTrue
MDU6SXNzdWUzMjQ3ODE3MTk=agent.restore_model Error2018-05-21T04:10:10Z2018-08-16T07:55:12ZTrue
MDU6SXNzdWUzMjQ4MDg1NTg=ValueError: low >= high2018-05-21T06:59:48Z2018-07-31T16:56:59ZTrue
MDU6SXNzdWUzMjU2ODcwOTE=weights are not being updated2018-05-23T12:40:44Z2018-05-29T11:29:21ZTrue
MDU6SXNzdWUzMjU3ODAzNjc=Restoring a model from checkpoint fails with InvalidArgumentError2018-05-23T16:21:20Z2018-05-23T18:25:14ZTrue
MDU6SXNzdWUzMjU5ODYwMTA=Not able to visualize graph. As well as showing error of optimizers.2018-05-24T06:50:51Z2018-05-25T11:09:45ZTrue
MDU6SXNzdWUzMjY1ODg3ODk=Model based RL2018-05-25T16:26:09Z2018-05-26T08:20:43ZTrue
MDU6SXNzdWUzMjY2NDAwNTg=example NAF2018-05-25T19:35:05Z2018-05-26T08:20:25ZTrue
MDU6SXNzdWUzMjcwNjk5OTA=Google TPU's Support2018-05-28T16:05:38Z2018-06-30T02:06:17ZTrue
MDU6SXNzdWUzMjcyOTU0NjM=Save network weights in a text file2018-05-29T12:18:45Z2018-06-13T21:01:15ZTrue
MDU6SXNzdWUzMjc4ODc3Njk=global_step issues when using the summarizer2018-05-30T20:31:14Z2018-05-30T20:34:53ZTrue
MDU6SXNzdWUzMjk2NzQxMTg=[openai_gym.py] test code in master?2018-06-06T00:56:24Z2018-06-07T07:38:57ZTrue
MDU6SXNzdWUzMzE2MzIyMjk=open_ai_gym_async error2018-06-12T15:11:18Z2018-06-13T19:03:51ZTrue
MDU6SXNzdWUzMzIyMjcyNjY=Memory capacity not aligned with episode length could result in InvalidArgumentError2018-06-14T02:16:23Z2018-06-18T21:24:26ZTrue
MDU6SXNzdWUzMzI2MjYxMDQ=A question about internal_lstm2018-06-15T02:26:33Z2018-07-24T16:53:13ZTrue
MDU6SXNzdWUzMzYxMDEyNzM=Use output of a custom network as action2018-06-27T07:03:30Z2018-07-29T18:16:15ZTrue
MDU6SXNzdWUzMzcwOTczODE=Using a custom environment with different discrete modes (progressive difficulty)2018-06-29T18:37:02Z2018-07-03T20:02:50ZTrue
MDU6SXNzdWUzMzcyNjM3MjQ=Environment Documentation Not Consistent for Execute(actions)2018-07-01T04:57:51Z2018-07-15T19:44:12ZTrue
MDU6SXNzdWUzMzczMDUwNzc=openai_gym_async failed to start2018-07-01T16:54:09Z2018-07-06T02:21:17ZTrue
MDU6SXNzdWUzMzczMDU4OTM=type-conflict int (=state) vs. float(weights)2018-07-01T17:07:07Z2018-07-01T22:28:57ZTrue
MDU6SXNzdWUzMzgwNjU0Mzk=OpenAI/Universe environment deprecated2018-07-03T21:48:56Z2018-07-25T19:22:23ZTrue
MDU6SXNzdWUzMzg3ODM4Nzc=Distribution Mode save/restore2018-07-06T02:26:29Z2018-07-31T16:56:37ZTrue
MDU6SXNzdWUzMzg4OTA5ODY=distributed mode with saver failed.2018-07-06T10:37:34Z2018-08-16T10:17:21ZTrue
MDU6SXNzdWUzMzk2NTg5MTA=Nested spaces.Dicts have no shape2018-07-10T01:14:06Z2018-07-24T16:37:29ZTrue
MDU6SXNzdWUzNDA3ODUyNTM=Environment for driving robot swarms2018-07-12T20:25:13Z2018-07-24T15:34:53ZTrue
MDU6SXNzdWUzNDIyNTA0NDQ=Inconsistent action/actions argument name (Runner / Environment.execute)2018-07-18T09:36:12Z2018-07-20T09:30:11ZTrue
MDU6SXNzdWUzNDIzMzgzMjA=What's the proper way to have multiple states in nested dicts?2018-07-18T13:52:43Z2018-07-25T18:42:38ZTrue
MDU6SXNzdWUzNDMyNTE5NDU=[silent BUG] Saving/Restoring/Seeding PPO model when action_spec has multiple actions2018-07-20T21:25:08Z2018-07-27T10:37:38ZTrue
MDU6SXNzdWUzNDMzOTM5NzU=FAQ not up to date 2018-07-22T10:11:52Z2018-07-24T15:24:59ZTrue
MDU6SXNzdWUzNDM3NzkyNDg=Unusable checkpoints2018-07-23T20:15:23Z2018-07-25T01:43:31ZTrue
MDU6SXNzdWUzNDU4OTE1MTY=examples/openai_gym_async.py issue 2018-07-30T19:01:06Z2018-07-31T14:48:12ZTrue
MDU6SXNzdWUzNDU5NDAzODg=Fully convolutional policies?2018-07-30T21:22:45Z2018-07-31T16:07:44ZTrue
MDU6SXNzdWUzNDU5ODE4MzQ=topological sort failed with message: Non-existent input ^ConstantFoldingCtrl/ppo/cond/add/Switch_0 for node ConstantFolding/ppo/cond/strided_slice_1/stack_const_axis2018-07-30T23:53:51Z2018-07-31T18:52:59ZTrue
MDU6SXNzdWUzNDYxNDk3NDM=learning from random agent recorded sessions2018-07-31T11:38:34Z2019-09-08T13:44:43ZTrue
MDU6SXNzdWUzNDYyNTUxNjc=examples/openai_gym_async.py gets stuck2018-07-31T16:04:25Z2019-09-08T13:44:10ZTrue
MDU6SXNzdWUzNDYzMTY4ODA=Failed to run optimizer ArithmeticOptimizer, stage HoistCommonFactor2018-07-31T18:59:09Z2018-08-14T20:48:17ZTrue
MDU6SXNzdWUzNDY5MzIxNjQ=Can you provide an example for the environment of Unreal Engine 4 Games?2018-08-02T09:30:37Z2019-09-08T13:44:01ZTrue
MDU6SXNzdWUzNDcwMDM1MTM=DQN agent is broken?2018-08-02T13:06:20Z2018-08-03T20:08:53ZTrue
MDU6SXNzdWUzNDcyMzc2NTY=Confusion about ComplexNetwork2018-08-03T02:32:21Z2018-08-03T15:39:12ZTrue
MDU6SXNzdWUzNDcyNzgzNjI=UnrealEngine Example/Environment Problems2018-08-03T06:45:26Z2018-08-14T22:05:10ZTrue
MDU6SXNzdWUzNDc1NzkwNTQ=Incompatible type conversion requested to type 'int64' for variable of type 'int32_ref'2018-08-04T02:38:17Z2018-08-04T03:49:15ZTrue
MDU6SXNzdWUzNDc1ODM3MDA=ResourceExhaustedError on CNN networks.2018-08-04T03:48:17Z2018-08-06T14:33:57ZTrue
MDU6SXNzdWUzNDkzNjA1MzM=Please add support for a new simulator `CARLA`2018-08-10T03:08:06Z2019-09-08T13:43:46ZTrue
MDU6SXNzdWUzNDk3MDM5Nzk=Adding a summarizer results in a NotFoundError2018-08-11T04:09:39Z2019-01-11T23:46:06ZTrue
MDU6SXNzdWUzNTA0MjMyMTk=Deploying a model in practice, deterministically2018-08-14T13:15:53Z2018-08-14T13:38:08ZTrue
MDU6SXNzdWUzNTMyNDcyNTk=training in parallel with several threads2018-08-23T06:52:23Z2019-01-17T23:29:49ZTrue
MDU6SXNzdWUzNTMzODQ4NTc=visualizing the PPO probability distribution2018-08-23T13:37:25Z2018-11-01T14:28:28ZTrue
MDU6SXNzdWUzNTM5MjI3NTg=Export graph for Unity2018-08-24T20:49:17Z2018-08-24T21:25:02ZTrue
MDU6SXNzdWUzNTQwMTMzNTg=Implementing reward shaping in TensorForce 2018-08-25T13:43:23Z2018-08-28T12:25:44ZTrue
MDU6SXNzdWUzNTQxMDg1Mjc=Documentation omitting bullet points2018-08-26T16:36:02Z2019-09-08T13:42:50ZTrue
MDU6SXNzdWUzNTQyMTIxMDk=Understanding the effect of the PPO metaparameters2018-08-27T07:34:41Z2019-09-08T13:42:30ZTrue
MDU6SXNzdWUzNTQ4ODExOTg=Infinite loop in unflatten_action2018-08-28T19:59:18Z2018-08-31T09:28:48ZTrue
MDU6SXNzdWUzNTQ5NzA2MzU=run error2018-08-29T02:25:02Z2018-08-29T07:36:19ZTrue
MDU6SXNzdWUzNTU3Njc5NzI='Custom' network architecture2018-08-30T21:53:31Z2018-08-31T07:36:22ZTrue
MDU6SXNzdWUzNTU5NjE3NDM=How to define multiple actions where some are available simultaneously ?2018-08-31T12:31:36Z2018-08-31T12:41:51ZTrue
MDU6SXNzdWUzNTc4MzUzNzI=Making a Layer Non-Trainable [Implemented]2018-09-06T21:31:47Z2018-09-13T22:21:30ZTrue
MDU6SXNzdWUzNTgxODk0NjM=Exception on init agent with states preprocessing2018-09-07T19:44:36Z2018-09-07T19:59:00ZTrue
MDU6SXNzdWUzNTgxOTIwMzU=PPO agent throws exception when using exploration2018-09-07T19:54:27Z2018-09-07T20:37:40ZTrue
MDU6SXNzdWUzNTg2NjU3MDg=Multi-threaded execution example2018-09-10T15:05:05Z2018-09-13T10:47:47ZTrue
MDU6SXNzdWUzNTg4MDQ5Mjk='RandomModel' object has no attribute 'action_spec'2018-09-10T21:39:46Z2018-09-11T08:16:51ZTrue
MDU6SXNzdWUzNTkyMzE1NTc=Parallel reward computation for one agent with update_mode['batch_size'] > 12018-09-11T21:30:09Z2018-09-24T14:06:04ZTrue
MDU6SXNzdWUzNTk1ODIyNDc=Create pb file2018-09-12T17:35:37Z2019-09-08T14:08:02ZTrue
MDU6SXNzdWUzNTk1OTQ5NDM=load .h5 model file for training2018-09-12T18:13:48Z2019-09-08T14:07:40ZTrue
MDU6SXNzdWUzNTk2MzIwMDk=Update on batch of single reward episodes2018-09-12T20:05:33Z2018-09-22T12:16:28ZTrue
MDU6SXNzdWUzNjAwOTc4NDQ=Implementing DeepMind Pycolab environment2018-09-13T23:05:15Z2018-09-19T22:41:00ZTrue
MDU6SXNzdWUzNjA1ODUwMzA=AttributeError: 'Linear' object has no attribute 'trainable'2018-09-15T23:27:11Z2018-09-16T19:45:41ZTrue
MDU6SXNzdWUzNjA2NDcyNjM=TypeError: constant() got an unexpected keyword argument 'dims'2018-09-16T15:00:16Z2018-09-16T19:54:58ZTrue
MDU6SXNzdWUzNjA2NTY5OTU=Transfer + Fine Tuning2018-09-16T16:52:09Z2020-02-11T17:51:40ZTrue
MDU6SXNzdWUzNjI1MDg4NzY=Problem with restore_model2018-09-21T08:28:59Z2018-09-21T09:01:19ZTrue
MDU6SXNzdWUzNjI4NDY2OTU=Problem with Memory2018-09-22T10:43:08Z2018-09-22T11:45:01ZTrue
MDU6SXNzdWUzNjI5NTcyNjg=Possible bug running DDPG2018-09-23T16:40:49Z2018-09-23T19:27:35ZTrue
MDU6SXNzdWUzNjI5ODQwNzc=Custom critic network support for DDPG2018-09-23T22:41:23Z2018-09-26T15:05:48ZTrue
MDU6SXNzdWUzNjQ5Mzg1NTA=Stateful for LSTM2018-09-28T15:30:56Z2018-10-12T17:08:45ZTrue
MDU6SXNzdWUzNjQ5OTQyMjA=Correct way of adding a new gym environment to openai_gym.py2018-09-28T18:26:12Z2018-09-28T23:23:16ZTrue
MDU6SXNzdWUzNjY4MDM2MTA=Testing a trained model2018-10-04T13:57:27Z2019-09-08T14:02:11ZTrue
MDU6SXNzdWUzNjc1ODE4NTU=Feed a value for placeholder tensor 'ppo/initialize/episode_index'2018-10-07T18:47:20Z2018-10-09T21:29:49ZTrue
MDU6SXNzdWUzNjc3OTQxNDI=Option: --load Episode_index problem2018-10-08T13:36:44Z2018-10-09T21:09:56ZTrue
MDU6SXNzdWUzNjkzMTkyMDM=DDPG - very rare convergence2018-10-11T21:51:53Z2019-09-08T14:01:46ZTrue
MDU6SXNzdWUzNzIzNDE5NDA=Custom Network2018-10-21T18:18:51Z2018-10-22T19:57:30ZTrue
MDU6SXNzdWUzNzMyMDQ1Njg=Network with Loops2018-10-23T21:14:16Z2018-10-26T21:07:13ZTrue
MDU6SXNzdWUzNzM4MjU4NDQ=agent parameters schedule2018-10-25T08:17:08Z2018-11-01T14:36:24ZTrue
MDU6SXNzdWUzODE3MDA4Nzk=Is the DeepMind Lab API Deprecated2018-11-16T17:54:33Z2019-09-08T14:00:27ZTrue
MDU6SXNzdWUzODIwODc1Mjk=examples/openai_gym can not work with dqn2018-11-19T07:25:18Z2018-11-19T09:10:45ZTrue
MDU6SXNzdWUzODI5MTI3MDg=Can't run quickstart example after installing Tensorforce with pip2018-11-21T01:06:59Z2019-09-08T13:59:43ZTrue
MDU6SXNzdWUzODkzMDU4MjM=Terminal state update error2018-12-10T13:40:03Z2018-12-12T00:12:05ZTrue
MDU6SXNzdWUzOTI2NTEzMDI=Reward Network2018-12-19T15:25:22Z2019-09-08T13:58:50ZTrue
MDU6SXNzdWUzOTMxMDYzNTQ=PPOAgent hyperparameters 2018-12-20T15:56:20Z2019-09-08T13:58:27ZTrue
MDU6SXNzdWUzOTQ2MzIxMTI=Use myself agent and environment2018-12-28T13:00:49Z2018-12-29T09:38:03ZTrue
MDU6SXNzdWUzOTQ2NTAxMzA=Summaries can't view in tensorboard2018-12-28T14:35:57Z2019-01-11T23:45:28ZTrue
MDU6SXNzdWUzOTU3Mjk5NTc="LSTM using PPO gives ""Cannot use '.../Identity' as input to '.../Identity_4' because they are in different while loops"""2019-01-03T21:26:05Z2019-02-06T17:24:00ZTrue
MDU6SXNzdWUzOTYwNzgyMTQ=PPO agent throws Retval[0] does not have value2019-01-04T21:45:30Z2019-01-20T23:23:13ZTrue
MDU6SXNzdWUzOTY0NDIzNzk=example A3C-Gym/train-atari errors2019-01-07T11:01:46Z2019-09-08T13:58:14ZTrue
MDU6SXNzdWUzOTY4OTI2NDE="File ""tensorforce/tensorforce/models/model.py"", line 483, in setup_placeholders     kwargs=dict(shape=self.states_spec[name]['shape'])) UnboundLocalError: local variable 'name' referenced before assignment"2019-01-08T12:49:39Z2019-01-08T12:50:00ZTrue
MDU6SXNzdWUzOTgwNzgwODI=Cannot make OpenAIGym environments from gym.Env objects2019-01-11T00:32:25Z2019-11-17T19:09:27ZTrue
MDU6SXNzdWU0MDA0MjMzNzQ=NAF example does not work -->  tensorforce/tensorforce/tests/test_naf_agent.py2019-01-17T19:15:17Z2019-09-08T13:57:41ZTrue
MDU6SXNzdWU0MDA2NzE1NDc=NAF unconstrained action space2019-01-18T10:48:52Z2019-09-08T13:57:28ZTrue
MDU6SXNzdWU0MDExMTMyODQ=How to write a custom Environment for tensorforce?  Cant find examples. Please provide one2019-01-20T16:20:19Z2019-01-20T17:59:09ZTrue
MDU6SXNzdWU0MDE0MTY1NDc=major-revision branch restore_model issue2019-01-21T16:00:59Z2019-01-21T23:10:54ZTrue
MDU6SXNzdWU0MDE2NDc3ODc=example use remote environment2019-01-22T08:24:08Z2019-05-06T07:47:54ZTrue
MDU6SXNzdWU0MDE2NjY3MjQ=ppoagent summarizer results in error2019-01-22T09:17:17Z2019-01-24T09:19:57ZTrue
MDU6SXNzdWU0MDE4MzgxMjI=Maze Explorer example doesn't work2019-01-22T16:09:34Z2019-01-23T12:34:01ZTrue
MDU6SXNzdWU0MDE5ODg1Mjk=how saving model is done2019-01-22T22:47:03Z2019-01-23T11:20:39ZTrue
MDU6SXNzdWU0MDIzMTU5MzM=major-revision branch - Two small issues with Runner2019-01-23T16:14:15Z2019-11-17T19:09:01ZTrue
MDU6SXNzdWU0MDI3OTA5MDQ=allow deterministic evaluation during training to benchmark performance2019-01-24T16:30:43Z2019-09-08T13:56:34ZTrue
MDU6SXNzdWU0MDI3OTIwNDc=saver: save both model and best model so far2019-01-24T16:33:10Z2019-01-26T13:38:06ZTrue
MDU6SXNzdWU0MDI4NDk4ODA=major-revision branch summarizer error with dqn (possibly others)2019-01-24T18:58:35Z2019-01-26T13:37:42ZTrue
MDU6SXNzdWU0MDM3MzM4Mzk=tensorforce.exception.TensorforceError: Invalid output arguments for tf_apply.2019-01-28T10:04:51Z2019-01-28T21:34:00ZTrue
MDU6SXNzdWU0MDM3NTAzMTY=callbacks as list of functions2019-01-28T10:42:25Z2019-01-28T21:34:37ZTrue
MDU6SXNzdWU0MDQ2OTIwNDM=InvalidArgumentError in combination with PPO + gru + l2_regularization2019-01-30T10:03:55Z2019-02-06T17:23:52ZTrue
MDU6SXNzdWU0MDU3OTk1NTY=allow training the network with also state prediciton goal2019-02-01T17:29:21Z2019-09-08T13:55:41ZTrue
MDU6SXNzdWU0MDU4MDM5NzE=list of projects / repos / blogs / publications that use tensorforce2019-02-01T17:42:18Z2019-09-08T13:55:16ZTrue
MDU6SXNzdWU0MDYyMDYzOTA=Game 2048 isn't runnable on major-revision branch2019-02-04T07:14:23Z2019-02-08T23:54:05ZTrue
MDU6SXNzdWU0MDc3MTU2Nzg=[Question] model/parameters optimization?2019-02-07T14:04:43Z2019-02-08T13:51:31ZTrue
MDU6SXNzdWU0MDg3MjE1NTI=l2_regularization and ppo (with tf 1.13rc0 and major-revision branch)2019-02-11T10:43:28Z2019-02-12T08:06:51ZTrue
MDU6SXNzdWU0MTQ1ODM1NzY=Some questions about tensorforce2019-02-26T12:19:26Z2019-03-02T02:46:27ZTrue
MDU6SXNzdWU0MTU0NzM1MTI=models/model.py create_atomic_observe_operations() is using buffers?2019-02-28T06:51:03Z2019-09-08T13:55:03ZTrue
MDU6SXNzdWU0MTY2MDM3OTE=negative loss when training PPOAgent2019-03-04T02:34:28Z2019-03-05T12:11:11ZTrue
MDU6SXNzdWU0MTY3NTEyOTY=TensorForce Actions-Exploration2019-03-04T11:14:06Z2019-03-04T17:42:25ZTrue
MDU6SXNzdWU0MTczMjQ5MjM=Rewarding knowledge...2019-03-05T14:26:50Z2019-03-05T16:04:08ZTrue
MDU6SXNzdWU0MTkwMjUwNDM=Receiving NotImplementedError when restoring agent.2019-03-09T02:47:49Z2019-03-11T10:17:45ZTrue
MDU6SXNzdWU0MTkzOTc1NDc=Does tensorforce's action exploration randomness decrease over time?2019-03-11T10:32:52Z2019-03-11T12:10:16ZTrue
MDU6SXNzdWU0MTk0MTEyMzc=The exploration setting of PPOAgent2019-03-11T11:03:20Z2019-03-13T04:23:20ZTrue
MDU6SXNzdWU0MTk1MzIxMTY=How to test trained model without creating memory pool?2019-03-11T15:28:24Z2019-09-08T13:54:09ZTrue
MDU6SXNzdWU0MTk4OTk2NTc=Can I add my own activation function to my agent?2019-03-12T10:07:31Z2019-03-13T02:39:22ZTrue
MDU6SXNzdWU0MjAyOTIzNjk=Add the ability to add new activation functions to tensorforce's network...2019-03-13T02:48:24Z2019-09-08T13:53:06ZTrue
MDU6SXNzdWU0MjE3ODk3NzI=Cannot restore large agent (18GB)2019-03-16T09:44:39Z2019-03-17T15:40:31ZTrue
MDU6SXNzdWU0MjI1ODQzMDE=training getting slower and slower2019-03-19T07:39:19Z2019-03-20T08:51:46ZTrue
MDU6SXNzdWU0MjYxOTU1Njc=Limit gpu resources 2019-03-27T21:39:16Z2019-03-29T17:43:57ZTrue
MDU6SXNzdWU0MjYzMTQ4Nzg=How to test validation set while training?2019-03-28T06:06:48Z2019-04-01T08:07:22ZTrue
MDU6SXNzdWU0MjcwOTMwMzI=actions generated at begining almost zero2019-03-29T17:17:44Z2019-04-01T13:05:24ZTrue
MDU6SXNzdWU0Mjk1MjMzNjU=Tensorforce network loss...2019-04-04T23:22:01Z2019-04-09T14:43:10ZTrue
MDU6SXNzdWU0Mjk5OTk2OTc=Tensorforce Installation Issue Major-Revision 2019-04-06T05:27:48Z2019-04-11T05:21:21ZTrue
MDU6SXNzdWU0MzE4NDcxNDY=PGProbRatioModel has no attribute 'act'2019-04-11T06:14:48Z2019-04-11T14:52:09ZTrue
MDU6SXNzdWU0MzI0MjE2NTQ=How to preprocess reward?2019-04-12T07:17:13Z2019-04-16T09:28:35ZTrue
MDU6SXNzdWU0MzM2NzE2MTQ=How to handle action mask in the custom network?2019-04-16T09:19:34Z2019-09-08T13:52:39ZTrue
MDU6SXNzdWU0MzU5OTU3NjQ=Argument of type 'method' is not iterable.2019-04-23T04:40:16Z2019-04-23T10:24:16ZTrue
MDU6SXNzdWU0MzY0MzA1ODI=Step Optimizer Recursion Error2019-04-23T23:21:56Z2019-04-24T03:39:42ZTrue
MDU6SXNzdWU0MzczNDE3Njk=Modify rewards for model.2019-04-25T18:39:44Z2019-04-25T18:46:18ZTrue
MDU6SXNzdWU0Mzk5MDk4ODc=InvalidArgumentError: indices[0] = [0, 1] does not index into shape [1,1]2019-05-03T06:47:57Z2019-09-08T13:52:20ZTrue
MDU6SXNzdWU0NDE5MDM1OTc=Python2.7 support in major-revision branch2019-05-08T19:54:01Z2019-09-08T13:50:31ZTrue
MDU6SXNzdWU0NDI5NzM1OTc=How-to beautifully restrict actions depend on environment state?2019-05-11T09:27:00Z2019-09-08T13:52:00ZTrue
MDU6SXNzdWU0NDMwNzMxODA=tensorflow 2.0 as backend2019-05-12T07:10:19Z2019-09-08T13:51:06ZTrue
MDU6SXNzdWU0NDc1MTU1MDM=PGModel type in the parallel runner2019-05-23T08:25:27Z2019-05-23T11:57:17ZTrue
MDU6SXNzdWU0NDk3NjA1NTk=about tensorforce2019-05-29T11:49:11Z2019-05-30T10:43:01ZTrue
MDU6SXNzdWU0NDk3NzMyMDE=how to begin  to use tensorforce2019-05-29T12:17:00Z2019-05-30T11:21:30ZTrue
MDU6SXNzdWU0NTYzNTgxOTc=restore  error2019-06-14T17:32:13Z2019-06-23T16:32:24ZTrue
MDU6SXNzdWU0NTY1ODQ5OTU=TypeError: restore_model() missing 1 required positional argument: 'self'2019-06-15T23:27:18Z2019-06-16T09:09:42ZTrue
MDU6SXNzdWU0NjEzNjU3NTE=Restoring from checkpoint failed 2019-06-27T07:33:28Z2019-07-04T13:23:05ZTrue
MDU6SXNzdWU0NjMyNDI4NjQ=Error while installing tensorflow on python2.72019-07-02T13:52:15Z2019-09-08T14:06:43ZTrue
MDU6SXNzdWU0NzA0NTk1Mzk=model checkpoint with and without exploration2019-07-19T17:52:45Z2019-07-22T20:36:40ZTrue
MDU6SXNzdWU0NzgxNTY5NTc=Testing large array of input states2019-08-07T21:55:14Z2020-02-11T17:56:06ZTrue
MDU6SXNzdWU0ODAwNDM2NDc=how to get saved/best data 2019-08-13T09:07:55Z2019-09-08T14:06:59ZTrue
MDU6SXNzdWU0OTExOTY4MjY=Incomplete installation using PyPi2019-09-09T16:22:44Z2019-09-16T08:24:57ZTrue
MDU6SXNzdWU0OTM4NjMxMTE=empty2019-09-16T06:07:42Z2019-09-16T08:16:51ZTrue
MDU6SXNzdWU0OTQ5ODYwNjI=MultiDiscrete Action Space TF Exception2019-09-18T04:46:38Z2019-09-19T02:38:56ZTrue
MDU6SXNzdWU0OTUwOTE3Mzc=Can add a full example to show how save and restore a train model which used for test data?2019-09-18T09:08:04Z2019-11-17T19:09:45ZTrue
MDU6SXNzdWU0OTk4NDkyOTc=anyone have documentation for execution specification dict?2019-09-29T03:08:11Z2019-11-17T19:10:00ZTrue
MDU6SXNzdWU1MDAwNTAyNTI=Custom Metrics for Tensorboard2019-09-30T04:56:42Z2020-03-08T16:01:36ZTrue
MDU6SXNzdWU1MDA1Nzg1ODc=Feature Request: OpenAI Gym instance to Tensorforce Environment2019-09-30T23:35:17Z2019-10-06T19:26:10ZTrue
MDU6SXNzdWU1MDA1ODM1NjE=Feature Request: Agent `spec_to_json` method for serialization2019-09-30T23:54:05Z2019-10-06T22:24:42ZTrue
MDU6SXNzdWU1MDE4MjUxMDg=Manually inputting an action for the agent to take2019-10-03T03:03:26Z2019-10-08T22:29:19ZTrue
MDU6SXNzdWU1MDI0OTE1NzU=quickstart with seed raises an exception (typo in agent.py)2019-10-04T08:05:47Z2019-10-04T21:54:14ZTrue
MDU6SXNzdWU1MDU1NjIzMjM=Error message for  'get_current_state' 2019-10-10T23:38:44Z2019-10-12T10:46:50ZTrue
MDU6SXNzdWU1MDY0ODU0ODg=0.5.1 OpenAIGym unable to create level from custom environment2019-10-14T07:20:07Z2019-10-14T17:23:32ZTrue
MDU6SXNzdWU1MDk1NTI2NjM=Progress bar2019-10-20T06:22:26Z2019-11-17T19:10:44ZTrue
MDU6SXNzdWU1MTE4MzAyNTM=tensorflow version 2.0 not compatible2019-10-24T09:43:19Z2019-11-17T20:17:01ZTrue
MDU6SXNzdWU1MTI1MTgzMTg=no longer python2 compatible: update readme2019-10-25T13:30:52Z2019-10-27T10:58:20ZTrue
MDU6SXNzdWU1MTI5MDYxMDc=PyGameLearningEnvironment documentation2019-10-27T01:23:00Z2019-10-27T10:58:00ZTrue
MDU6SXNzdWU1MTQxMTEyNTk=Custom network and layer freezing2019-10-29T17:54:56Z2020-02-11T17:53:10ZTrue
MDU6SXNzdWU1MTUyNDA0NTc=possible bug with the ```max_episode_timesteps``` param of the Agent2019-10-31T08:07:34Z2019-11-17T19:11:53ZTrue
MDU6SXNzdWU1MTUyNDIyMTE=erased best_model problem2019-10-31T08:11:26Z2019-11-07T20:52:18ZTrue
MDU6SXNzdWU1MTY5MDMzNDE='dict' object has no attribute 'modules' - network.modules.values()2019-11-03T22:59:42Z2019-11-03T23:23:49ZTrue
MDU6SXNzdWU1MjEzMDg1MTE=Tensorforce DPG Actions parameter 2019-11-12T03:36:05Z2019-11-12T23:59:48ZTrue
MDU6SXNzdWU1MjE4MjIxODg=execute() got an unexpected keyword argument 'actions'2019-11-12T21:46:54Z2019-11-13T09:27:13ZTrue
MDU6SXNzdWU1NDA2Njg0MTU=InvalidArgumentError on terminal observe call2019-12-20T00:43:28Z2020-01-04T15:56:55ZTrue
MDU6SXNzdWU1NDEzNjQyMzk=OpenAI Gym environment assumes termination after max_episode_timesteps2019-12-21T19:55:20Z2019-12-27T15:35:33ZTrue
MDU6SXNzdWU1NDU5NzE2Mzc=Installation Missing Documentation2020-01-06T22:23:09Z2020-01-07T22:29:04ZTrue
MDU6SXNzdWU1NDY1MTIyMzk="TensorFlow Error: ""Object was never used (type <class tensorflow.python.framework.ops.Operation>)"""2020-01-07T21:17:21Z2020-01-08T10:08:44ZTrue
MDU6SXNzdWU1NDg1MDgyNDI=cannot set execution_spec?2020-01-12T02:00:01Z2020-01-12T16:09:44ZTrue
MDU6SXNzdWU1NTY0ODYwMjM=Quickstart.py errors with Invalid value for util.dtype argument x: <dtype: 'int64_ref'>.2020-01-28T21:41:13Z2020-01-29T15:27:41ZTrue
MDU6SXNzdWU1NTgzOTgwNDE=Rainbow Agent Request2020-01-31T22:05:44Z2020-02-10T13:24:59ZTrue
MDU6SXNzdWU1NTg1MTcyNTQ=[Bug Report] 'name' is used without declaration2020-02-01T10:05:57Z2020-02-01T15:33:15ZTrue
MDU6SXNzdWU1NTk1NzE0MTU=quickstart.py fails with gym 0.15.62020-02-04T08:52:12Z2020-02-04T17:23:23ZTrue
MDU6SXNzdWU1NjIzNDY2MTU=Quick start example is not working2020-02-10T06:10:28Z2020-02-10T08:41:47ZTrue
MDU6SXNzdWU1NjM5NzcxMTg=Summarizer argument not working properly2020-02-12T12:46:36Z2020-02-12T21:39:38ZTrue
MDU6SXNzdWU1NjQ4NzQyNDc=Quickstart.py example is inoperable2020-02-13T18:35:28Z2020-02-13T19:33:30ZTrue
MDU6SXNzdWU1NjUzMDgzNTA="RandomAgent fails with and without ""parallel_interactions"" parameter"2020-02-14T12:50:33Z2020-02-15T11:30:46ZTrue
MDU6SXNzdWU1NjYyOTg3NjM="""parallel_interactions"" breaks Agent.create()   (Tensorforce 0.5.3)"2020-02-17T13:27:43Z2020-02-18T12:28:07ZTrue
MDU6SXNzdWU1NzEyMDY4OTQ=summarizer arguments (refs #651)2020-02-26T09:56:18Z2020-03-09T07:23:48ZTrue
MDU6SXNzdWU1NzY1Nzk5Mjc=Runner.run() got TypeError when giving save_best_agent2020-03-05T22:53:04Z2020-03-06T08:51:05ZTrue
MDU6SXNzdWU1Nzg3ODcwNDA=Unable to train for many episodes: RAM usage too high!2020-03-10T18:43:09Z2020-05-03T15:30:58ZTrue
MDU6SXNzdWU1ODMwNzc1MzI=Error about parallels2020-03-17T15:01:26Z2020-03-20T06:26:25ZTrue
MDU6SXNzdWU1ODUwNDI5NzI=Can training and running be separated ?2020-03-20T12:39:30Z2020-03-26T21:29:07ZTrue
MDU6SXNzdWU1ODUwODk2NzU=Can training and running be separated ?2020-03-20T14:01:28Z2020-03-20T19:16:20ZTrue
MDU6SXNzdWU1OTE1NTYyNzU=Missing concrete function in the saved optimized Protobuf model2020-04-01T01:47:17ZFalse
MDU6SXNzdWU1OTIyMjc5NDY=How to add exploration parameters to an agent?2020-04-01T21:56:59Z2020-04-02T08:51:45ZTrue
MDU6SXNzdWU1OTYyMDc2NDI=Agent class actions and states arguments are not type dict2020-04-07T23:20:32Z2020-04-13T13:23:53ZTrue
MDU6SXNzdWU1OTkyNjI4NTc=What is the purpose of num_parallel?2020-04-14T03:06:14Z2020-04-14T08:35:07ZTrue
MDU6SXNzdWU2MDQwMTI4OTk=Parallel environments inconsistent behavior 2020-04-21T13:47:47ZFalse
MDU6SXNzdWU2MDU4ODEyODk=customized gym environment run into 'float division by zero'2020-04-23T21:19:13Z2020-05-02T15:38:32ZTrue
MDU6SXNzdWU2MDY4MTE4Mzk=Getting started: Custom environment2020-04-25T16:59:36Z2020-04-26T14:06:34ZTrue
MDU6SXNzdWU2MDcwMzA5MTE=how to make a C++ based parallel running environment 2020-04-26T14:51:39ZFalse
MDU6SXNzdWU2MDk2NjIyMDY=cannot run example: AttributeError: module 'tensorflow._api.v1.keras.optimizers' has no attribute 'Ftrl'2020-04-30T07:23:32Z2020-04-30T10:14:26ZTrue
MDU6SXNzdWU2MTAyMzgwMDc=Networks: How to define ResNet-like Skip Connections?2020-04-30T16:57:24ZFalse
MDU6SXNzdWU2MTA0NDQwMDU=Recreating the NAF Agent2020-04-30T23:00:28ZFalse
MDU6SXNzdWU2MTEzMzQ4NzA=Is it possible to build a hybrid continuous and discrete action model using tensorforce?2020-05-03T05:14:05Z2020-05-11T08:17:00ZTrue
MDU6SXNzdWU2MTE0MTE4MDg=[not issue] How to implement Soft Actor-Critic?2020-05-03T13:13:03ZFalse
MDU6SXNzdWU2MTE3MzUyNzQ=How to render the environment?2020-05-04T09:35:45Z2020-05-04T13:04:01ZTrue
MDU6SXNzdWU2MTE3MzY3Mzc=Does Tensorforce require a normalized action space?2020-05-04T09:38:10Z2020-05-05T07:43:31ZTrue
MDU6SXNzdWU2MTE4MTIxNjE=Unexpected error when using DPG agent2020-05-04T11:46:10Z2020-05-10T19:25:16ZTrue
MDU6SXNzdWU2MTI0MTIyODE=Error when building a CSTR environment2020-05-05T07:57:41ZFalse
MDU6SXNzdWU2MTQyNzIxMzg=Agent does not learn in continuous environments2020-05-07T18:54:31Z2020-05-10T19:21:40ZTrue
MDU6SXNzdWU2MTU4NTUyNDE=How to load a supervised pre-trained model 2020-05-11T12:43:08ZFalse
MDU6SXNzdWU2MTYyOTMxMTE=Calling agent.act must be preceded by agent.observe when running a customized environment2020-05-12T01:33:23Z2020-05-12T01:42:00ZTrue
