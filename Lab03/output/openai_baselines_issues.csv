idtitlecreatedAtclosedAtclosed
MDU6SXNzdWUyMzExNTExNDM=Pip install fails on OS-X 2017-05-24T19:47:30Z2017-05-24T21:54:59ZTrue
MDU6SXNzdWUyMzExNzk4MTA=Example fails2017-05-24T21:41:34Z2017-05-24T21:54:13ZTrue
MDU6SXNzdWUyMzExODc0NTg=Running into issues on example execution2017-05-24T22:17:35Z2017-05-26T23:26:02ZTrue
MDU6SXNzdWUyMzEyMjQwMzM=no BreakoutNoFrameskip-v3 env2017-05-25T02:33:56Z2017-05-25T02:34:13ZTrue
MDU6SXNzdWUyMzEzMDkwMjA=Requirements not clearly stated2017-05-25T11:03:25Z2017-05-25T21:54:21ZTrue
MDU6SXNzdWUyMzEzMTQ3Mjg=Last command line fails2017-05-25T11:31:28Z2017-05-25T21:54:01ZTrue
MDU6SXNzdWUyMzE1Nzc5MDA=failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED2017-05-26T09:47:45Z2017-05-26T09:53:19ZTrue
MDU6SXNzdWUyMzIwMjM2MzI=Error when restoring model to run enjoy.py2017-05-29T13:29:26Z2017-06-05T02:22:21ZTrue
MDU6SXNzdWUyMzI2MDIxMzc=Baseline network does not learn Pong2017-05-31T15:21:06Z2017-05-31T15:51:21ZTrue
MDU6SXNzdWUyMzI2NDI2MDc=Load act and continue learning2017-05-31T17:43:32Z2017-06-01T03:02:09ZTrue
MDU6SXNzdWUyMzI3MDYyMTg=cannot import name 'deepq'2017-05-31T21:46:06Z2017-06-01T02:57:08ZTrue
MDU6SXNzdWUyMzMwNDc1Njg=Integration with Rllab2017-06-02T00:42:34Z2017-06-04T07:39:41ZTrue
MDU6SXNzdWUyMzMwNDc5OTI=Blas GEMM launch failed2017-06-02T00:46:42Z2017-06-04T07:40:03ZTrue
MDU6SXNzdWUyMzMxMDAyNDk=Roadmap of DDPG, TRPO and Q-prop updates2017-06-02T07:34:11Z2017-06-04T07:38:09ZTrue
MDU6SXNzdWUyMzM5NTEyMTg=warnings in cartpole example2017-06-06T16:31:36Z2017-06-09T04:55:56ZTrue
MDU6SXNzdWUyMzQxNDE3MDE=Gym and ALE2017-06-07T09:08:27Z2017-06-09T04:55:25ZTrue
MDU6SXNzdWUyMzQ1MjIzMjY=Fails to import a module, from itself2017-06-08T13:11:08Z2017-06-24T00:10:44ZTrue
MDU6SXNzdWUyMzUwMTAxNTE=what's the detail means of the available models name?2017-06-10T15:48:09Z2017-06-24T00:11:39ZTrue
MDU6SXNzdWUyMzU1NjAwOTY=How to set the randomness of the act during enjoy?2017-06-13T13:47:29Z2017-06-24T00:12:10ZTrue
MDU6SXNzdWUyMzcyOTQxMTY=Unable to download all pretrained models2017-06-20T17:55:19Z2017-06-24T00:13:18ZTrue
MDU6SXNzdWUyMzgzMTM3MDI=Unable to download all pretrained models2017-06-24T12:06:55Z2018-05-10T00:49:13ZTrue
MDU6SXNzdWUyMzkwMjMyMDQ=Program Fails in def log.2017-06-28T00:53:03Z2017-07-12T21:58:55ZTrue
MDU6SXNzdWUyMzkwNDMzNzA=Is a plan to replicate other RL algorithms like A3C, DPG, etc ?2017-06-28T03:31:34Z2017-06-28T04:39:58ZTrue
MDU6SXNzdWUyMzkzMjIzODk=recording activations2017-06-28T23:04:43Z2017-07-12T21:58:44ZTrue
MDU6SXNzdWUyMzk0Njc0NjQ=Small possible divergence with original DQN paper2017-06-29T12:48:45Z2017-06-29T17:48:13ZTrue
MDU6SXNzdWUyMzk1NTkwNzM=Confusion between `done` and `info` in `env.step`, and the correct way we need to detect for when episodes complete.2017-06-29T17:41:58Z2017-07-12T21:59:39ZTrue
MDU6SXNzdWUyMzk2NjE5MjY=Not able to run pre-trained model2017-06-30T02:34:04Z2017-06-30T11:45:17ZTrue
MDU6SXNzdWUyMzk5ODI2MTI=Pop-Art implementation ? 2017-07-02T01:33:32Z2017-07-03T22:28:09ZTrue
MDU6SXNzdWUyNDAyNTE5Mjg=LSTM Model2017-07-03T19:42:36Z2017-07-12T21:58:16ZTrue
MDU6SXNzdWUyNDA4MzE1MTk=Fail to load pretrained checkpoints2017-07-06T03:02:09Z2017-07-12T22:00:17ZTrue
MDU6SXNzdWUyNDA5MTgyNjU=MaxAndSkipEnv is calculating the max over the last two time steps only2017-07-06T10:52:30Z2017-07-12T21:31:54ZTrue
MDU6SXNzdWUyNDExMzY0MTY=Baseline code for policy gradient methods?2017-07-07T02:03:56Z2017-07-16T02:05:33ZTrue
MDU6SXNzdWUyNDE1Nzc4MjM=a2017-07-10T01:40:26Z2017-07-10T01:40:30ZTrue
MDU6SXNzdWUyNDIxMjA5MDI=Support for continuous action spaces2017-07-11T17:14:08Z2017-07-12T21:30:27ZTrue
MDU6SXNzdWUyNDI4ODQ3MTE=Low GPU usage2017-07-14T03:03:24Z2017-07-20T02:10:53ZTrue
MDU6SXNzdWUyNDM0NTc4NTM=Integration with Google Cloud ML Engine2017-07-17T16:47:57Z2017-07-17T17:57:37ZTrue
MDU6SXNzdWUyNDQxODQxNDY=Crash on import2017-07-19T21:46:58Z2017-07-20T02:11:26ZTrue
MDU6SXNzdWUyNDQyMTAzMTk=Atari wrappers deprecated?2017-07-20T00:12:59Z2017-07-20T02:11:51ZTrue
MDU6SXNzdWUyNDQyMzg4OTU=sum-tree unit test2017-07-20T03:57:21Z2017-08-09T05:29:43ZTrue
MDU6SXNzdWUyNDQ1MzUxODA=Benchmarking for PPO and TRPO2017-07-21T00:48:05ZFalse
MDU6SXNzdWUyNDQ2MzAxMDQ=ImportError: cannot import name 'weakref'2017-07-21T10:31:17Z2017-08-28T05:42:14ZTrue
MDU6SXNzdWUyNDUxNTcyMDY=Poor PPO 1 CPU  performance on the Pong task2017-07-24T17:46:26Z2018-02-05T21:51:03ZTrue
MDU6SXNzdWUyNDUyNTEwMzY=Pretrained Breakout model error2017-07-25T00:19:08Z2017-07-25T00:20:13ZTrue
MDU6SXNzdWUyNDUyNTkwNzQ=Could you explain how to execute PPO and TRPO?2017-07-25T01:22:15Z2017-07-31T08:17:05ZTrue
MDU6SXNzdWUyNDU0MDkzODg=Can not execute PPO and TRPO2017-07-25T13:54:37Z2017-08-28T05:41:47ZTrue
MDU6SXNzdWUyNDU2MDE2NTg=PPO OOM2017-07-26T04:15:44Z2018-02-05T21:48:56ZTrue
MDU6SXNzdWUyNDU2ODU2NDc=Multi thread to run on Mujoco?2017-07-26T11:06:21Z2018-02-05T21:35:00ZTrue
MDU6SXNzdWUyNDU4ODg0NjA=Save and load of the trained TRPO and PPO agents2017-07-26T23:57:48Z2017-08-28T05:26:55ZTrue
MDU6SXNzdWUyNDYwOTQ3NzQ=A little bug in line 111 in replay_buffer.py??2017-07-27T16:17:30ZFalse
MDU6SXNzdWUyNDY0ODcwMTU=Please expose size, number and activation function parameters for DDPG  actor and critic2017-07-29T00:32:01Z2018-02-05T21:56:50ZTrue
MDU6SXNzdWUyNDY1ODE3MTU=suggestion - encourage users to use miniconda instead of pip2017-07-30T12:45:43Z2017-08-28T05:25:47ZTrue
MDU6SXNzdWUyNDY2MDM5NjY=I have an issue.... I can't run ppo algorithm... and trpo, either...2017-07-30T19:32:58Z2017-07-31T08:16:56ZTrue
MDU6SXNzdWUyNDY2MDQ5MTI=I got an error when i run ppo and trpo....2017-07-30T19:50:38Z2017-07-31T08:16:48ZTrue
MDU6SXNzdWUyNDY2MTM0NjI=Storing results in csv2017-07-30T22:32:47Z2017-08-28T05:41:08ZTrue
MDU6SXNzdWUyNDY2NjU4MzU=WHen i run ddpg, I got this error:2017-07-31T07:19:46Z2017-07-31T08:16:41ZTrue
MDU6SXNzdWUyNDY2NzkwMjA=Error about import folders and files2017-07-31T08:21:39Z2017-08-28T05:54:13ZTrue
MDU6SXNzdWUyNDY4MjExNzc=Import Error2017-07-31T17:05:50Z2017-08-02T07:49:06ZTrue
MDU6SXNzdWUyNDY5ODk1MDE=Is DDPG cod all right??2017-08-01T08:13:22Z2018-02-05T21:33:41ZTrue
MDU6SXNzdWUyNDc5OTE5Mzk=Is the old-policy updated at each epoch? 2017-08-04T12:30:16Z2018-02-05T22:00:05ZTrue
MDU6SXNzdWUyNDgyMzEyMjI=Are there some documents which teach these algorithm?2017-08-06T07:36:00Z2017-08-28T05:25:25ZTrue
MDU6SXNzdWUyNDgyNDkyMDI=NoFrameskip?2017-08-06T14:05:10Z2018-02-05T21:33:05ZTrue
MDU6SXNzdWUyNDgzNzg4Nzg=Expanding the documentation2017-08-07T11:11:43Z2018-02-05T21:44:29ZTrue
MDU6SXNzdWUyNDg0NTE4NjM=IN PPO, clipping the value loss with max is OK?2017-08-07T15:43:34Z2017-08-12T13:28:41ZTrue
MDU6SXNzdWUyNDg1NzI3Mjk=baselines/baselines/pposgd/pposgd_simple.py- clipping causing no gradient?2017-08-08T01:09:32Z2017-08-28T05:25:05ZTrue
MDU6SXNzdWUyNDg5MjEwMTI=Still planning to release A3C?2017-08-09T05:31:19Z2017-08-28T05:24:21ZTrue
MDU6SXNzdWUyNDkyMTUxOTU=LazyFrame in Atari wrappers2017-08-10T02:31:24Z2018-02-05T21:01:23ZTrue
MDU6SXNzdWUyNDk4MjcxODA=Is DDPG faster on CPU with multiple processes or with GPU?2017-08-12T18:15:11ZFalse
MDU6SXNzdWUyNDk5MjAxNTg=InvalidArgumentError occured when visualizing the trained model2017-08-14T02:46:41ZFalse
MDU6SXNzdWUyNDk5NTgzNTE=Qeustion about DDPG algorithm2017-08-14T08:00:26Z2017-12-16T02:16:22ZTrue
MDU6SXNzdWUyNTA3OTEyMzE=New algorithms of VIME and Q-Prop2017-08-16T23:43:29Z2018-02-05T21:31:22ZTrue
MDU6SXNzdWUyNTA4NDUyMTM=Question on perturb_vars function in Parameter Noise2017-08-17T06:49:13ZFalse
MDU6SXNzdWUyNTExMzUyMjk=Add name and variable scopes to the PPO graph2017-08-18T04:17:23Z2017-08-28T05:23:57ZTrue
MDU6SXNzdWUyNTE0ODY3OTI=Question on tf.stop_gradient in dqn build_graph2017-08-20T12:55:39Z2018-02-05T21:30:16ZTrue
MDU6SXNzdWUyNTE1NTU3NTM=Restoring checkpoint in next training run2017-08-21T05:50:25ZFalse
MDU6SXNzdWUyNTE4NDY4MDU=mpi4py dependent algorithms broken in commit 3f676f7d1e16a0880cb7fed5850bc00efba413d62017-08-22T06:03:11Z2017-08-28T05:42:05ZTrue
MDU6SXNzdWUyNTIxNTIzODc=Manually reset the environment?2017-08-23T03:52:53Z2017-08-28T05:17:41ZTrue
MDU6SXNzdWUyNTIxNTYwOTA=baselines/baselines/trpo_mpi/run_atari.py   Bug about bench.Monitor2017-08-23T04:25:17Z2017-08-28T05:17:10ZTrue
MDU6SXNzdWUyNTIxOTU5NjU='install_requires' problem when installing using pip2017-08-23T08:08:27Z2017-08-28T05:53:09ZTrue
MDU6SXNzdWUyNTI0MzAyNjI=configure logger before use2017-08-23T22:04:13Z2017-08-28T05:38:24ZTrue
MDU6SXNzdWUyNTI1MDU1NDc=fatal error: mpi.h: No such file or directory2017-08-24T06:59:39Z2017-08-28T05:17:01ZTrue
MDU6SXNzdWUyNTI3NDM4NDE=No enjoy.py for acktr2017-08-24T22:01:13ZFalse
MDU6SXNzdWUyNTMwMjk4MDQ=basic issues2017-08-25T22:11:21Z2017-08-25T22:28:00ZTrue
MDU6SXNzdWUyNTMxMDg3NzE=Is evaluation proper? [concerns]2017-08-26T17:09:29Z2017-08-28T05:38:11ZTrue
MDU6SXNzdWUyNTMxOTg5MzE=Missing Benchmark Scripts for PPO2017-08-27T23:53:32Z2017-08-28T05:16:20ZTrue
MDU6SXNzdWUyNTM2NTIyOTg=Taking action limits into account in PPO/TRPO/ACKTR.2017-08-29T13:06:07ZFalse
MDU6SXNzdWUyNTQzMjk5OTA=Why the batchsize in for PPO for mujoco is so small?2017-08-31T13:13:24Z2017-09-21T19:20:12ZTrue
MDU6SXNzdWUyNTQ3Nzc2Nzg=Replicating PPO Paper A2C results2017-09-02T00:33:43Z2018-02-05T22:03:36ZTrue
MDU6SXNzdWUyNTQ4MDY0Njc=Replicating PPO result2017-09-02T11:34:56Z2018-02-05T22:03:01ZTrue
MDU6SXNzdWUyNTUwNDA1MzU=NaN values in acktr2017-09-04T13:20:03Z2017-09-28T10:53:18ZTrue
MDU6SXNzdWUyNTY3Mzg1NDU=ACKTR inconsistencies with paper2017-09-11T15:11:36Z2017-09-18T08:37:44ZTrue
MDU6SXNzdWUyNTY3ODczOTU=Zero gradient from clipping critic output in DDPG?2017-09-11T17:46:36ZFalse
MDU6SXNzdWUyNTY5MzY1NjY=python 2.7 can't support iterbatches in dataset.py2017-09-12T06:54:25Z2017-09-13T03:59:39ZTrue
MDU6SXNzdWUyNTc2MjA4MjA=Running Baselines on Windows2017-09-14T07:27:15Z2017-09-14T08:08:53ZTrue
MDU6SXNzdWUyNTc2NDY1OTU=How to force PPO to render environment?2017-09-14T09:06:25Z2018-02-05T21:08:44ZTrue
MDU6SXNzdWUyNTc5MDk2MDI=How to reproduce benchmarks2017-09-15T02:30:29Z2017-09-15T04:48:00ZTrue
MDU6SXNzdWUyNTgyMDI5ODA=Unknown MS Compiler version 19002017-09-16T02:54:27Z2018-02-05T21:02:41ZTrue
MDU6SXNzdWUyNTgyMTc2MTg=PPO Atari performance2017-09-16T08:23:39Z2017-10-20T01:55:24ZTrue
MDU6SXNzdWUyNTg0NDk1MjY=Nan in PPO1 lossandgrad function2017-09-18T11:37:58Z2018-02-05T21:07:23ZTrue
MDU6SXNzdWUyNTg4NzMwNDQ=Outputting activations from ReLU in DQN2017-09-19T15:54:09ZFalse
MDU6SXNzdWUyNTk5MTYwMjc=MPI not working with python3.6 in mac2017-09-22T19:37:21Z2018-02-05T21:05:20ZTrue
MDU6SXNzdWUyNjAwMDQzMjY=setup.py forces CPU version of tensorflow to be installed.2017-09-23T12:24:15Z2018-08-22T00:20:32ZTrue
MDU6SXNzdWUyNjA1Mzg4NTc=ACKTR with LSTM from A2C2017-09-26T08:40:47Z2017-12-16T00:05:00ZTrue
MDU6SXNzdWUyNjA3MDMxMTA=Pre-trained DQN (+variants) models on the deprecated Atari wrapper2017-09-26T17:17:03ZFalse
MDU6SXNzdWUyNjEyMzQxNjU=Monitor Early Resets2017-09-28T08:29:11Z2018-02-05T21:04:11ZTrue
MDU6SXNzdWUyNjE5OTA5Mzk=Is there a way to access the Q-Values for a given observation?2017-10-02T06:18:20Z2017-10-04T02:26:20ZTrue
MDU6SXNzdWUyNjI2NDc3NjA=Saving and restoring states2017-10-04T02:30:53ZFalse
MDU6SXNzdWUyNjMyMjAzMTE=potential bug in baselines/deepq/build_graph.py line2252017-10-05T18:22:16Z2018-01-16T18:25:33ZTrue
MDU6SXNzdWUyNjM3MzczMjA=How to define multi-dimensional action_space for deepq2017-10-08T18:02:16Z2017-12-16T00:03:11ZTrue
MDU6SXNzdWUyNjM5OTQwNzE=Saving and restoring DDPG agent2017-10-09T18:57:29ZFalse
MDU6SXNzdWUyNjQ0MzU0Mjk=How to get the value of pi.pd.logp(ac)2017-10-11T02:16:29Z2017-10-11T06:54:00ZTrue
MDU6SXNzdWUyNjQ1NzQ4Njg=Cannot pip install2017-10-11T12:53:25Z2018-02-05T21:15:44ZTrue
MDU6SXNzdWUyNjU0NDkwNTk=Which version of python should I be using?2017-10-14T00:19:19Z2017-12-15T20:11:55ZTrue
MDU6SXNzdWUyNjU0ODA5NzQ=mlp() got an unexpected keyword argument 'layer_norm'2017-10-14T10:19:08ZFalse
MDU6SXNzdWUyNjU1MDU3Mjc=Is MPI being used in baselines DDPG?2017-10-14T16:51:00ZFalse
MDU6SXNzdWUyNjYwMTA3OTc=Sampling from prioritized replay buffer of size 1 causes stack overflow2017-10-17T06:47:29ZFalse
MDU6SXNzdWUyNjYwMTIwMDI=Prioritized  replay buffer does not sample the last experience record2017-10-17T06:54:00ZFalse
MDU6SXNzdWUyNjYzMjc4MjA=Double tf.log()'s when calculating CategoricalPd.sample() ?2017-10-18T01:01:55Z2017-12-15T20:10:40ZTrue
MDU6SXNzdWUyNjY1MTAxNzQ=Can't install baselines on Windows2017-10-18T14:16:46Z2018-02-05T22:04:03ZTrue
MDU6SXNzdWUyNjcxNTU3NzY=FileExistsError when saving models2017-10-20T12:12:18Z2018-02-05T21:29:17ZTrue
MDU6SXNzdWUyNjcxNjYyNDE=Cannot reproduce Breakout benchmark using Double DQN2017-10-20T12:54:03ZFalse
MDU6SXNzdWUyNjc1MjU2MzY=Is there any way to run this with python 2.7?2017-10-23T02:42:36Z2017-12-15T20:09:06ZTrue
MDU6SXNzdWUyNjg1Mjg0MDk=Bug in baselines/ddpg/ddpg.py?2017-10-25T19:56:49ZFalse
MDU6SXNzdWUyNjg1Mjk4ODU=ModuleNotFoundError: No module named 'baselines.bench.simple_bench'2017-10-25T20:01:43ZFalse
MDU6SXNzdWUyNjg1NzAyODQ=baselines/baselines/trpo_mpi difference between value functions for mujoco vs atari2017-10-25T22:23:18Z2018-03-22T07:56:35ZTrue
MDU6SXNzdWUyNjg4MDIyMzc=confusing of adapt_param_noise in ddpg2017-10-26T15:17:02ZFalse
MDU6SXNzdWUyNjg5ODk3Mzc=run_atari.py versus atari/train.py2017-10-27T05:09:29ZFalse
MDU6SXNzdWUyNjk1MDA0Mjg=Performance of PPO on Pong2017-10-30T07:08:52Z2018-02-05T21:20:49ZTrue
MDU6SXNzdWUyNjk4MDA1MTk=train_pong.py appears to have been deleted but is still referenced in the deepq readme2017-10-31T00:55:01ZFalse
MDU6SXNzdWUyNjk5MzgyMTA=TRPO with parameter space noise2017-10-31T12:21:08ZFalse
MDU6SXNzdWUyNzAwNTcyMzM=Number of steps for PPO2017-10-31T17:54:45Z2017-10-31T18:36:04ZTrue
MDU6SXNzdWUyNzA1NTU5NDk=Plotting Results2017-11-02T07:19:10Z2018-02-05T22:02:26ZTrue
MDU6SXNzdWUyNzA4MTAwMzk=ModuleNotFoundError: No module named 'baselines.common.atari_wrappers_deprecated'2017-11-02T21:16:04Z2018-11-14T22:38:25ZTrue
MDU6SXNzdWUyNzEyMjAyNTI=Pass pretrained supervised q function in deepq.learn 2017-11-04T20:05:18Z2018-02-05T21:00:23ZTrue
MDU6SXNzdWUyNzEzOTI3MDY=A2C multicore usage is not parallel2017-11-06T08:25:40Z2018-02-05T20:58:06ZTrue
MDU6SXNzdWUyNzE2MDg1NDE=Doesn't install correctly2017-11-06T20:25:04Z2018-02-05T20:54:35ZTrue
MDU6SXNzdWUyNzE2NzI4NDQ=ModuleNotFoundError: No module named 'baselines.bench.simple_bench'2017-11-07T00:50:38ZFalse
MDU6SXNzdWUyNzMyMTU4ODE=TypeError: list indices must be integers or slices, not NoneType2017-11-12T09:35:41Z2018-02-05T20:51:02ZTrue
MDU6SXNzdWUyNzMyNDAwNjc=Pretrained Atari agent fails with tensorflow error2017-11-12T15:46:13ZFalse
MDU6SXNzdWUyNzMyODE3NDM=Process finished with exit code 137 (interrupted by signal 9: SIGKILL)2017-11-13T00:43:41ZFalse
MDU6SXNzdWUyNzM2MDI0MDc=Does not work on Reacher-v12017-11-13T22:34:32ZFalse
MDU6SXNzdWUyNzM2NDkyODg=maximum episode length of NoFrameskip2017-11-14T02:45:26ZFalse
MDU6SXNzdWUyNzQwMjA3MTM=PPO replication of original paper2017-11-15T03:14:43Z2017-11-24T02:27:59ZTrue
MDU6SXNzdWUyNzQyOTE5Mzc=SubprocVecEnv does not support rendering2017-11-15T20:23:09ZFalse
MDU6SXNzdWUyNzQ3MDQwOTA=How to do Single Prediction2017-11-16T23:46:00Z2017-11-23T20:20:11ZTrue
MDU6SXNzdWUyNzQ3MDQyMzU=PPO2: missing VecNormalize and DummyVecEnv2017-11-16T23:46:43Z2017-11-17T19:29:58ZTrue
MDU6SXNzdWUyNzQ3MzY2NTM=Failed building wheel for atari-py2017-11-17T03:01:06Z2017-12-15T20:06:52ZTrue
MDU6SXNzdWUyNzQ5MTMyOTc=`gaussian_fixed_var` boolean flag learns the variance2017-11-17T16:04:10ZFalse
MDU6SXNzdWUyNzUwMTYyODI=PPO1 examples in PPO2 readme2017-11-17T22:32:46Z2017-12-15T20:06:15ZTrue
MDU6SXNzdWUyNzUyNjEyNTI=Viewing a model in Tensorboard2017-11-20T07:33:58Z2018-02-05T20:54:15ZTrue
MDU6SXNzdWUyNzY4OTg2MDM=Training time of PPO22017-11-27T04:33:27Z2018-02-05T20:35:13ZTrue
MDU6SXNzdWUyNzcyNjM5Mzc=A problem on TRPO implementation2017-11-28T05:30:52Z2017-12-04T21:54:15ZTrue
MDU6SXNzdWUyNzc3NzE5NTY=Major Bug in a2c implementation2017-11-29T14:22:12Z2017-11-30T12:23:17ZTrue
MDU6SXNzdWUyNzg1MTcyOTE=I can't install this project2017-12-01T16:16:40Z2017-12-04T21:56:03ZTrue
MDU6SXNzdWUyNzkwOTA5NDE=Could you please provide an example use the LSTM as policy network?2017-12-04T18:01:23Z2018-10-31T05:27:40ZTrue
MDU6SXNzdWUyODA0ODg4ODE=Pre-trained model performance issues?2017-12-08T13:25:46Z2018-02-05T20:23:18ZTrue
MDU6SXNzdWUyODE4ODUyMTI=Inconsistent returns calculation across algorithms2017-12-13T20:30:01Z2017-12-15T20:04:59ZTrue
MDU6SXNzdWUyODI2MDYwMzY=Deepq algorithm can't converge when change to another atari game2017-12-16T07:17:37ZFalse
MDU6SXNzdWUyODI2NDA0MjA="ImportError: Library ""GL"" not found"2017-12-16T18:15:34Z2018-02-05T20:12:17ZTrue
MDU6SXNzdWUyODM2NjUwMTg=reset_keywords in bench/monitor.py is non-functional2017-12-20T19:06:55Z2018-02-05T20:10:14ZTrue
MDU6SXNzdWUyODQzNTU5NzI=regarding the method sample in A2C2017-12-24T10:52:58Z2018-02-05T16:16:00ZTrue
MDU6SXNzdWUyODQ0NjcwODE=subproc_vec_env stacking shouldn't always use numpy arrays2017-12-25T20:06:11ZFalse
MDU6SXNzdWUyODQ4NzU0MzE=Can you provide the training curve of DQN and DoubleDQN in MountainCar and CartPole2017-12-28T09:26:45ZFalse
MDU6SXNzdWUyODUyMjA0MTU=Reference for DeepMind using FireResetEnv wrapper's functionality2017-12-30T19:47:09ZFalse
MDU6SXNzdWUyODUyNjk2OTM=DDPG in parallel not working2017-12-31T17:54:50ZFalse
MDU6SXNzdWUyODYwNTkyMjY=Will DDPG work with prioritized replay2018-01-04T17:33:22ZFalse
MDU6SXNzdWUyODY0NjU4MDg=Where are the stacking frames?2018-01-06T07:09:26Z2018-01-06T07:33:04ZTrue
MDU6SXNzdWUyODY0NzIxNTY=Try multiple actions on each step?2018-01-06T09:22:47Z2018-02-05T16:39:37ZTrue
MDU6SXNzdWUyODY1MTI1MzE=dlopen: cannot load any more object with static TLS2018-01-06T19:39:58Z2018-01-27T00:28:15ZTrue
MDU6SXNzdWUyODY1ODk0NTY=Frame stack in vec_env2018-01-07T18:40:31Z2018-01-11T17:30:38ZTrue
MDU6SXNzdWUyODcxNjc1MDA=Possible memory leak while running DeepQ for Atari games ?2018-01-09T17:23:28Z2018-02-05T16:29:12ZTrue
MDU6SXNzdWUyODczOTA5MjA=python-dev is required for installation2018-01-10T10:53:00ZFalse
MDU6SXNzdWUyODc4NTk3NDY=A2C stops(crashes?) with no error message after a couple million steps2018-01-11T17:19:15Z2018-02-05T16:33:39ZTrue
MDU6SXNzdWUyODc5Mzc2MDc=TypeError in custom_cartpole example2018-01-11T21:55:41Z2018-02-05T16:35:59ZTrue
MDU6SXNzdWUyODgyNTU3MDA=NaN episode rewards on ppo2 PongNoFrameskip-v42018-01-12T21:47:56Z2018-01-13T07:17:37ZTrue
MDU6SXNzdWUyODgyOTYzODk=what's the input state to the pre-trained pong agent?2018-01-13T02:41:56Z2018-01-13T03:31:56ZTrue
MDU6SXNzdWUyODk4ODcyMzc=parameter  space noise2018-01-19T07:38:14ZFalse
MDU6SXNzdWUyOTAyMTQ0MjA=Request for prioritized reply example in DDPG baseline2018-01-20T18:54:47ZFalse
MDU6SXNzdWUyOTAyMzM5NjE=Some problems with baseline PPO12018-01-21T00:01:11Z2018-02-06T11:31:43ZTrue
MDU6SXNzdWUyOTEzOTEyNTU=TRPO bug2018-01-24T22:59:13Z2018-01-26T05:18:35ZTrue
MDU6SXNzdWUyOTEzOTk3MDc=Simple way to run trained policy with PPO1/PPO2/TRPO?2018-01-24T23:37:30ZFalse
MDU6SXNzdWUyOTE0ODA0NzY=Adapting to different environments?2018-01-25T07:57:01Z2018-02-05T15:24:36ZTrue
MDU6SXNzdWUyOTE1ODUwNjk=Running mean-variance during first batches2018-01-25T14:18:04Z2018-02-05T16:21:53ZTrue
MDU6SXNzdWUyOTE3MDMwODk=Too much GPU memory occupied by ACKTR2018-01-25T20:21:55ZFalse
MDU6SXNzdWUyOTIzNDA4MDc=TypeError: 'LazyFrames' object is not subscriptable2018-01-29T09:42:23Z2018-01-29T09:50:41ZTrue
MDU6SXNzdWUyOTIzODAwNTU=PPO1 does not work on 'Pendulum-v0'2018-01-29T11:55:51ZFalse
MDU6SXNzdWUyOTMxMjU0MzM=NotImplementedError when executing Pong example2018-01-31T11:34:29Z2018-02-01T10:45:23ZTrue
MDU6SXNzdWUyOTM1MDkxNjU=Getting information from info doesn't seem to work anymore2018-02-01T12:17:24Z2018-03-01T12:11:37ZTrue
MDU6SXNzdWUyOTM5NzEwMzA=ImportErrorTraceback  import baselines.common.tf_util as U2018-02-02T17:51:41ZFalse
MDU6SXNzdWUyOTQxMzEwNDA=AttributeError: module 'baselines.common.tf_util' has no attribute 'logit_bernoulli_entropy'2018-02-03T17:52:34Z2018-02-05T15:53:43ZTrue
MDU6SXNzdWUyOTQxNjYzNDA=DDPG seed replicability issue2018-02-04T03:27:35ZFalse
MDU6SXNzdWUyOTQyMjI3MjY=a2c/run_atari.py module error2018-02-04T19:25:33Z2018-02-05T10:11:43ZTrue
MDU6SXNzdWUyOTQzNDMzNTU=Reason for train_freq=4 for DQN?2018-02-05T10:30:35Z2018-02-05T16:12:11ZTrue
MDU6SXNzdWUyOTQ0NTg0NDM=Exploration in PPO2018-02-05T16:35:26Z2018-02-05T20:46:35ZTrue
MDU6SXNzdWUyOTYwNzAwNTU=Need reuse and scope parameters in deepq.simple.learn2018-02-10T03:43:56ZFalse
MDU6SXNzdWUyOTYyMDIzOTA=No comments in A2C code, very hard to read2018-02-11T16:20:58ZFalse
MDU6SXNzdWUyOTYzNDc1OTI=__init__() got an unexpected keyword argument 'dtype'2018-02-12T11:46:23Z2018-02-12T13:13:23ZTrue
MDU6SXNzdWUyOTY3OTc2NTA=ModuleNotFoundError: No module named 'gym.monitoring'2018-02-13T16:20:05ZFalse
MDU6SXNzdWUyOTgwMjg0NjI=fatal error: mjmodel.h: No such file or directory2018-02-17T19:10:16ZFalse
MDU6SXNzdWUyOTgxMjkyNTQ=gym.error.DeprecatedEnv: Env HalfCheetah-v1 not found2018-02-18T23:25:43ZFalse
MDU6SXNzdWUyOTg4NDMzNTY=Is LSTM policy in PPO2 working correctly?2018-02-21T05:05:52ZFalse
MDU6SXNzdWUyOTk2ODE5NTE=New LazyFrames is broken2018-02-23T11:30:07ZFalse
MDU6SXNzdWUyOTk5NjA4MTA=module baselines.common.filters not present2018-02-24T18:15:35ZFalse
MDU6SXNzdWUzMDAyNDcxODE=param noise--AttributeError: module 'baselines.common.tf_util' has no attribute 'scope_vars'2018-02-26T13:55:08Z2018-02-27T10:00:21ZTrue
MDU6SXNzdWUzMDAzNjUzODI=trpo_mpi.py: parallelism issue with fisher_vector_product2018-02-26T19:28:50Z2018-02-26T19:35:51ZTrue
MDU6SXNzdWUzMDA1NTg2OTg=Error Saving ACER/A2C Model2018-02-27T09:47:41Z2018-03-01T18:45:05ZTrue
MDU6SXNzdWUzMDA1OTM3Nzc=The Atari pretrained model downloader be removed?2018-02-27T11:29:14ZFalse
MDU6SXNzdWUzMDA5NjQxNTM=which python version support?2018-02-28T10:10:28Z2018-02-28T10:34:14ZTrue
MDU6SXNzdWUzMDEwOTA1ODM=Why memory addresses that belong to different processes are the same ?2018-02-28T16:25:56ZFalse
MDU6SXNzdWUzMDExMTEwNjQ=HER requires 'mujoco_py2018-02-28T17:21:57Z2018-03-05T10:08:46ZTrue
MDU6SXNzdWUzMDEyNzAyMTU=why subproc_vec reset the env when step returns done=True?2018-03-01T04:00:06Z2018-03-08T17:45:06ZTrue
MDU6SXNzdWUzMDE0NjM4ODA=exceptions inside deepq model and agent are suppressed2018-03-01T16:11:38Z2018-03-01T17:56:19ZTrue
MDU6SXNzdWUzMDE1MDcxMDc=gym.error.UnregisteredEnv: No registered env with id: FetchReach-v02018-03-01T18:16:05Z2018-03-01T18:24:43ZTrue
MDU6SXNzdWUzMDE2MzgxNTE=Bug in her.experiment.plot2018-03-02T03:07:06Z2018-03-05T10:07:14ZTrue
MDU6SXNzdWUzMDE5MTQ0NTA=Unable to reproduce HER results from Plappert et al., 20182018-03-02T21:43:14Z2018-03-05T10:06:40ZTrue
MDU6SXNzdWUzMDE5MzIxNDU=Version of gym for a2c2018-03-02T22:58:36ZFalse
MDU6SXNzdWUzMDIwMjk1Mzk=Understanding multi iteration update of model in PPO2018-03-03T20:27:42Z2018-03-05T09:31:00ZTrue
MDU6SXNzdWUzMDIxOTkzMDQ=PPO for Continuous action space for atari2018-03-05T07:43:05Z2018-03-05T08:31:29ZTrue
MDU6SXNzdWUzMDIyMDQzMDk=Problem installing2018-03-05T08:07:13ZFalse
MDU6SXNzdWUzMDIyNDk2NjE=Why tanh is chosen as the activation function?2018-03-05T10:40:03ZFalse
MDU6SXNzdWUzMDI0NzQ2OTU=Problem in using SubprocVecEnv for training custom gym envs 2018-03-05T21:42:46ZFalse
MDU6SXNzdWUzMDQxMTU2MjY=PPO2 Normalizing advantage across multiple episodes2018-03-10T23:40:08ZFalse
MDU6SXNzdWUzMDQxNzk5NDM=HER, out of memory using 3 or more CPUs and one GPU2018-03-11T18:12:20Z2018-03-20T09:09:08ZTrue
MDU6SXNzdWUzMDQxOTcxMzQ=HER, dealing with overload-allowed in mpirun commands2018-03-11T21:44:21Z2018-03-20T09:12:35ZTrue
MDU6SXNzdWUzMDQ0MDU1NTA=How to use trained A2C agent2018-03-12T14:55:33ZFalse
MDU6SXNzdWUzMDQ0OTI0NTU=Deepq no atari folder, now download_model2018-03-12T18:38:01ZFalse
MDU6SXNzdWUzMDUyODEyNjk=How to play games with trained agents2018-03-14T18:36:32ZFalse
MDU6SXNzdWUzMDU1MjE2ODE=policies of PPO2 contains relu as activation function in pi and vf2018-03-15T11:55:45Z2018-03-15T12:09:19ZTrue
MDU6SXNzdWUzMDU2OTM5Mzg=Bug in Acer Runner2018-03-15T19:43:05Z2018-03-15T20:34:56ZTrue
MDU6SXNzdWUzMDYyMzU0Nzc=How to run Cartpole using trpo?2018-03-18T12:20:17Z2018-03-21T06:43:53ZTrue
MDU6SXNzdWUzMDYyNzMxOTU=GAIL behavior cloning missing save_state bug2018-03-18T19:25:14Z2018-05-21T22:27:29ZTrue
MDU6SXNzdWUzMDcyMzUxNTU=Using multiple CPUs with HER fails2018-03-21T13:02:08Z2018-04-05T14:06:33ZTrue
MDU6SXNzdWUzMDc1Mzg4NDY= baselines/baselines/trpo_mpi difference between value functions for mujoco vs atari2018-03-22T07:55:32ZFalse
MDU6SXNzdWUzMDc3OTAzMzI=typo in acktr_cont.py line 72018-03-22T19:49:36ZFalse
MDU6SXNzdWUzMDgyMjg2NDQ=Is the lstm states evolution in ppo2.py correct?2018-03-24T04:35:35ZFalse
MDU6SXNzdWUzMDgyNzE0Njg=Reason for denormalizing critic value in DDPG2018-03-24T15:27:24ZFalse
MDU6SXNzdWUzMDg1MTk5MzU=Missing opencv dependency2018-03-26T10:41:52Z2018-05-14T17:52:37ZTrue
MDU6SXNzdWUzMDg1NDk1OTk=No module named util2018-03-26T12:23:25Z2018-04-23T21:48:09ZTrue
MDU6SXNzdWUzMDkwMzEwMjc=Odd value for rolling observations in VecFrameStack2018-03-27T15:56:46Z2018-04-12T16:19:33ZTrue
MDU6SXNzdWUzMTAyMzI5NDM=PrioritizedReplayBuffer - assert alpha > 0?2018-03-31T11:08:44Z2018-05-01T23:58:01ZTrue
MDU6SXNzdWUzMTA0NDA4OTY=Change PPO2 display/output 2018-04-02T09:34:05ZFalse
MDU6SXNzdWUzMTA1ODg2Nzg=Rollout in her is not parallelized2018-04-02T19:49:41ZFalse
MDU6SXNzdWUzMTE0MDk4MTM=Why does a2c implement an MLP network policy option?2018-04-04T22:15:43Z2018-04-13T00:23:32ZTrue
MDU6SXNzdWUzMTE1NTcxNTY=Fix bug in Deepq module import error2018-04-05T10:33:28Z2018-05-03T00:56:56ZTrue
MDU6SXNzdWUzMTI2NjEwMTM=ConnectionResetError with multiprocessing in A2C2018-04-09T19:43:02Z2018-04-12T23:55:57ZTrue
MDU6SXNzdWUzMTMxMzc2MDg=tmux ctrl_b  Prevent her  program from running2018-04-11T00:55:04ZFalse
MDU6SXNzdWUzMTMxODA2MzA=pip install -e . failed2018-04-11T05:42:04ZFalse
MDU6SXNzdWUzMTMxOTI0NDY=Load the model2018-04-11T06:44:00Z2018-04-13T02:12:44ZTrue
MDU6SXNzdWUzMTMyOTg3NTI=Initial observations are zero in A2C 2018-04-11T12:18:29Z2018-05-01T23:56:49ZTrue
MDU6SXNzdWUzMTM2NDk1MDM=PPO2 sample action strategy?2018-04-12T09:37:46Z2018-04-12T16:16:58ZTrue
MDU6SXNzdWUzMTM2NTE0MDU=Using Vectorized environement2018-04-12T09:43:09Z2018-04-17T21:26:53ZTrue
MDU6SXNzdWUzMTM3NTk4MjE=PPO2 very slow2018-04-12T14:46:27Z2018-05-28T10:57:36ZTrue
MDU6SXNzdWUzMTM4MzA0NjI=PPO update schedule2018-04-12T18:02:21Z2018-04-12T18:25:03ZTrue
MDU6SXNzdWUzMTM5NTIzMDA=A2C does not normalize either advantage estimate or returns ? -> Loss explosion2018-04-13T02:52:05ZFalse
MDU6SXNzdWUzMTQxODgwMzU=PPO2 - tuple index out of range when using Discrete action state. 2018-04-13T17:21:49Z2018-05-01T23:55:35ZTrue
MDU6SXNzdWUzMTQzMzg4OTQ=HER: with --num_cpu 19, still unable to reproduce results from Plappert et al., 20182018-04-14T15:45:53Z2018-04-14T18:11:27ZTrue
MDU6SXNzdWUzMTQ0Nzg4MDg=Default value of 'stochastic' argument in deepq build_act()2018-04-16T02:16:35ZFalse
MDU6SXNzdWUzMTQ1MDgwNjU=PPO1 and A2C2018-04-16T05:39:58Z2018-04-17T21:24:58ZTrue
MDU6SXNzdWUzMTQ3OTM0OTY=HER multi cpus mpi run error2018-04-16T19:50:19Z2018-05-21T22:27:16ZTrue
MDU6SXNzdWUzMTU1MDg1ODc=Render an Environment Using A2C?2018-04-18T14:37:23Z2018-04-20T15:42:36ZTrue
MDU6SXNzdWUzMTU4MDg4NzM=PPO2: policy entropy grows over time2018-04-19T10:00:07ZFalse
MDU6SXNzdWUzMTU4OTUwMTg=Why adding .5 whereas a `vf_coef` is used for the loss?2018-04-19T14:09:59Z2018-05-04T18:39:43ZTrue
MDU6SXNzdWUzMTYwNDY3MDE=TypeError: Can't instantiate abstract class SubprocVecEnv with abstract methods step_async, step_wait2018-04-19T21:25:49Z2018-05-08T00:59:25ZTrue
MDU6SXNzdWUzMTYzNTQ3NzA=How can I run PPO2 on GPU ?2018-04-20T17:43:14Z2018-05-03T00:24:05ZTrue
MDU6SXNzdWUzMTcwNDAxNzk=module 'baselines.common.tf_util' has no attribute 'dense'2018-04-24T01:39:22Z2018-09-10T22:41:27ZTrue
MDU6SXNzdWUzMTczMTU2MDA=[A2C] The initial observation is wrong?2018-04-24T17:04:52Z2018-04-28T13:54:57ZTrue
MDU6SXNzdWUzMTc3Nzc4OTc=Support for Fetch environments?2018-04-25T20:15:56Z2018-05-03T17:58:18ZTrue
MDU6SXNzdWUzMTc4OTY5MDU=PPO2 render after loading trained model2018-04-26T06:42:55Z2018-05-21T22:25:41ZTrue
MDU6SXNzdWUzMTgzMDMyOTM=HER reshape error2018-04-27T07:19:45Z2018-04-27T07:25:48ZTrue
MDU6SXNzdWUzMTg2MTMwNzE=[A2C] Bug zeroing out done observations?2018-04-28T08:18:00Z2019-01-10T06:31:06ZTrue
MDU6SXNzdWUzMTg2NDIzOTU=Any pretrained agents for the MuJoCo envs for OpenAI/baselines?2018-04-28T14:47:07ZFalse
MDU6SXNzdWUzMTkzODY0NDI=the commit 2 hours ago have issue with importing2018-05-02T02:03:10Z2018-05-03T05:07:44ZTrue
MDU6SXNzdWUzMTk2NjA4ODQ=atari_py not found2018-05-02T18:21:43Z2018-05-12T01:10:53ZTrue
MDU6SXNzdWUzMTk3MDE5ODU=A2C saving and loading issues2018-05-02T20:33:34Z2018-08-13T20:10:38ZTrue
MDU6SXNzdWUzMTk3MzI3MzE=No Module named baselines.common.runners2018-05-02T22:24:15Z2018-05-03T05:07:52ZTrue
MDU6SXNzdWUzMjAwMDE1NzI="Implement ""parallel"" algorithm for computing variance"2018-05-03T16:56:26ZFalse
MDU6SXNzdWUzMjAwNjgxNjY=A2C Loss Function Value vs Policy2018-05-03T20:27:43Z2018-05-03T21:17:58ZTrue
MDU6SXNzdWUzMjA2NTUyNzc=Fix bug in Deepq module import error2018-05-07T02:27:49Z2018-05-18T05:47:53ZTrue
MDU6SXNzdWUzMjEwMzQyMzg=PPO: run atari. go out OOM2018-05-08T03:56:47ZFalse
MDU6SXNzdWUzMjExMDY1MTQ=frames vs. step in --num-timestep flag2018-05-08T09:18:26Z2018-05-09T22:41:20ZTrue
MDU6SXNzdWUzMjEyODI4MjI=Running an LSTM Policy A2C Trained Model2018-05-08T17:47:04ZFalse
MDU6SXNzdWUzMjEzMzExNTA=AC2 MLP Policy pdparam2018-05-08T20:15:17Z2018-05-09T12:52:42ZTrue
MDU6SXNzdWUzMjE2MDA2MDM=Improving code quality 2018-05-09T14:45:34ZFalse
MDU6SXNzdWUzMjIwNjE2NDc="Integration of the ""Distributed evolution"" algorithm"2018-05-10T19:53:27ZFalse
MDU6SXNzdWUzMjIxNTA0Nzc=PPO2 loading checkpoint issue2018-05-11T02:56:13ZFalse
MDU6SXNzdWUzMjIzOTkxNjU=AC2 Dones Population2018-05-11T18:53:40Z2019-02-20T12:27:31ZTrue
MDU6SXNzdWUzMjI1MDQ5ODE=DDPG eval return much lower than rollout return2018-05-12T11:46:48ZFalse
MDU6SXNzdWUzMjI1NDY4NjQ=HER- fatal error: mjmodel.h: No such file or directory2018-05-12T22:15:10Z2018-05-14T18:35:04ZTrue
MDU6SXNzdWUzMjMzOTU3NTE=PPO2 value function clipping2018-05-15T21:48:32Z2018-05-19T13:57:36ZTrue
MDU6SXNzdWUzMjQ2NDczNjk=Trained networks for experimentation2018-05-19T17:46:36ZFalse
MDU6SXNzdWUzMjYzNTI2Mjg=Robotics env. Failure to record video2018-05-25T01:42:22ZFalse
MDU6SXNzdWUzMjY4MDE1Mjc=A2C some feedback2018-05-27T09:28:19ZFalse
MDU6SXNzdWUzMjcxNjU1NDE=ppo2 (env humanoid-v2) does not work with default parameters2018-05-29T04:21:35ZFalse
MDU6SXNzdWUzMjcxOTgzMTc=[TRPO] Is assign_old_eq_new() called at the right place?2018-05-29T07:23:37Z2018-06-08T01:15:35ZTrue
MDU6SXNzdWUzMjg1ODc3NzM=Consider creating a tag for releases2018-06-01T16:42:25Z2019-05-08T00:43:18ZTrue
MDU6SXNzdWUzMjg4NzM4NjA=A3C implementation2018-06-03T23:53:38Z2018-06-03T23:54:04ZTrue
MDU6SXNzdWUzMjkyMTc0OTM=Terminating episodes in HER2018-06-04T21:07:46ZFalse
MDU6SXNzdWUzMjk2ODI4ODk=neglogac in a2c.py2018-06-06T01:55:54Z2018-06-06T18:06:30ZTrue
MDU6SXNzdWUzMzAwNDM4Mjk=What is a good hyperparameter setting for DDPG on HalfCheetah?2018-06-06T21:43:35ZFalse
MDU6SXNzdWUzMzA0MjA4MjI=deepq training breaks on loops2018-06-07T20:18:23Z2018-06-07T20:38:51ZTrue
MDU6SXNzdWUzMzA0NTYyODA=HER does not converge on simple envs that adhere to GoalEnv interface2018-06-07T22:21:31ZFalse
MDU6SXNzdWUzMzA0OTQ3NDE=Missing module2018-06-08T02:08:28Z2018-06-08T16:42:16ZTrue
MDU6SXNzdWUzMzA3MjU1ODA=Default hyperparamters for `run_atari.py` (using P-DDDQN) fail with Pong and Breakout (log files attached)2018-06-08T16:33:43ZFalse
MDU6SXNzdWUzMzA3NDI4MzY=Load Keras model as policy in PPO2018-06-08T17:32:40Z2018-06-18T20:43:24ZTrue
MDU6SXNzdWUzMzA3OTgzNzU=Should not update running mean in TRPO / GAIL2018-06-08T20:49:32ZFalse
MDU6SXNzdWUzMzA4ODMwOTM=ppo1.run_humanoid RuntimeWarning: invalid value encountered in sqrt2018-06-09T12:31:23Z2018-06-10T12:50:24ZTrue
MDU6SXNzdWUzMzA5NjE0NTE=PPO1 does not stack frame for Atari by default2018-06-10T11:38:07ZFalse
MDU6SXNzdWUzMzEwOTEyNjg=Wrong bootstrap in baselines DDPG ?2018-06-11T08:29:34ZFalse
MDU6SXNzdWUzMzE5NDU2OTE=Save policy using PPO1 baseline2018-06-13T11:02:12ZFalse
MDU6SXNzdWUzMzI1MTk5NDg=Trouble saving model for in run_atari deepq2018-06-14T18:44:30ZFalse
MDU6SXNzdWUzMzI5NjczMzc=Expert data in GAIL can't be downloaded2018-06-16T07:18:37ZFalse
MDU6SXNzdWUzMzI5NzkzMzY=Choice of value loss function in policy gradients2018-06-16T10:56:31ZFalse
MDU6SXNzdWUzMzMwNjQ1NTA=PPO some feedback2018-06-17T13:54:33ZFalse
MDU6SXNzdWUzMzM0MzQ4MTU=Why multiply mean by 0.0 in PPO1 mlp_policy?2018-06-18T21:06:21Z2018-09-10T22:26:27ZTrue
MDU6SXNzdWUzMzM5MTcwNTg=Cannot run deepq.experiments.run_atari with gpu?2018-06-20T04:06:30Z2018-06-21T06:20:19ZTrue
MDU6SXNzdWUzMzQ4MzIxODQ=Support for MultiDiscrete and MultiContinuous action spaces in A2C2018-06-22T10:42:38ZFalse
MDU6SXNzdWUzMzUzMTg0Mzc=Downloading pretrained agents for DQN and updating Readme for deepq2018-06-25T09:02:09ZFalse
MDU6SXNzdWUzMzU3NTgzNjA=Reproduce HER with 10 cores CPU?2018-06-26T10:37:38Z2018-07-27T04:47:00ZTrue
MDU6SXNzdWUzMzU4NjcwMjU=Is [Clipped Surrogate Objective] in PPO simply ignoring the clipped terms?2018-06-26T15:11:11ZFalse
MDU6SXNzdWUzMzYyNTEwNzU=Adversary loss function2018-06-27T14:19:48Z2018-06-27T16:15:48ZTrue
MDU6SXNzdWUzMzY1NDMyNjI=ppo2: explained variance will decrease  to negative sometimes2018-06-28T09:28:24Z2019-03-04T08:03:54ZTrue
MDU6SXNzdWUzMzY4NTQ2NzQ=Theoretical question about convergence2018-06-29T03:56:01ZFalse
MDU6SXNzdWUzMzY4OTgwNjI=ppo2: approxkl increase dramatically, but clipfrac decrease2018-06-29T07:47:09Z2018-07-27T00:58:30ZTrue
MDU6SXNzdWUzMzc2MzYxMzI=looking for a module that generated expert demonstration data2018-07-02T19:31:20Z2018-07-31T19:01:20ZTrue
MDU6SXNzdWUzMzkyNDYzMTY=why every algo reimplements policies that show up in others as well2018-07-08T18:23:47Z2018-08-14T22:40:04ZTrue
MDU6SXNzdWUzMzk1Njg2NTc=ddpg strange behaviour using mpirun2018-07-09T18:59:24ZFalse
MDU6SXNzdWUzMzk4Nzc1OTc=Running PPO2 on multiple CPU cores 2018-07-10T14:49:31Z2018-07-10T16:18:13ZTrue
MDU6SXNzdWUzNDAwMzc0MjU=LSTM policy in ppo22018-07-10T23:09:43Z2018-08-14T00:34:55ZTrue
MDU6SXNzdWUzNDA0NzAwMjc=HER appear segmentation error2018-07-12T02:45:11ZFalse
MDU6SXNzdWUzNDA3MDc2OTc=ppo2.py not updating act model policy from old policy   2018-07-12T16:19:22Z2018-08-14T00:40:42ZTrue
MDU6SXNzdWUzNDExNTg1MDM=HER train success rate is lower than test success rate2018-07-13T21:01:20Z2018-07-18T17:25:02ZTrue
MDU6SXNzdWUzNDEyODkyODE=A2C and ACKTR do not output their scores2018-07-15T03:38:04Z2019-03-28T16:21:48ZTrue
MDU6SXNzdWUzNDI0ODYzODY=policy entropy in PPO22018-07-18T20:48:33ZFalse
MDU6SXNzdWUzNDMyODQyNTk=PPO2 not converge on continuous env Pendulum-v02018-07-21T01:07:32ZFalse
MDU6SXNzdWUzNDM1NTM4NzE=ppo2: what's the point of create 2 network?2018-07-23T09:24:01Z2018-07-23T09:25:14ZTrue
MDU6SXNzdWUzNDM1NTczNDQ=ppo2: what's the point of creating 2 network2018-07-23T09:33:27Z2018-07-27T00:51:07ZTrue
MDU6SXNzdWUzNDM3ODcwNTQ=DDPG epoch_episode_rewards container mis-placed in training.py?2018-07-23T20:39:19ZFalse
MDU6SXNzdWUzNDQxMjA1OTY=Question about neglogp in PPO2 policies2018-07-24T16:51:06Z2018-08-20T16:07:18ZTrue
MDU6SXNzdWUzNDQxMzgwMjQ=Doubts regarding step model2018-07-24T17:45:41Z2018-07-27T05:12:10ZTrue
MDU6SXNzdWUzNDQ0NDIwNjk=DQN can't work well in Pong and Breakout2018-07-25T13:24:14Z2018-07-27T02:27:54ZTrue
MDU6SXNzdWUzNDQ2ODAyOTk=Why does ppo2_mujoco only use a single process to a env? 2018-07-26T03:20:28Z2018-09-10T22:35:13ZTrue
MDU6SXNzdWUzNDUxODczNDE=ddpg throws nans for environments different than halfcheetah-v12018-07-27T11:12:50ZFalse
MDU6SXNzdWUzNDUyNTg1MzE=DDPG with ou_0.2 noise does not converge in MountainCarContinuous-v02018-07-27T14:57:30ZFalse
MDU6SXNzdWUzNDUzMzUyMDc=[PPO2] Agent learning schedule destabilizes after ~420k timesteps2018-07-27T19:07:22Z2018-08-24T16:21:50ZTrue
MDU6SXNzdWUzNDUzODE5NjU=test script for ddpg performance? 2018-07-27T22:18:05ZFalse
MDU6SXNzdWUzNDU0NjIyNTE=[PPO2] What’s the difference between PPO1 and PPO2 ?2018-07-28T16:43:30Z2018-08-17T00:26:48ZTrue
MDU6SXNzdWUzNDU0NzU0MzY=PPO2 implementation question2018-07-28T20:07:49ZFalse
MDU6SXNzdWUzNDU1MTg5MDM=DDPG hyperparameter suggestion for roboschool envs2018-07-29T10:13:53ZFalse
MDU6SXNzdWUzNDU4NDIxMzM=TypeError: 'numpy.float64' in GAIL dataset loading2018-07-30T16:47:20ZFalse
MDU6SXNzdWUzNDY1NjI0NTQ=Confused about Her+DDPG policy-loss2018-08-01T11:37:01Z2018-08-07T11:10:53ZTrue
MDU6SXNzdWUzNDc2MTE3MzE=A2C code for MuJoCo2018-08-04T11:25:13Z2018-08-14T00:25:04ZTrue
MDU6SXNzdWUzNDc5MzcwMjg=How to improve the success rate2018-08-06T14:02:16ZFalse
MDU6SXNzdWUzNDgwMTA5NTE='async' is a reserved word in Python >= 3.72018-08-06T17:16:55Z2018-09-21T02:16:39ZTrue
MDU6SXNzdWUzNDgwMjY4NzI=PPO not working for robotics tasks 2018-08-06T18:06:34ZFalse
MDU6SXNzdWUzNDkxMzcyOTc=Which HER implementation is more stable/usable?2018-08-09T13:52:08ZFalse
MDU6SXNzdWUzNDk4MDY1MDk=How to design LSTM policy for the mujoco env ?2018-08-12T11:52:02ZFalse
MDU6SXNzdWUzNDk4NTg4MzU=how to play GAIL result in mujoco2018-08-13T00:52:15ZFalse
MDU6SXNzdWUzNTA5Nzc2MDc=Run script fails on Classic Control2018-08-15T21:41:44Z2018-08-16T20:33:03ZTrue
MDU6SXNzdWUzNTA5ODc1MDI=Classic Control does not implement bench.Monitor so PPO2 does not report rewards2018-08-15T22:18:47Z2018-08-16T20:33:25ZTrue
MDU6SXNzdWUzNTEwMjYxNTQ=Several issues with deepq serialization2018-08-16T01:47:27Z2018-08-16T20:33:14ZTrue
MDU6SXNzdWUzNTEwNjY0Nzc=Where is run_atari?2018-08-16T06:03:23Z2018-08-17T00:09:50ZTrue
MDU6SXNzdWUzNTEwNjc2MDc=CompileError GCC - can't run several files2018-08-16T06:09:11Z2018-08-21T17:09:26ZTrue
MDU6SXNzdWUzNTEwOTY1MjQ=ImportError using run_atari in ACKTR 2018-08-16T08:00:12Z2018-08-20T06:33:43ZTrue
MDU6SXNzdWUzNTEyNzkxNjM=[PPO2] No module named baselines.ppo2.run_mujoco2018-08-16T16:28:08Z2018-08-17T00:08:02ZTrue
MDU6SXNzdWUzNTEzNTk5NDM=PPO2 has fixed batch size in observation placeholder, disallowing querying the action for a single observation.2018-08-16T20:39:03ZFalse
MDU6SXNzdWUzNTE0NTIxOTg=There is a clerical error 2018-08-17T04:23:04Z2018-08-17T20:57:31ZTrue
MDU6SXNzdWUzNTE2MDg2NTY=TypeError: conv() got an unexpected keyword argument 'num-timesteps'2018-08-17T14:06:40Z2018-08-17T20:58:27ZTrue
MDU6SXNzdWUzNTE2MTk1NTE=VecEnv SubProcEnv2018-08-17T14:35:55Z2018-09-11T22:25:13ZTrue
MDU6SXNzdWUzNTE4MTMyOTY=TypeError: learn() missing 1 required positional argument: 'network'2018-08-18T11:27:28Z2018-08-27T23:44:25ZTrue
MDU6SXNzdWUzNTE4MzUyODQ=[Solved] Error using baselines.results_plotter 2018-08-18T16:56:35ZFalse
MDU6SXNzdWUzNTE4NDM2NzQ="Question: Commit id and hyper-parameter settings for results in ""benchmarks_atari10M.htm"""2018-08-18T19:00:56Z2018-08-18T21:02:08ZTrue
MDU6SXNzdWUzNTE4OTQ0ODM=Expanding bencmark for Mujoco1M with DDPG, ACKTR and A2C2018-08-19T11:14:44ZFalse
MDU6SXNzdWUzNTE5MjU4MDM=Trouble saving and loading a model 2018-08-19T18:58:56Z2018-08-20T03:04:27ZTrue
MDU6SXNzdWUzNTE5MzI2NzU=LSTM/RNN code is confusing2018-08-19T20:40:01Z2018-08-20T20:56:48ZTrue
MDU6SXNzdWUzNTE5MzQwNzA=PPO1 and Distributed Training2018-08-19T21:01:01Z2018-09-11T18:45:05ZTrue
MDU6SXNzdWUzNTIxMzE1MTM=sed2018-08-20T13:18:55Z2018-08-20T13:38:32ZTrue
MDU6SXNzdWUzNTIyMTk4MDM=Normalize state values?2018-08-20T17:04:26Z2018-08-20T21:09:13ZTrue
MDU6SXNzdWUzNTI3Mjg1MjI=PPO2 selects random actions on the test run2018-08-21T22:05:45ZFalse
MDU6SXNzdWUzNTI5MTE5Njg=Issue with openai robotics environments2018-08-22T11:28:53ZFalse
MDU6SXNzdWUzNTM1ODc0MjA=save_interval in PPO22018-08-23T23:12:10Z2018-09-17T08:48:46ZTrue
MDU6SXNzdWUzNTM2ODE2ODM=Gamma in VecNormalize for rms updates.2018-08-24T07:53:08Z2018-09-14T23:08:19ZTrue
MDU6SXNzdWUzNTM4OTIzNjI=Unknown env_type box2d2018-08-24T18:55:54Z2018-09-06T17:23:06ZTrue
MDU6SXNzdWUzNTM5MTAwMDM=TensorFlow tries allocating 0 bytes2018-08-24T20:01:25Z2018-09-08T21:07:57ZTrue
MDU6SXNzdWUzNTQyMDQ3MzM=Why does ScaledFloatFrame wrapper gives HUGE boost in convergence speed?2018-08-27T07:07:55ZFalse
MDU6SXNzdWUzNTQzOTYzNjA=Understanding normalization of advantage function2018-08-27T16:57:08ZFalse
MDU6SXNzdWUzNTQ2OTg2MDU=ACKTR on MuJoCo2018-08-28T12:00:21ZFalse
MDU6SXNzdWUzNTQ4MDExNDM=Small Control values in PPO2018-08-28T16:08:06ZFalse
MDU6SXNzdWUzNTUxODQ4Mjk=ValueError: Variable ppo2_model/pi/c1/w/Adam/ already exists2018-08-29T14:21:51ZFalse
MDU6SXNzdWUzNTUyNDI1MTc=DummyVecEnv always use render_mode == 'rgb_array'2018-08-29T16:37:10Z2018-09-04T17:29:36ZTrue
MDU6SXNzdWUzNTUyNzM5NjE=Distributed training discrepancy from original PPO paper2018-08-29T18:09:37Z2018-08-30T00:31:45ZTrue
MDU6SXNzdWUzNTUyODQ4MDU=[Question] A2C entropy of the policy2018-08-29T18:41:18Z2018-08-31T17:44:38ZTrue
MDU6SXNzdWUzNTU2MDA0NTI=what's the difference between the same game with different version : v0,...,v42018-08-30T14:03:15Z2019-01-29T04:53:01ZTrue
MDU6SXNzdWUzNTU2NTczNDA=Tensorboard logging2018-08-30T16:15:12Z2018-09-06T17:20:22ZTrue
MDU6SXNzdWUzNTU3OTc5Mjc=Support Tuple and Dict spaces 2018-08-31T00:19:39ZFalse
MDU6SXNzdWUzNTU5ODk4NTU=deepq and trpo_mpi call two resets consecutively, and resents ent-coeffs2018-08-31T13:52:56ZFalse
MDU6SXNzdWUzNTYwMjYzMTc=HalfCheetah Benchmark Results for TRPO - Unable to reproduce2018-08-31T15:27:32Z2018-09-12T00:28:21ZTrue
MDU6SXNzdWUzNTYyNjg2Mzk=Baselines on Windows installation failure: TensorFlow AssertionError2018-09-02T09:45:27Z2018-12-19T01:27:47ZTrue
MDU6SXNzdWUzNTcxNjQ0MDY=Updates to `deepq ` make it not runnable any more!2018-09-05T10:26:49Z2018-09-10T18:51:16ZTrue
MDU6SXNzdWUzNTc0NjQyMjM=There is no save model option for DDPG2018-09-06T01:38:52Z2018-09-13T06:01:39ZTrue
MDU6SXNzdWUzNTc2MDU0MjM=Pong + EpisodicLifeEnv2018-09-06T11:05:46ZFalse
MDU6SXNzdWUzNTgxNzI2Mjk=Issues about testing Ddpg2018-09-07T18:46:28ZFalse
MDU6SXNzdWUzNTgxNzg0ODQ=open MPI error: A system-required executable either could not be found or was not executable by this user in file ess_singleton_module.c at line 4582018-09-07T19:06:16Z2018-09-12T18:10:36ZTrue
MDU6SXNzdWUzNTgzNDMwMDg=SubProcEnv and TRPO issues in traj_segment_generator2018-09-09T02:17:51Z2018-09-10T20:02:41ZTrue
MDU6SXNzdWUzNTg1NjY2OTI=Problem using PLAY when LSTM network is selected2018-09-10T10:42:20Z2018-09-17T21:33:49ZTrue
MDU6SXNzdWUzNTg4NjQ1NzQ=Where can I find the command --play corresponding code definition and analysis2018-09-11T02:39:40Z2018-09-11T23:43:08ZTrue
MDU6SXNzdWUzNTkxMjA0MDY=cannot train LSTM policy by PPO2 when mujoco env is selected2018-09-11T16:17:09ZFalse
MDU6SXNzdWUzNTkxMzEyNDg=Return True before conditional check in _check_shape (common/tf_util.py)2018-09-11T16:46:44Z2018-09-11T22:57:14ZTrue
MDU6SXNzdWUzNTkzMTAwMjg=Why the training of ppo2 never stop ?2018-09-12T04:20:46ZFalse
MDU6SXNzdWUzNTkzMjg5ODA=Some use cases for `VecMonitor` ?2018-09-12T06:10:12ZFalse
MDU6SXNzdWUzNTk3MzYyNjk=How to generater deterministic.ppo2...npz and stochastic.ppo2..npz as expert policy in gail2018-09-13T04:13:45ZFalse
MDU6SXNzdWUzNTk3NjkxODM=[RunningMeanStd]: Why use a small epsilon to initialize the `count` ?2018-09-13T07:09:11ZFalse
MDU6SXNzdWUzNTk3OTcyMzA=Error in using ppo2 to train HalfCheetah-v22018-09-13T08:40:10ZFalse
MDU6SXNzdWUzNjAxNTAzMTg=[Question]: Should `RunningMeanStd` more general than `RunningStat` ?2018-09-14T04:12:53ZFalse
MDU6SXNzdWUzNjA1NzUyNTI=Two links in readme don't work2018-09-15T20:57:09Z2018-09-19T22:05:11ZTrue
MDU6SXNzdWUzNjA1Nzk5OTA=unable to reproduce hindsight experience replay(HER) results2018-09-15T22:07:18ZFalse
MDU6SXNzdWUzNjA1OTI5NDI=How to make a new robotics env?2018-09-16T01:59:36Z2018-09-16T17:19:41ZTrue
MDU6SXNzdWUzNjA2MjUwNzQ=Is KL divergence between two MultiCategoricals correct?2018-09-16T10:19:09Z2018-09-20T00:29:01ZTrue
MDU6SXNzdWUzNjA3MzE0OTc=Why the her/experiment take so little memory of GPU?2018-09-17T06:00:38Z2018-09-20T00:41:06ZTrue
MDU6SXNzdWUzNjA5MDcwOTI=About Tensorboard2018-09-17T14:52:34ZFalse
MDU6SXNzdWUzNjEyNTg2OTc=Model Save/Load implementation2018-09-18T11:27:15ZFalse
MDU6SXNzdWUzNjE3MTc1MTA=PPO2 is not using the cpu cores effectively 2018-09-19T12:09:07ZFalse
MDU6SXNzdWUzNjE5OTE5MjA=PPO2 sample efficiency2018-09-20T02:12:54ZFalse
MDU6SXNzdWUzNjIxNzk4MzU=Question about FetchPickAndPlace-v2 task 2018-09-20T13:08:52Z2019-02-09T01:52:03ZTrue
MDU6SXNzdWUzNjIzODQxOTY="""kfac.py"""2018-09-20T22:01:23ZFalse
MDU6SXNzdWUzNjIzOTAxNTI=Why fixed the batch_size in policy?2018-09-20T22:25:16ZFalse
MDU6SXNzdWUzNjI0NzA5MDI=About the reward on A2C & ACKTR2018-09-21T06:25:10Z2019-03-28T16:21:49ZTrue
MDU6SXNzdWUzNjI1NzI1MjM=`SubprocVecEnv` speedup does not scale linearly compared with `DummyVecEnv`2018-09-21T11:27:16ZFalse
MDU6SXNzdWUzNjI2ODc2MDE=Install Baselines2018-09-21T16:22:17Z2018-09-24T19:57:36ZTrue
MDU6SXNzdWUzNjI4NTIzOTg=tf_util check shape won't work!2018-09-22T12:11:49ZFalse
MDU6SXNzdWUzNjI5NTM2OTc=PPO2:Total reward keep declining over time2018-09-23T15:55:23Z2018-10-23T14:57:11ZTrue
MDU6SXNzdWUzNjMwMjgyMTI=Learning Curve and Rendering Mismatch2018-09-24T06:40:14Z2018-09-30T14:42:30ZTrue
MDU6SXNzdWUzNjM1OTY0Njg=possible bug in baselines/ac2/runner.py ??2018-09-25T14:05:22ZFalse
MDU6SXNzdWUzNjM4NTA2NDM=Double logger initialisation2018-09-26T04:08:07ZFalse
MDU6SXNzdWUzNjM4NjI2OTQ=RuntimeError: received 0 items of ancdata with custom gym2018-09-26T05:23:20Z2018-10-10T22:38:33ZTrue
MDU6SXNzdWUzNjQzOTcxODU=failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED2018-09-27T10:02:36ZFalse
MDU6SXNzdWUzNjQ0NTUzOTg=Error using custom environment with the OpenAI baselines2018-09-27T12:38:08ZFalse
MDU6SXNzdWUzNjQ1ODYzNzU=ACER Models don't run ... dimension error?2018-09-27T17:49:51Z2018-10-24T17:33:39ZTrue
MDU6SXNzdWUzNjQ4NzQ3OTM=load_results method throws Monitor error2018-09-28T12:48:27Z2018-10-05T20:02:06ZTrue
MDU6SXNzdWUzNjQ5OTk3OTY=What is the version of mujoco and gym that is required to run a baseline code?2018-09-28T18:44:57Z2019-04-26T23:16:01ZTrue
MDU6SXNzdWUzNjUwMjYzMzI=Uncompiling Tests2018-09-28T20:17:06ZFalse
MDU6SXNzdWUzNjUyNjA5NTc=PPO2 does not converge on LunarLander-v22018-09-30T20:59:02Z2018-10-01T07:02:57ZTrue
MDU6SXNzdWUzNjUzNTg3NzY=Custom environnment2018-10-01T08:35:16ZFalse
MDU6SXNzdWUzNjU0MTE5ODY=VecNormalize vs Scaling2018-10-01T11:07:48Z2018-10-01T16:37:22ZTrue
MDU6SXNzdWUzNjU1NjcwMTA=PPO2 + VecNormalize 2018-10-01T17:37:26ZFalse
MDU6SXNzdWUzNjU1NjgyMTg=ppo2 atari not converging with policy=mlp 2018-10-01T17:40:27Z2018-10-16T17:36:13ZTrue
MDU6SXNzdWUzNjY2MDMyMjc=Modify code, no effect2018-10-04T02:57:05ZFalse
MDU6SXNzdWUzNjY3ODA3MjI=Bench Viewer Mujoco1M What Version Environments?2018-10-04T13:06:37Z2018-10-11T02:43:30ZTrue
MDU6SXNzdWUzNjc1ODIyNDE=Loaded a trained agent, ran envs in parallel, but one env2018-10-07T18:52:20Z2018-10-07T18:52:24ZTrue
MDU6SXNzdWUzNjc1ODM4OTY=Loaded a trained agent, ran envs in parallel, but one env's score is not counted/recorded (video proof attached)2018-10-07T19:12:37ZFalse
MDU6SXNzdWUzNjc1ODU3OTI=Error during cloudpickle for custom environment (TypeError: can't pickle select.epoll objects)2018-10-07T19:33:57Z2018-10-07T21:46:27ZTrue
MDU6SXNzdWUzNjc1ODgwMjE=SubprocVecEnv not working2018-10-07T19:58:49ZFalse
MDU6SXNzdWUzNjgzNzg2NDc=vecframestack stack dimension2018-10-09T20:15:41ZFalse
MDU6SXNzdWUzNjg0OTM3MDI=On DQN-based algorithms and parallel environments2018-10-10T04:51:14ZFalse
MDU6SXNzdWUzNjg4ODM4NzA=Delete.2018-10-10T22:41:53Z2018-10-15T22:07:15ZTrue
MDU6SXNzdWUzNjg4ODcwNDI=DDPG Loss function for Actor2018-10-10T22:54:54Z2018-10-18T20:44:52ZTrue
MDU6SXNzdWUzNjkwNjg2MDY=AttributeError: 'ScaledFloatFrame' object has no attribute 'num_envs'2018-10-11T11:06:17Z2018-10-23T16:58:45ZTrue
MDU6SXNzdWUzNjkxMzQwNDM=The updates of DDPG target network parameters in normalized layers2018-10-11T13:56:37ZFalse
MDU6SXNzdWUzNjk1Mjk2ODQ=Why the ag_2, g ......variables have 100 sets of data?2018-10-12T12:26:36ZFalse
MDU6SXNzdWUzNjk3NTAwNzU=PPO2 GPU usage?2018-10-13T00:40:43ZFalse
MDU6SXNzdWUzNjk3NzYzNDU=ddpg lose main.py2018-10-13T07:24:08Z2018-10-17T17:25:06ZTrue
MDU6SXNzdWUzNjk5MjE3MTA=Scope issues2018-10-14T17:01:39Z2018-10-20T00:39:06ZTrue
MDU6SXNzdWUzNzAwMjIyODQ=Question about PPO2 ratio calculation2018-10-15T06:52:33Z2018-10-17T01:11:30ZTrue
MDU6SXNzdWUzNzA4NTc2MDc=Enable multiple DQN and DDPG models to be used at the same time2018-10-17T01:04:49ZFalse
MDU6SXNzdWUzNzEzMzQzOTU=FileNotFoundError2018-10-18T02:16:48Z2018-10-24T18:01:22ZTrue
MDU6SXNzdWUzNzE0MjYwMDc="How to access the ""info"" that is generated during each env.step()"2018-10-18T08:54:20Z2018-10-18T23:50:43ZTrue
MDU6SXNzdWUzNzE0NjkzMDY=Arguments 'network' and 'seed' are not documented in the docstring of the `learn` function of deepq.py2018-10-18T10:43:31Z2018-10-20T00:51:17ZTrue
MDU6SXNzdWUzNzE2Njk2NjI=acktr with recurrent policies2018-10-18T19:07:59ZFalse
MDU6SXNzdWUzNzE3MzEwODc=How to use baselines without proprietary mujoco2018-10-18T22:01:26Z2018-10-18T23:40:12ZTrue
MDU6SXNzdWUzNzE3NjA3NzQ=[question] VecFrameStack with recurrent policies?2018-10-19T00:16:51ZFalse
MDU6SXNzdWUzNzIyMjM4OTI=[question] Compute Atari score and not reward2018-10-20T15:42:50Z2018-10-23T19:52:43ZTrue
MDU6SXNzdWUzNzI0MjE4MzU=Regarding computations of HER transitions.2018-10-22T07:40:26Z2019-04-20T11:40:12ZTrue
MDU6SXNzdWUzNzI4ODU1MzA=TypeError: wrap_deepmind() argument after ** must be a mapping, not NoneType2018-10-23T08:59:14Z2018-10-23T18:23:14ZTrue
MDU6SXNzdWUzNzMwOTY5MzQ=Cannot reproduce the benchmark results of DQN on Breakout2018-10-23T16:46:53Z2018-10-23T19:30:03ZTrue
MDU6SXNzdWUzNzMxMDQ2NDc=ACKTR MultiDiscrete2018-10-23T17:07:42ZFalse
MDU6SXNzdWUzNzM3NTM0MjU=Motion Planning with Fetch Environment2018-10-25T02:58:36ZFalse
MDU6SXNzdWUzNzQwMTE5ODQ=DDPG has unused arguments2018-10-25T15:42:17ZFalse
MDU6SXNzdWUzNzQ2Njg0Njc=pkg_resources.get_distribution('tensorflow') isn't strict2018-10-27T17:26:22Z2018-10-30T20:50:15ZTrue
MDU6SXNzdWUzNzUwNzE0OTk=Gradient normalization in HER MPI setting2018-10-29T15:24:21Z2018-11-02T19:58:53ZTrue
MDU6SXNzdWUzNzUxMTIxMTQ=Ensuring that all envs in a parallelized VecEnv are correctly reset2018-10-29T16:45:10Z2018-11-19T21:39:27ZTrue
MDU6SXNzdWUzNzU0MTk3NTY=DDPG with default hyper-paras doesn't work in mujoco swimmer-v2 env2018-10-30T10:33:45ZFalse
MDU6SXNzdWUzNzU2MDEwMDU=add ddpg parameter noise unit test cases2018-10-30T17:13:00Z2018-11-27T01:57:47ZTrue
MDU6SXNzdWUzNzU2MDU1ODE=Unify MpiAdam and MpiAdamOptimizer2018-10-30T17:23:16ZFalse
MDU6SXNzdWUzNzU5ODI4NjE=Update PyPi package2018-10-31T14:13:40Z2018-11-01T22:54:13ZTrue
MDU6SXNzdWUzNzYxNzg3Mzg=normalize_observations=True not working with MPI-parallelized PPO22018-10-31T22:06:06Z2018-11-02T02:14:13ZTrue
MDU6SXNzdWUzNzYyODY0Mjc=PPO2 does not seem to work on continuous env Pendulum-v02018-11-01T07:39:42Z2018-11-09T19:42:56ZTrue
MDU6SXNzdWUzNzYyOTk2MDQ=Train_Pong in the deepq/experiments does not converge2018-11-01T08:31:44Z2018-11-02T09:26:32ZTrue
MDU6SXNzdWUzNzY2NDE2OTA=Unify observation normalization code2018-11-02T02:13:27ZFalse
MDU6SXNzdWUzNzY4MTYzNzI=How does the save/restore model work?2018-11-02T13:46:21ZFalse
MDU6SXNzdWUzNzc0Njg2NjQ="""deepq"" or ""deepq_1"""2018-11-05T16:07:42Z2018-11-05T19:46:49ZTrue
MDU6SXNzdWUzNzg4NDQ5MzY=Cannot modify environments by MJCF files.2018-11-08T17:43:04Z2018-11-09T11:09:06ZTrue
MDU6SXNzdWUzNzkwNzEwMzU=Incomplete policy loss gradient in PPO22018-11-09T08:43:46Z2018-11-09T22:53:08ZTrue
MDU6SXNzdWUzNzk1NDIzODk=Parallel Environments 2018-11-11T17:10:09Z2018-11-13T18:45:52ZTrue
MDU6SXNzdWUzNzk1NzY3MDQ=PPO1 declines after 530 iterations2018-11-11T23:36:32Z2018-11-16T10:19:01ZTrue
MDU6SXNzdWUzODAzMTY4NjY=Do is possible use DQN in Vectorized Environment ?2018-11-13T16:38:20Z2018-11-13T18:49:08ZTrue
MDU6SXNzdWUzODA5NjUxNTg=PPO on Atari Specifics2018-11-15T01:53:10Z2018-11-19T21:44:40ZTrue
MDU6SXNzdWUzODEwMjUwMjQ=I truly do not know how to plot my work with HER2018-11-15T06:56:25Z2018-11-17T09:14:01ZTrue
MDU6SXNzdWUzODE1ODM2MjE=DDPG cannot work on the robotics environment.2018-11-16T12:41:04Z2018-12-20T17:01:21ZTrue
MDU6SXNzdWUzODE2MDYxMzM=[Question] Default hyperparameter choice for Prioritized Experience Replay2018-11-16T13:46:46Z2018-11-19T18:42:17ZTrue
MDU6SXNzdWUzODE2MTAzOTQ=ModuleNotFoundError when running PPO12018-11-16T13:58:40Z2018-11-16T14:07:33ZTrue
MDU6SXNzdWUzODE2NDM5ODA=Customize reward function with DDPG+HER2018-11-16T15:24:04ZFalse
MDU6SXNzdWUzODE5MTA0NzI=DDPG popart bug fixed, code needs to be modified2018-11-17T23:13:09Z2018-11-27T00:27:33ZTrue
MDU6SXNzdWUzODIyMDIyMjI=What's the best parameters configuration to run humanoid-v2 env?2018-11-19T12:55:22ZFalse
MDU6SXNzdWUzODI0OTYyOTY=Running Atari Ram Environments with PPO22018-11-20T04:31:15ZFalse
MDU6SXNzdWUzODMxNTk3MDA=best hardware combination for deep-RL2018-11-21T15:05:08Z2018-11-27T00:57:24ZTrue
MDU6SXNzdWUzODMyMjU3NDQ=MPI with PPO2 broken2018-11-21T17:33:23Z2018-11-27T01:56:57ZTrue
MDU6SXNzdWUzODM0NzIyNDQ=Is it possible to run the baseline algorithm on self-made gym environments?2018-11-22T10:30:36Z2018-11-22T14:01:44ZTrue
MDU6SXNzdWUzODQxNzI2MjE=Sticky Frame Skip 2018-11-26T04:45:08Z2018-11-26T04:51:56ZTrue
MDU6SXNzdWUzODQyOTg1OTQ=why i can't  find the instructions on how to load and display the training data.2018-11-26T11:59:42ZFalse
MDU6SXNzdWUzODUyNzY4MjI=ScaledFloatFrame bottleneck2018-11-28T13:50:27ZFalse
MDU6SXNzdWUzODUzNjUyNTM=Deactivate learning rate scheduling to PPO with Adam2018-11-28T16:58:27ZFalse
MDU6SXNzdWUzODUzODM1NDA=`FetchPickAndPlace-v1`: AssertionError: Can only deal with Discrete and Box observation spaces for now2018-11-28T17:40:57Z2018-12-19T22:44:50ZTrue
MDU6SXNzdWUzODU1NTQxNTA=Unable to debug the PPO2 algorithm with VSCode2018-11-29T03:28:38Z2018-12-13T00:57:26ZTrue
MDU6SXNzdWUzODU2NTYwOTQ=confirm if my understanding of mpi usage is right2018-11-29T10:01:50Z2018-12-13T01:15:35ZTrue
MDU6SXNzdWUzODU2ODIzNjc=undefined symbol2018-11-29T11:06:12Z2018-12-20T02:11:35ZTrue
MDU6SXNzdWUzODU3NTEzMjQ=Reward and Length Buffer2018-11-29T14:04:07Z2018-12-19T23:16:43ZTrue
MDU6SXNzdWUzODYxMzE5Mjk=A way to disable default logging configuration2018-11-30T10:39:51ZFalse
MDU6SXNzdWUzODY1Njg2MzQ=ACER impl. broken?2018-12-02T15:13:34ZFalse
MDU6SXNzdWUzODcwOTkzNTM=Error using cnn-deepq to train CartPole-v02018-12-04T03:05:42Z2018-12-04T08:22:12ZTrue
MDU6SXNzdWUzODczODQwMjE=ShmemVecEnv does not support float642018-12-04T16:55:48ZFalse
MDU6SXNzdWUzODc0Njk4MDE=VecEnv which parallelizes whole roll-out and does not sync on steps2018-12-04T20:38:31ZFalse
MDU6SXNzdWUzODgxMDMzNzE=run baseline with mpi -np 1, performance decrease dramatically2018-12-06T08:22:36Z2019-03-04T08:03:33ZTrue
MDU6SXNzdWUzODgyODAwNDg=JSONOutputFormat has invalid ndarray conversion2018-12-06T16:01:33ZFalse
MDU6SXNzdWUzODg1NzE4OTg=Intuition behind batch sizes2018-12-07T09:31:15Z2018-12-20T21:17:00ZTrue
MDU6SXNzdWUzODg3Njk1NjU=SubprocVecEnv sometimes hangs if tensorflow is being used2018-12-07T18:42:33Z2019-01-07T21:33:49ZTrue
MDU6SXNzdWUzODg5NTA5Mjk="Failed testing with ""RuntimeError: Graph is finalized and cannot be modified."""2018-12-08T19:18:44Z2018-12-09T12:29:27ZTrue
MDU6SXNzdWUzODkwNTQwMjI=Can't match(or come close to) results in paper2018-12-09T19:15:30ZFalse
MDU6SXNzdWUzODk0NDI3NDA=Action space in custom gym env is ignored2018-12-10T19:05:31Z2018-12-20T17:26:27ZTrue
MDU6SXNzdWUzODk1OTAwMTY=ModuleNotFoundError: No module named 'cv2'2018-12-11T04:15:33ZFalse
MDU6SXNzdWUzOTAxMTc5Njc=why minus maximum in function entropy in a2c ?2018-12-12T08:39:07Z2018-12-13T05:36:22ZTrue
MDU6SXNzdWUzOTEzOTQzODA=eplenmean and eprewmean always nan2018-12-15T17:00:43Z2018-12-21T19:06:08ZTrue
MDU6SXNzdWUzOTE0MzI0ODY=DDPG never converge2018-12-16T03:46:03Z2018-12-17T09:00:48ZTrue
MDU6SXNzdWUzOTE2MTM1NTI=PPO2 clip value loss2018-12-17T08:59:24ZFalse
MDU6SXNzdWUzOTIzNjM1Mjk=PPO2 Why combine loss function when parameters not shared between policy and value?2018-12-18T22:31:16Z2018-12-19T22:16:41ZTrue
MDU6SXNzdWUzOTI0NzIwNDE=How to draw the score curve of PPO? 2018-12-19T07:25:39ZFalse
MDU6SXNzdWUzOTI4NzAzMzQ=Her Result Plot2018-12-20T03:21:56ZFalse
MDU6SXNzdWUzOTMxMTkxNDg=Undesirable reset every 100 steps in my custom environment2018-12-20T16:27:00ZFalse
MDU6SXNzdWUzOTM1MDQyMTk=PPO2: Epsilon exploration?2018-12-21T15:48:48Z2018-12-21T20:49:18ZTrue
MDU6SXNzdWUzOTM1MDk3MjY=Baseline implementations seem non-modular2018-12-21T16:05:19ZFalse
MDU6SXNzdWUzOTM1Mjg2MjI=About benchmark2018-12-21T17:06:40ZFalse
MDU6SXNzdWUzOTQwMzg3Mzg="""exception loading monitor file in PATH: 't'"" "2018-12-25T17:15:38ZFalse
MDU6SXNzdWUzOTQ4MjgzNDk=SubprocVecEnv now creates each environment twice2018-12-29T22:09:10ZFalse
MDU6SXNzdWUzOTQ5MjU2MzA=Segmentation fault (core dumped) when running example script PPO22018-12-31T04:02:37ZFalse
MDU6SXNzdWUzOTY0ODc1ODU=Handling MultiDiscrete action_space2019-01-07T13:30:00ZFalse
MDU6SXNzdWUzOTc0MjM3NzQ="what does the command ""python -m baseline.run **args"" mean?"2019-01-09T15:31:04ZFalse
MDU6SXNzdWUzOTc4MTQ0MzE=Human Starts used in DQN Paper2019-01-10T12:44:49ZFalse
MDU6SXNzdWUzOTc4OTczMzM=Missing arguments in call to build policy when using ACER with LSTM. 2019-01-10T16:00:02Z2019-01-24T02:32:00ZTrue
MDU6SXNzdWUzOTgzNjUwNTk=AssertionError in HER with num_env > 12019-01-11T16:39:19ZFalse
MDU6SXNzdWUzOTg3NzAxNDY=make_pdtype raise NotImplementedError2019-01-14T07:09:52ZFalse
MDU6SXNzdWUzOTk0NDYyMzc=error when i run python -m baselines.run --alg=ppo2 .... HELP please2019-01-15T17:09:28Z2019-01-24T23:05:35ZTrue
MDU6SXNzdWUzOTk0NDg1MTc=Jessica Lockwood& Johnnyhelm2019-01-15T17:15:15Z2019-01-23T01:27:15ZTrue
MDU6SXNzdWUzOTk4MzU3MTg=Regarding GAIL dataset2019-01-16T14:33:30ZFalse
MDU6SXNzdWU0MDEwNTk2ODc=why i can't get the performance presented in the benchmark?2019-01-20T03:47:04ZFalse
MDU6SXNzdWU0MDExMTIxMzM=[common] Inconsistent usage and implementation of tf_util.function2019-01-20T16:07:49Z2019-01-31T18:23:48ZTrue
MDU6SXNzdWU0MDI1MDU3Njc=AttributeError: 'function' object has no attribute 'reset' inf rollout.py 2019-01-24T01:43:58Z2019-01-31T05:49:16ZTrue
MDU6SXNzdWU0MDI1NjQxNzQ=what kind of exploration does baseline ppo2 use for discrete action space,2019-01-24T06:41:58ZFalse
MDU6SXNzdWU0MDI5NDg3ODc=create test setup with MPI2019-01-25T00:01:44ZFalse
MDU6SXNzdWU0MDMwNTI1NDc=observation normalization?2019-01-25T08:28:41Z2019-04-01T22:44:42ZTrue
MDU6SXNzdWU0MDM3MDM3ODc=run behavior_clone.py ，get error   X11/Xlib.h: No such file or directory #include <X11/Xlib.h> and  ‘gnu-cc’  failed with exit status 12019-01-28T08:45:39Z2019-01-31T18:21:47ZTrue
MDU6SXNzdWU0MDQyMDczMzQ=Memory Leak when --num_env > 12019-01-29T09:55:14Z2019-02-11T18:38:52ZTrue
MDU6SXNzdWU0MDQzOTY2MzE=Non-deterministic behaviour when ran on GPU2019-01-29T17:12:37ZFalse
MDU6SXNzdWU0MDQ3ODQ3MTE=Currently only the latest policy will be saved.2019-01-30T14:02:08ZFalse
MDU6SXNzdWU0MDUxODE4Mjc=Where does the aggregation happen?2019-01-31T10:37:58ZFalse
MDU6SXNzdWU0MDUzODA3Mzc=Baselines does not work for algorithmic env with mlp network2019-01-31T18:32:28ZFalse
MDU6SXNzdWU0MDU4MTk1ODE=custom_cartpole example make_session parameter problem2019-02-01T18:25:47Z2019-04-01T23:24:03ZTrue
MDU6SXNzdWU0MDYxNDM0OTU=About Reproducing benchmarks_atari10M and benchmarks_mujoco1M2019-02-03T23:17:26ZFalse
MDU6SXNzdWU0MDY0NzU1Mzc=PPO2 policy DiagGaussian distribution latent vector2019-02-04T19:11:00ZFalse
MDU6SXNzdWU0MDY1NDAwNDU=flatten_dict_observations issue2019-02-04T22:03:45Z2019-12-08T21:53:32ZTrue
MDU6SXNzdWU0MDc2MTI3NTI=Feature request: Plug in arbitrary model for policy2019-02-07T09:43:48Z2019-02-07T18:02:02ZTrue
MDU6SXNzdWU0MDg2MDQ2MjY=Decrease logstd in Mlp policy after some iterations 2019-02-11T00:48:15ZFalse
MDU6SXNzdWU0MDkyNjg5NDk=PPO2 globalseeds2019-02-12T12:14:58Z2019-02-21T14:23:54ZTrue
MDU6SXNzdWU0MTA5NzgyMzk=Failed to download resource2019-02-15T22:47:59ZFalse
MDU6SXNzdWU0MTExMTczMDI=Invalid Action Masking2019-02-16T21:45:16ZFalse
MDU6SXNzdWU0MTE0MzUzMzI=Is DQN agent trained using baselines more robust?2019-02-18T11:40:53ZFalse
MDU6SXNzdWU0MTIwODUwODI=Where does the ppo2 reset the env?2019-02-19T19:24:14Z2019-02-20T07:53:24ZTrue
MDU6SXNzdWU0MTI4MDMzMTE= About NaN values  in plot_results when num_env >12019-02-21T08:29:06ZFalse
MDU6SXNzdWU0MTMwODgxMzY=ConnectionResetError2019-02-21T19:32:13ZFalse
MDU6SXNzdWU0MTMwODkwMTQ=tf.py_func is deprecated in TF V2. Instead2019-02-21T19:34:29ZFalse
MDU6SXNzdWU0MTM2Nzc4NTU=PPO NAN Issue After 475 Iterations and NaN after 27 iterations if Loading Model (Replicated Multiple PC's)2019-02-23T09:36:11ZFalse
MDU6SXNzdWU0MTQwNzY4MDY=Could not broadcast input array from shape (0) into shape (75)2019-02-25T12:16:04ZFalse
MDU6SXNzdWU0MTQ5NTg5ODk=Is there any way to reset the envs manually?2019-02-27T06:02:11ZFalse
MDU6SXNzdWU0MTU4MTEwMzU=Latest changes in MpiAdamOptimizer causes tf to hang2019-02-28T20:34:11ZFalse
MDU6SXNzdWU0MTU4MTMwNjA=IndexError: invalid index to scalar variable.2019-02-28T20:39:35Z2019-03-12T00:44:18ZTrue
MDU6SXNzdWU0MTYxMjg0NDk=Mixed action space2019-03-01T14:31:38ZFalse
MDU6SXNzdWU0MTcxOTQyMjA=Failure in saving models to file with MPI2019-03-05T09:17:58ZFalse
MDU6SXNzdWU0MTc2MTIyNjY=HER MPI broadcasting issue with non-Reach environments2019-03-06T04:01:11ZFalse
MDU6SXNzdWU0MTg1MzI0NzM=Mean final scores of PPO paper2019-03-07T21:58:27Z2019-03-11T22:28:47ZTrue
MDU6SXNzdWU0MTg4OTUyNTg=Environment close not called in monitor.py2019-03-08T18:02:16ZFalse
MDU6SXNzdWU0MTkxMjU2Mjc=Issue Saving Video of Runs2019-03-09T23:09:06ZFalse
MDU6SXNzdWU0MTk3NzYxMTA=PPO2 does not work on mujoco env 2019-03-12T02:50:20Z2019-04-05T23:49:02ZTrue
MDU6SXNzdWU0MjAwODIzNDE=Logging reward for plotting2019-03-12T16:25:31Z2019-03-28T16:21:49ZTrue
MDU6SXNzdWU0MjA3ODIyMzE=deepq is not working in the latest master branch2019-03-14T00:44:28Z2019-04-05T23:43:36ZTrue
MDU6SXNzdWU0MjEwMjY3Mjk=Can anyone report successful parameters for A2C playing Pong or another Atari game?2019-03-14T13:45:14ZFalse
MDU6SXNzdWU0MjE5MTYxNjM=DDPG doesn't work2019-03-17T12:24:24ZFalse
MDU6SXNzdWU0MjI1MTIxODc=why alpha=0.99, epsilon=1e-5 in RMSprop?2019-03-19T02:16:22Z2019-04-04T01:23:21ZTrue
MDU6SXNzdWU0MjMxNDAyNTQ=How to run Super Mario Bros ?2019-03-20T09:09:34ZFalse
MDU6SXNzdWU0MjQ3OTgyMjA=Is evaluation supported in the current version?2019-03-25T09:25:40ZFalse
MDU6SXNzdWU0MjUxOTkwODA=how to take full advantage of training resources when train RL agent2019-03-26T02:37:30ZFalse
MDU6SXNzdWU0Mjg5MDc2ODU=Why two perturbed actors are needed in ddpg parameter space exploration?2019-04-03T18:18:18ZFalse
MDU6SXNzdWU0MjkxNzAxODk=Cannot find commit cbd21ef mentioned by benchmark page2019-04-04T09:20:37Z2019-04-07T03:04:21ZTrue
MDU6SXNzdWU0MzAxMzEyMDI=[Atari Wrapper]: further memory save by efficient LZ4 compression2019-04-07T11:03:02ZFalse
MDU6SXNzdWU0MzMyMzE4Mjg=Testing2019-04-15T11:30:58ZFalse
MDU6SXNzdWU0MzMzOTQwNjQ=Comment in ScaledFloatFrame of atari_wrappers2019-04-15T17:20:50ZFalse
MDU6SXNzdWU0MzM2MTgyNjI=PPO2 Atari cannot reproduce paper results2019-04-16T07:10:42Z2019-04-27T12:21:00ZTrue
MDU6SXNzdWU0MzM3MzY4NTk=PPO2: Non-deterministic behavior when setting --num_timesteps2019-04-16T11:50:02Z2019-06-01T13:31:44ZTrue
MDU6SXNzdWU0MzUzMjQ1Njg=Custom path to for tensorboard logs2019-04-19T22:47:23ZFalse
MDU6SXNzdWU0MzY1MTgyNDE="results_plotter error: ""tuple indices must be integers or slices, not str"""2019-04-24T06:28:54ZFalse
MDU6SXNzdWU0MzY5Mjg3MjQ=How to visualize training environments...?2019-04-24T22:09:36ZFalse
MDU6SXNzdWU0MzczNDU1MDE=How do I modify rewards in model?2019-04-25T18:48:59ZFalse
MDU6SXNzdWU0Mzc2NDUxMDA=TfRunningMeanStd error with mujoco2019-04-26T12:16:30Z2019-04-29T10:36:17ZTrue
MDU6SXNzdWU0Mzc5MzUxNDg=Is entropy loss duplicate when control cost exist ?2019-04-27T11:43:52ZFalse
MDU6SXNzdWU0MzgwNDk1NTE=Better Exploration with Parameter Noise2019-04-28T12:07:56ZFalse
MDU6SXNzdWU0Mzk1MDUwNzk=Reference for dt=1e-2 of OrnsteinUhlenbeckActionNoise2019-05-02T09:42:23ZFalse
MDU6SXNzdWU0Mzk1NDQ3NTA=Bug in ppo22019-05-02T11:32:39ZFalse
MDU6SXNzdWU0Mzk5NTM3MDE=ValueError: cannot copy sequence with size 84 to array axis with dimension 42019-05-03T09:05:52ZFalse
MDU6SXNzdWU0Mzk5NTY0Nzk=The Agent's performance is poor after achieving good training results (DQN on Pong) 2019-05-03T09:13:01Z2019-05-04T14:23:17ZTrue
MDU6SXNzdWU0NDAwMDc4Nzc=No shmem_vec_env in pip distribution2019-05-03T11:40:03ZFalse
MDU6SXNzdWU0NDA0MjYyNTA=Memory problem in using LazyFrames2019-05-05T10:00:30Z2019-08-30T06:32:47ZTrue
MDU6SXNzdWU0NDA2MTYwMjc=Saving the model2019-05-06T09:35:15ZFalse
MDU6SXNzdWU0NDIxMzQ1MTU=f-strings breaks Python < 3.62019-05-09T09:18:07Z2019-05-31T21:27:12ZTrue
MDU6SXNzdWU0NDI2ODY1MDY=deepq not using gamma for calculating returns2019-05-10T12:09:22ZFalse
MDU6SXNzdWU0NDI4NzUyMzk=env: how to pass a own defined environment2019-05-10T20:04:56ZFalse
MDU6SXNzdWU0NDM2NzE4ODg=the table in the benchmark2019-05-14T01:43:38ZFalse
MDU6SXNzdWU0NDQwMzY4MzI=baseline codes are written by python2.7 but the end of support for python 2.X is just 20202019-05-14T17:28:17Z2019-05-16T09:39:59ZTrue
MDU6SXNzdWU0NDY5NDE2NjE=Training stops with: Self-adjoint eigen decomposition was not successful. The input might not be valid.2019-05-22T05:40:02ZFalse
MDU6SXNzdWU0NDcwODM4Nzk=Example play from main Readme stops after a short time with: RuntimeError: Tried to step environment that needs reset2019-05-22T11:33:53ZFalse
MDU6SXNzdWU0NDc4OTc1MDk=DDPG bug: layer norm not really applied when initializing the critic (Q) network2019-05-23T22:17:42ZFalse
MDU6SXNzdWU0NDkyNzk4NzI=Computing the gradient of the KL in A2C2019-05-28T13:47:31ZFalse
MDU6SXNzdWU0NTAxNDYxNzg=PPO1 ob_rms effects the oldpi2019-05-30T06:06:50ZFalse
MDU6SXNzdWU0NTA2NTkxMzE=TypeError: _cnn_to_mlp() missing 2 required positional arguments: 'num_actions' and 'scope'2019-05-31T07:36:48ZFalse
MDU6SXNzdWU0NTExMDg5ODk=Multiple Layers in Discriminator2019-06-01T17:51:06ZFalse
MDU6SXNzdWU0NTExNjIyMTc=Installation problem2019-06-02T07:20:31ZFalse
MDU6SXNzdWU0NTE3MDc5OTI=[HER] Why does sampling goals from future transitions work the best2019-06-03T21:58:10ZFalse
MDU6SXNzdWU0NTE4Mjg4ODU=Are hyper-parameters in ppo2/default.py used in benchmark?2019-06-04T06:59:51ZFalse
MDU6SXNzdWU0NTIxODYwOTI=Deepq Favoring Action 0 in Discrete Action Space2019-06-04T20:13:57Z2019-06-13T23:14:47ZTrue
MDU6SXNzdWU0NTI4MzY5OTY=How to make expert data2019-06-06T05:22:22ZFalse
MDU6SXNzdWU0NTM1MDc2Mjk=Release policy2019-06-07T13:13:51ZFalse
MDU6SXNzdWU0NTM3NjQ5MDI=How the policy gradient be applied in multi-heads action output network?2019-06-08T07:04:12ZFalse
MDU6SXNzdWU0NTU2NDI0OTY="deepq/experiments/train_cartpole.py inability to change ""is_solved"" condition"2019-06-13T09:48:22ZFalse
MDU6SXNzdWU0NTYxOTE0NzI=DDPG does not support load_path parameter2019-06-14T11:08:07ZFalse
MDU6SXNzdWU0NTYyMTYwODE="missing argument in _mlp() call in file ""deepq/models.py "2019-06-14T12:12:33ZFalse
MDU6SXNzdWU0NTg3NjUzNzQ=DDPG implementation fails to learn well on at least five MuJoCo-v2 envs for all three noise types. I report steps to reproduce and learning curve plots [and show that PPO2 seems to work fine].2019-06-20T16:50:38ZFalse
MDU6SXNzdWU0NTk0NTE5Mzc="When use 'env.render()', the error:""dexError: list index out of range"""2019-06-22T08:29:26ZFalse
MDU6SXNzdWU0NTk1NTc5MDM=DDPG without HER in Fetch?2019-06-23T09:57:37ZFalse
MDU6SXNzdWU0NTk4NDQyNDA=MPI improve performance or Speed up training on PPO?2019-06-24T11:43:36ZFalse
MDU6SXNzdWU0NjA0Mzg3ODQ=Some misunderstand about the paper and code in HER2019-06-25T13:53:04ZFalse
MDU6SXNzdWU0NjA1NjI1MTA=Atari replay buffer won't fit into memory even with LazyFrames2019-06-25T17:59:32ZFalse
MDU6SXNzdWU0NjA3OTAwMDQ=will gail work if i provide expert path with different length?2019-06-26T06:43:12ZFalse
MDU6SXNzdWU0NjE1Nzc4MTY=DQN documentation2019-06-27T14:53:54Z2020-03-17T16:50:23ZTrue
MDU6SXNzdWU0NjIxMjU0MjQ=MaxAndSkipEnv does not apply max on reset()2019-06-28T16:56:39ZFalse
MDU6SXNzdWU0NjI0MjE2NTI=HER model don't learn again after load2019-06-30T16:17:18Z2019-06-30T16:25:31ZTrue
MDU6SXNzdWU0NjMxMDM5MTI=Typo in GAIL dataset log2019-07-02T08:53:49Z2019-08-05T23:55:34ZTrue
MDU6SXNzdWU0NjMxMTAyOTk=Default trajectory limitation should be an integer value2019-07-02T09:06:50ZFalse
MDU6SXNzdWU0NjM3MDQ3MDI=Allow specifying an activation function for the output layer of the actor 2019-07-03T11:43:47ZFalse
MDU6SXNzdWU0NjQ3MDIyMjM=deepq and simple files are not existing at the same time2019-07-05T16:07:24ZFalse
MDU6SXNzdWU0NjQ5MDc0MzU=MPI ？？2019-07-07T02:18:26ZFalse
MDU6SXNzdWU0NjQ5MDkzMjg=fetch_data_generation.py2019-07-07T02:56:17ZFalse
MDU6SXNzdWU0NjYzNTM3MDM=Definition of custom environments when using PPO22019-07-10T14:47:43Z2019-08-23T10:44:40ZTrue
MDU6SXNzdWU0NjY5MjI5NTk=Trouble loading model(didn't work)2019-07-11T14:19:51ZFalse
MDU6SXNzdWU0Njc3OTE0MTA=module 'baselines.deepq.models' has no attribute 'cnn_to_mlp'2019-07-14T03:45:56ZFalse
MDU6SXNzdWU0Njk2MTA2OTg=Trick on FetchPickAndPlace-v12019-07-18T07:24:23ZFalse
MDU6SXNzdWU0NzAyOTgzODg=The loaded model predicts different actions for the same observations2019-07-19T12:25:17ZFalse
MDU6SXNzdWU0NzIxODkwNjU='Box' object has no attribute 'n'2019-07-24T09:54:53Z2019-11-08T23:34:42ZTrue
MDU6SXNzdWU0NzMwNDk4MTc=AttributeError: entry_point variable broken in run.py2019-07-25T20:13:41Z2019-08-10T14:00:18ZTrue
MDU6SXNzdWU0NzMxMTkxODY=Pretraining PPO using off-policy data2019-07-26T00:05:12ZFalse
MDU6SXNzdWU0NzM5MTg4MjU=The file that README.md mentioned is not exist.2019-07-29T08:29:19ZFalse
MDU6SXNzdWU0NzQ1NDk4MzA=how to get image in  Fetch environments？I want get it for network input2019-07-30T12:15:13ZFalse
MDU6SXNzdWU0NzUxMDQxMTU="VecNormalize ""return"" calculation"2019-07-31T12:05:43Z2019-08-12T19:44:54ZTrue
MDU6SXNzdWU0NzUyNTE2MTI=Saving a Model2019-07-31T16:49:03Z2019-11-08T23:21:06ZTrue
MDU6SXNzdWU0NzU3MzA5NjI=DDPG does not train2019-08-01T14:55:31ZFalse
MDU6SXNzdWU0NzY0OTk4NzY=kl divergence suddenly increase after training for a while2019-08-03T23:56:32ZFalse
MDU6SXNzdWU0NzY1MjU3MDc=AttributeError: 'EnvSpec' object has no attribute '_entry_point'2019-08-04T07:51:48ZFalse
MDU6SXNzdWU0NzgwNTU3MDQ=AttributeError: 'int' object has no attribute 'get'2019-08-07T17:37:22ZFalse
MDU6SXNzdWU0NzgxODUxNTA=Behavior of RunningMeanStd.update when MPI is not available2019-08-07T23:42:21ZFalse
MDU6SXNzdWU0Nzg0NTYzNzQ=DDPG doesn't work in mujoco2019-08-08T12:59:32Z2019-10-25T22:54:42ZTrue
MDU6SXNzdWU0Nzg4NTUwNzY=Not able to run any baseline example due to missing entry point2019-08-09T08:07:25Z2019-11-08T23:16:33ZTrue
MDU6SXNzdWU0Nzg4NzQ1Njg=Cannot reproduce the benchmark results of DQN (vanilla and PDD) on Breakout 2019-08-09T08:52:31ZFalse
MDU6SXNzdWU0Nzk0ODU5ODE=GAIL gpu runs slower than cpu2019-08-12T06:29:39Z2019-12-08T04:21:23ZTrue
MDU6SXNzdWU0Nzk4NzM4MDc=Concat action and observation2019-08-12T22:12:46ZFalse
MDU6SXNzdWU0Nzk5MzQ1NTI=deleted2019-08-13T02:51:49Z2019-08-26T01:31:42ZTrue
MDU6SXNzdWU0ODAyNzk0MTk=PPO Breakout Score2019-08-13T17:37:01ZFalse
MDU6SXNzdWU0ODA1NTk4NjI=PPO1 time steps and relationship with episodes2019-08-14T08:54:46Z2019-08-20T11:07:16ZTrue
MDU6SXNzdWU0ODQyODYyOTQ=I found out in Fetch Env, the action space is the linear velocity of grip, instead of grip position described in the paper2019-08-23T01:28:40Z2019-08-23T06:42:20ZTrue
MDU6SXNzdWU0ODQzNjE5ODk="HER: How to modified the target not in the air and do not use ""start after pick up the object"" in PickAndPlace Env?"2019-08-23T06:57:56ZFalse
MDU6SXNzdWU0ODQ2ODgwMzE=Is the PPO2 optimization synchronous or asynchronous between the different agents ?2019-08-23T19:56:50Z2019-10-25T22:45:55ZTrue
MDU6SXNzdWU0ODUxODIyOTM=why `step_model` and `train_model` instead of one?2019-08-26T11:14:52Z2019-10-25T22:44:45ZTrue
MDU6SXNzdWU0ODY0OTU1MzM=AttributeError: 'list' object has no attribute 'device'2019-08-28T16:50:54ZFalse
MDU6SXNzdWU0ODc3NjIwNjc=When I run multiple thread experiment, it showed up this error. Could anyone help to solve it?2019-08-31T11:48:15Z2019-09-27T21:45:42ZTrue
MDU6SXNzdWU0ODc4NDYyNDc=Reward prediction in PPO12019-09-01T06:02:17ZFalse
MDU6SXNzdWU0OTA3OTYwMzU=Gradient with respect to input is `None`2019-09-08T19:32:44Z2019-11-05T15:38:33ZTrue
MDU6SXNzdWU0OTQ2MjI4MzA=Code Reproducibility2019-09-17T13:25:31ZFalse
MDU6SXNzdWU0OTU4MDU4ODI=Tensorflow 2 version with Box constraints2019-09-19T13:38:30ZFalse
MDU6SXNzdWU0OTY0NDU2MjM=Not train with vectorized environment for APIs you don't have control over?2019-09-20T16:35:46Z2019-11-08T23:02:49ZTrue
MDU6SXNzdWU0OTc0NTk0NDQ=using the OpenAI baselines to train the robotic arm2019-09-24T05:11:51Z2019-09-24T11:55:22ZTrue
MDU6SXNzdWU0OTgxNjE4NTA=No Monitor Files for TRPO and DeepQ2019-09-25T09:26:28ZFalse
MDU6SXNzdWU0OTg2MTc1ODk="when I use ""pytest"", there is error like ""OSError: [Errno 9] Bad file descriptor"""2019-09-26T02:29:33ZFalse
MDU6SXNzdWU1MDE2NjMzNDY=PPO2 on Swimmer-v2. Avg reward plot matches the one in the paper but video is not meaningful.2019-10-02T18:49:40ZFalse
MDU6SXNzdWU1MDU2MTg2NzA=does this toolbox support PyTorch?2019-10-11T03:38:11Z2019-10-25T22:33:36ZTrue
MDU6SXNzdWU1MDU2MzYzNjc=About mpi running for HER DDPG2019-10-11T04:51:22Z2019-11-08T23:01:07ZTrue
MDU6SXNzdWU1MDY2NTQxNzk=Internal States2019-10-14T13:29:04ZFalse
MDU6SXNzdWU1MDc5NTYyNDE=Mismatch between plotted iterations and actual iterations2019-10-16T16:19:24Z2019-11-08T23:14:55ZTrue
MDU6SXNzdWU1MTM4OTAzNjc=Python crashes when starting baselines or pytest2019-10-29T12:43:17Z2019-11-08T22:51:05ZTrue
MDU6SXNzdWU1MTQzODAwMDk=TypeError: mlp() got an unexpected keyword argument 'value_network'2019-10-30T03:50:19ZFalse
MDU6SXNzdWU1MTUxOTg3OTc=Number of episodes & timesteps not matching paper, with same hyperparameters2019-10-31T06:30:25ZFalse
MDU6SXNzdWU1MTU4NjI0Njc=What's the pros and cons of using the entire episodes to train an PPO with LSTM at each step?2019-11-01T01:47:10ZFalse
MDU6SXNzdWU1MTg1OTUwOTA=poor training result when training mountaicar-v0 using PPO2 without observation normalize2019-11-06T16:44:34ZFalse
MDU6SXNzdWU1MTkwNzc5NjQ=module 'stable_baselines.deepq' has no attribute 'models'2019-11-07T06:54:15Z2019-11-08T22:42:58ZTrue
MDU6SXNzdWU1MjAyNDE3MTE=Tensorboard logging does not work in TF2 branch2019-11-08T21:27:25ZFalse
MDU6SXNzdWU1MjE4NjAxODk=GAIL: Humanoid data environment id2019-11-12T23:21:02ZFalse
MDU6SXNzdWU1MjE4OTIzNTc=Failed to learn when using deepq in Atari Learning Environment2019-11-13T01:05:42ZFalse
MDU6SXNzdWU1MjMwNTgyMTU=Runner masks-dones problem2019-11-14T19:28:29ZFalse
MDU6SXNzdWU1MjMyODc0NDk=Masks, what do they do, where are they?2019-11-15T06:46:22ZFalse
MDU6SXNzdWU1MjUwOTE5NjM=Why do I achieve better results in HER + DDPG than presented in baselines?2019-11-19T15:43:45ZFalse
MDU6SXNzdWU1MjYzNzEyNDY=About transfer to tf2 ,no module from tensorflow.contrib.staging 2019-11-21T05:20:44Z2019-11-25T04:09:51ZTrue
MDU6SXNzdWU1Mjc1NzMzMjc=What might cause conjugate gradient calculation in TRPO to fail2019-11-23T15:50:51ZFalse
MDU6SXNzdWU1Mjc4MzI2NjI=Tf2 version her/ddpg.py no loss function2019-11-25T04:11:20ZFalse
MDU6SXNzdWU1Mjg3OTY3MTQ=[Question] why do we need to stack image for training PPO?2019-11-26T15:24:23ZFalse
MDU6SXNzdWU1Mjk4NDk4NTM=mpi_rank_weight in ppo22019-11-28T11:15:37ZFalse
MDU6SXNzdWU1MzAwNzYwMDY=Help with meaning of stats/ log2019-11-28T21:08:12Z2019-12-30T18:33:47ZTrue
MDU6SXNzdWU1MzA3NTg1MTY=Empirical returns != td(lambda) returns?2019-12-01T16:39:16ZFalse
MDU6SXNzdWU1MzE2MTUwNTg=Passing dictionary of boxes as observation 2019-12-02T23:51:24ZFalse
MDU6SXNzdWU1MzI0NzQ0OTU=How to use evaluate mode of GAIL?2019-12-04T06:17:59ZFalse
MDU6SXNzdWU1MzQzNzE1NTI=Is SubProcvecEnv well-suited for large-CPU architecture ?2019-12-07T08:14:55ZFalse
MDU6SXNzdWU1MzU0OTI2NTI=from gym.wrappers import FlattenDictWrapper of cmd_util.py in ~/baselines/baselines/common is not correct2019-12-10T04:54:34ZFalse
MDU6SXNzdWU1MzU1NTIzNjk=Does LSTM support PPO2?2019-12-10T07:42:42ZFalse
MDU6SXNzdWU1MzYxNjkzNzU=Trained model not working2019-12-11T06:27:19ZFalse
MDU6SXNzdWU1MzY3ODUyNTg=Non-compatibility with mujoco-py 2.0?2019-12-12T06:35:24ZFalse
MDU6SXNzdWU1MzgzOTI5MzE=ImportError: cannot import name 'FlattenObservation'2019-12-16T12:50:50ZFalse
MDU6SXNzdWU1MzkyNDIwMzk=about  tf2 branch ,model can  not be  saved2019-12-17T18:38:36ZFalse
MDU6SXNzdWU1NDAxNTI2MDA=model.save() will cause the nodes of the tensorflow graph to increase!2019-12-19T08:18:29ZFalse
MDU6SXNzdWU1NDE2NzkyMjc=ValueError: could not broadcast input array from shape (21,10) into shape (50,10)2019-12-23T10:03:52ZFalse
MDU6SXNzdWU1NDU0MTc4NTY=Bug in baselines/deepq/replay_buffer.py2020-01-05T13:28:35ZFalse
MDU6SXNzdWU1NDcwMzU4NDU=How to Pass PyTorch Tensors to OpenAI's Baselines SubprocVecEnv Step Method?2020-01-08T18:40:03ZFalse
MDU6SXNzdWU1NDcxMjM5OTI=Majority errors when running pytest after install (tf2)2020-01-08T21:53:55Z2020-01-09T17:03:23ZTrue
MDU6SXNzdWU1NDg2MDQyODE=ERROR: Could not find a version that satisfies the requirement mpi4py (from baselines==0.1.5) 2020-01-12T17:56:33ZFalse
MDU6SXNzdWU1NTAwNTg3NDQ=A2C Colab Atari - ConnectionResetError: [Errno 104] Connection reset by peer2020-01-15T09:15:30ZFalse
MDU6SXNzdWU1NTExNDcwMzU=pytest fails in bench\test_monitor.py2020-01-17T01:25:31ZFalse
MDU6SXNzdWU1NTIxNzc1MjM=Use the baselines for non-gym environments2020-01-20T09:32:19ZFalse
MDU6SXNzdWU1NTI5NjUzODk=ValueError int32 and int64 in baselines.gail.behavior_clone2020-01-21T15:52:30ZFalse
MDU6SXNzdWU1NTM4MzQ3NjE=PPO2: the magnitude of the noise added to the sampled action is too big!2020-01-22T22:35:34ZFalse
MDU6SXNzdWU1NTUyNTM0NzM=PPO: no way to handle divergence?2020-01-26T16:34:06ZFalse
MDU6SXNzdWU1NTY1NTY3NzE=SubProcVecEnv raises  ConnectionResetError: [Errno 104] Connection reset by peer2020-01-28T23:48:21ZFalse
MDU6SXNzdWU1NTc3MzQ2NTU=Pytorch2020-01-30T20:30:17ZFalse
MDU6SXNzdWU1NTk0MDMzODU=Failing to reproduce DDPG results on Plappert et al., 20182020-02-03T23:48:38ZFalse
MDU6SXNzdWU1NjIzMTg4NjM=How to uninstall baselines?2020-02-10T04:35:06ZFalse
MDU6SXNzdWU1NjI1NTEyMDg=Her algorithm not working in tf2 branch2020-02-10T13:12:02ZFalse
MDU6SXNzdWU1NjI3NDg4MTk=Her.RolloutWorker not finding env.reset method2020-02-10T18:33:48Z2020-02-27T15:29:46ZTrue
MDU6SXNzdWU1NjU5NDc5Nzc=How are my Hindsight Experience Replay (HER) results obtained 50 times faster than original paper?2020-02-16T18:32:57Z2020-02-23T12:50:26ZTrue
MDU6SXNzdWU1NjczMTM4MTY=can i apply any baseline algo to game like chess?2020-02-19T04:48:49ZFalse
MDU6SXNzdWU1Njc3MDEzODE=CategoricalPd:entropy2020-02-19T17:01:53ZFalse
MDU6SXNzdWU1NzI1NDk2MzU=Support request: tf2.0 KFAC optimizer 2020-02-28T06:30:10ZFalse
MDU6SXNzdWU1NzYyMTIzODU=Signal code: Address not mapped (1)2020-03-05T12:18:09ZFalse
MDU6SXNzdWU1ODAxNDA2ODA=SubprocVecEnv not treated as VecEnv by PPO22020-03-12T18:30:53Z2020-03-12T20:04:40ZTrue
MDU6SXNzdWU1ODA1ODM5NjY=How to use tf.Print / tf.print to show experience tuples in DQN2020-03-13T12:56:20ZFalse
MDU6SXNzdWU1ODU5ODA0NDI=where is the 'run_atari.py' in a2c?2020-03-23T07:06:39ZFalse
MDU6SXNzdWU1OTE5OTk2MTA=Attribute activation is not used in conv in a2c.utils2020-04-01T15:21:34ZFalse
MDU6SXNzdWU1OTIzNDc5NDY=[HELP NEEDED] KfacOptimizer & cross-entropy loss2020-04-02T04:23:48ZFalse
MDU6SXNzdWU1OTI1MzUyMzY=Not A2C but REINFORCE with baseline2020-04-02T10:46:53Z2020-04-02T11:35:28ZTrue
MDU6SXNzdWU1OTc2MzY3MjU=SubProcVecEnv for curricula2020-04-10T01:04:54ZFalse
MDU6SXNzdWU1OTgyMjU1MjY=How to use the plot.py in baselines/her/experiment?2020-04-11T09:06:48ZFalse
MDU6SXNzdWU2MDMzNjMzNTc=Why is the MaxAndSkip not used in wrap_deepmind? 2020-04-20T16:20:39Z2020-04-20T16:24:13ZTrue
MDU6SXNzdWU2MDcyNzgzNTU=What is the difference between monitor.csv and progres.csv?2020-04-27T06:37:08ZFalse
MDU6SXNzdWU2MTA1MjY1MjQ=get a unique action in all iterations of DDPG2020-05-01T02:54:03ZFalse
MDU6SXNzdWU2MTA5NDcxMDg=Blackjack-v0 throws NotImplementedError error2020-05-01T20:16:40Z2020-05-01T20:25:20ZTrue
MDU6SXNzdWU2MTI0NzI3NTE=[Classic Control Promble] Training baselines branch TF2-PPO2 to solve Pendulum-v0 extremely slow and unstable2020-05-05T09:42:55Z2020-05-10T05:46:44ZTrue
MDU6SXNzdWU2MTMxMzcxODE=About Env Step time on PPO2 mujoco benchmarks2020-05-06T08:22:17ZFalse
MDU6SXNzdWU2MTQ0ODAyNjY=Any advice on how to fix this?2020-05-08T03:36:14ZFalse
MDU6SXNzdWU2MTg2MzY0OTg=Why baselines GAIL uses reward log(1-D) rather than log(D)?2020-05-15T01:37:39ZFalse
