idtitlecreatedAtclosedAtclosed
MDU6SXNzdWUzODk0NTUwNDY=no attribute 'register_symbolic_tensor_type'2018-12-10T19:38:12Z2018-12-10T20:34:12ZTrue
MDU6SXNzdWUzOTM1MjQzMTI=How to load and use trained policy ?2018-12-21T16:52:09Z2019-01-10T17:48:53ZTrue
MDU6SXNzdWU0MDAzMjEwNzM=How should we set the network to `training` mode ?2019-01-17T15:10:44ZFalse
MDU6SXNzdWU0MDM1NTE5OTg=What is your plan about the Agent design and TF 2.0 ?2019-01-27T13:39:53Z2019-04-03T16:05:56ZTrue
MDU6SXNzdWU0MDk2MjUxMzY=Potential memory issue with tf_py_environment2019-02-13T04:47:26Z2019-02-13T16:54:43ZTrue
MDU6SXNzdWU0MTAxMTAzNzk=tf-agents pip is no longer available2019-02-14T03:36:10Z2019-02-19T23:13:07ZTrue
MDU6SXNzdWU0MTEwMTQ3Njg=Issue with observation normalization when len(observation_spec) > 12019-02-16T02:16:20Z2019-02-20T23:58:53ZTrue
MDU6SXNzdWU0MTM1ODE5NDM=Multi-gpu training support?2019-02-22T21:15:03Z2019-02-26T16:34:39ZTrue
MDU6SXNzdWU0MTQyNDY5ODg=Layer sharing between networks2019-02-25T18:37:25ZFalse
MDU6SXNzdWU0MTU0MjU2OTk=Sample action outside the valid range2019-02-28T03:05:32Z2019-03-07T23:03:12ZTrue
MDU6SXNzdWU0MTgxOTQ3ODM=What is your plan for distributing RL agents over a huge cluster ?2019-03-07T08:55:54ZFalse
MDU6SXNzdWU0MTg1MzczNjI=TF Agents & Dopamine2019-03-07T22:11:35Z2019-09-10T18:52:24ZTrue
MDU6SXNzdWU0MTkwODc3NTM=Support for vectorized environments? 2019-03-09T16:10:55Z2019-09-10T18:53:03ZTrue
MDU6SXNzdWU0MTk2OTEzMTM=Dueling DDQN? 2019-03-11T21:26:11ZFalse
MDU6SXNzdWU0MjAwNjMzNzQ=tf.function throws error when used in PP0 TF2.0 GPU example2019-03-12T15:51:05ZFalse
MDU6SXNzdWU0MjAxNDQzNTI=OOM after a couple of iterations2019-03-12T18:34:31Z2019-09-10T18:55:47ZTrue
MDU6SXNzdWU0MjAxNDc4NTc=Intrinsic rewards2019-03-12T18:42:50Z2019-09-10T18:57:27ZTrue
MDU6SXNzdWU0MjAxNjYyMDY=MaskedCategorical distribution example?2019-03-12T19:27:51Z2019-03-12T22:56:37ZTrue
MDU6SXNzdWU0MjAxNzIxODE=Collabs for REINFORCE 2019-03-12T19:43:06Z2019-03-12T20:24:10ZTrue
MDU6SXNzdWU0MjAxNzIxOTI=Collabs for REINFORCE 2019-03-12T19:43:08Z2019-03-12T20:23:55ZTrue
MDU6SXNzdWU0MjAyNjM4MDQ=Getting import error in TF-Agents Policies Tutorial.2019-03-13T00:27:32Z2019-03-13T15:25:07ZTrue
MDU6SXNzdWU0MjAyNzY3NjI=Training TF-agents using custom OpenAI Gym environments2019-03-13T01:32:19Z2019-03-13T15:12:13ZTrue
MDU6SXNzdWU0MjA2NDQ1MjY=MultiDiscrete spaces support2019-03-13T18:05:55Z2019-03-25T17:30:03ZTrue
MDU6SXNzdWU0MjA3NTMxNTE=debug_summaries makes PPOAgent fail when observation is a tuple2019-03-13T22:45:55Z2019-03-27T18:49:19ZTrue
MDU6SXNzdWU0MjA3NzA2NDU=Training PPO in non-episodic environments2019-03-13T23:49:37ZFalse
MDU6SXNzdWU0MjA5NTIwMjY=ParallelPyEnvironment is slow to start2019-03-14T10:56:49Z2019-03-18T20:07:59ZTrue
MDU6SXNzdWU0MjE2Njg4NjY=Would you like to see a MultiCategorical projection network?2019-03-15T19:20:06ZFalse
MDU6SXNzdWU0MjE5NjcyNTI=Notebook tutorials are not py3k-ready2019-03-17T20:13:10Z2019-03-18T22:09:31ZTrue
MDU6SXNzdWU0MjI0Nzk3NjM=EncodingNetwork breaks with only one preprocessing layer.2019-03-18T23:33:41ZFalse
MDU6SXNzdWU0MjI1MDM1ODY=Rainbow agent implementation?2019-03-19T01:33:26Z2019-09-06T17:58:27ZTrue
MDU6SXNzdWU0MjMxMDUzNDE=What does the return function do in the _step function?2019-03-20T07:28:58Z2019-03-20T15:21:50ZTrue
MDU6SXNzdWU0MjM3OTM1NzE=Should we check for termination in collect step in the dqn_tutorial colab?2019-03-21T15:31:50Z2019-03-21T16:39:11ZTrue
MDU6SXNzdWU0MjQ0MjY4NTM=@abc.abstractmethod2019-03-22T22:51:16Z2019-03-25T16:45:00ZTrue
MDU6SXNzdWU0MjUxNjU0NDQ=Invalid actions in a custom ActorDistributionNetwork for a DdpgAgent2019-03-25T23:52:37Z2019-03-26T17:54:46ZTrue
MDU6SXNzdWU0MjU2MzYyOTg=Performance benchmarks2019-03-26T20:35:31ZFalse
MDU6SXNzdWU0MjU3MzA3ODI=How to load trained model or resume training?2019-03-27T02:18:39Z2019-09-10T19:00:24ZTrue
MDU6SXNzdWU0MjU3MzIxMjc=Reusability for Pytorch2019-03-27T02:24:37Z2019-04-09T00:57:08ZTrue
MDU6SXNzdWU0MjYzNjUxMjA=Add a folder for examples?2019-03-28T08:41:38Z2019-09-10T18:59:01ZTrue
MDU6SXNzdWU0MjcyNjYxODM=API documentation2019-03-30T09:59:37Z2019-04-09T11:23:50ZTrue
MDU6SXNzdWU0MjczODQwOTA=SimPLe efficient Agent?2019-03-31T11:00:06ZFalse
MDU6SXNzdWU0Mjc2MTQzMjQ=Recommendation and best practice for pre-training and transfer learning ?2019-04-01T09:57:29Z2019-09-10T19:03:18ZTrue
MDU6SXNzdWU0Mjg0MjYxNzI=What is the recommended way to `render` a TensorFlow Agents TF-Agent?2019-04-02T20:24:13Z2019-04-04T21:18:39ZTrue
MDU6SXNzdWU0MjkxMTI5MDk=Observation summary may cause OOM2019-04-04T06:58:23Z2019-09-10T19:04:32ZTrue
MDU6SXNzdWU0MjkyNDQ2NTM=Issue test SAC2019-04-04T12:14:08Z2019-04-04T15:51:06ZTrue
MDU6SXNzdWU0Mjk0MzM3MDg=SAC for Car Control2019-04-04T18:59:04ZFalse
MDU6SXNzdWU0Mjk1MjgwNjY=VideoWrapper not found in environment/wrappers.py2019-04-04T23:43:25ZFalse
MDU6SXNzdWU0Mjk3NzIwODc=Inconsistencty between tf_env.action_spec(action).is_compatible_with() and  tf_env.step(action)2019-04-05T14:03:49ZFalse
MDU6SXNzdWU0Mjk5NzkzMDk=py_driver constructor, question  2019-04-06T00:48:16Z2019-04-09T20:12:43ZTrue
MDU6SXNzdWU0MzA0MzQwMzA=PyEnvironmentBaseWrapper doesn't forward batch_size and batched calls.2019-04-08T12:52:35Z2019-04-10T02:43:51ZTrue
MDU6SXNzdWU0MzA0NzU0MDQ=Using RNN Q Networks2019-04-08T14:13:14Z2019-06-27T00:32:37ZTrue
MDU6SXNzdWU0MzA1MDIwMDU=Bug in ActionDiscretizeWrapper when working with batched actions.2019-04-08T15:02:07Z2019-06-27T00:35:53ZTrue
MDU6SXNzdWU0MzExMDg1OTA=Using PPO in colab doesn't work?2019-04-09T18:18:36Z2019-04-10T13:55:53ZTrue
MDU6SXNzdWU0MzI1OTgzNTg=self.steps = common.create_variable('steps', 0)2019-04-12T14:36:16Z2019-06-27T00:36:55ZTrue
MDU6SXNzdWU0MzMxNTYyMTg=Cannot import policy_saver2019-04-15T08:28:15Z2019-04-19T07:42:22ZTrue
MDU6SXNzdWU0MzMzNjcwNjE=[GSoC] add new environments to agents2019-04-15T16:13:39Z2019-04-15T19:49:15ZTrue
MDU6SXNzdWU0MzM0NDg4OTQ=Support / documentation for quantization-aware training2019-04-15T19:47:18Z2019-09-10T19:06:26ZTrue
MDU6SXNzdWU0MzM0ODA1MDQ=Why using replay_buffer.gather_all in PPO?2019-04-15T21:11:49ZFalse
MDU6SXNzdWU0MzM1NTA2NTM=[Bug] The first training step of PPO takes very long to complete2019-04-16T02:06:28ZFalse
MDU6SXNzdWU0MzM4NjAyMjQ=Parallel Environment didn't speed up2019-04-16T15:57:28Z2019-05-27T21:26:29ZTrue
MDU6SXNzdWU0MzQwMjUzOTY=Have a bunch of environments to contribute2019-04-16T23:18:56Z2019-09-10T20:16:26ZTrue
MDU6SXNzdWU0MzQ3NzAwMzE=import error: No module named 'tensorflow.python.platform 2019-04-18T13:26:31Z2019-04-18T22:54:33ZTrue
MDU6SXNzdWU0MzQ5ODIzOTg=Error Import tensorflow : __init__() takes exactly 3 arguments (4 given)2019-04-18T22:23:05Z2019-04-18T22:36:02ZTrue
MDU6SXNzdWU0MzUwNzA0NjA=[Suggestion] Cross-pollinate project with project Ray2019-04-19T06:50:12Z2019-09-10T19:10:32ZTrue
MDU6SXNzdWU0MzU0MTg2NDg=Action Spec for custom environment returning AttributeError2019-04-20T16:55:15Z2019-04-24T11:01:42ZTrue
MDU6SXNzdWU0MzYzNzk3MTg=ValueRnnNetwork added extra dimention when input does not have time dimension2019-04-23T20:39:26Z2020-02-13T22:34:22ZTrue
MDU6SXNzdWU0MzYzODE2NjM=Duplicated weight name in networks.utils.mlp_layers2019-04-23T20:44:18ZFalse
MDU6SXNzdWU0MzY0NDQ4MDY=AttributeError: module 'tensorflow.python.util.nest' has no attribute 'flatten_with_tuple_paths'2019-04-24T00:28:03Z2019-04-24T15:59:43ZTrue
MDU6SXNzdWU0Mzg3MjY3NDU=Unable to import a lot of dependencies due to: AttributeError: module 'tensorflow.python.ops.linalg.linear_operator_util' has no attribute 'matmul_with_broadcast' 2019-04-30T11:19:53Z2019-04-30T22:58:42ZTrue
MDU6SXNzdWU0Mzg3NTM2MTM=What versions of tf and tf-probability am I supposed to use for the collab examples?2019-04-30T12:31:25Z2019-04-30T22:58:42ZTrue
MDU6SXNzdWU0Mzk1MjY2NTQ=[Question] Tune/speed up training?2019-05-02T10:40:50Z2019-05-05T09:54:54ZTrue
MDU6SXNzdWU0Mzk1NDUyNDI=[Suggestion] Add/improve support for multi-agent environments2019-05-02T11:34:01Z2019-05-04T17:46:10ZTrue
MDU6SXNzdWU0Mzk2OTYzNTg=Reinforce and PPO train_eval under example/v2 do not support checkpoint2019-05-02T17:10:37ZFalse
MDU6SXNzdWU0NDAyNjM0MDA=environment not maximizing average return2019-05-04T00:10:00Z2019-09-10T18:48:38ZTrue
MDU6SXNzdWU0NDAyNzI1MDY=Multi-dimensional action DQN2019-05-04T01:44:59ZFalse
MDU6SXNzdWU0NDA0NTYzOTM=tensorboard2019-05-05T15:22:54Z2019-05-08T17:07:29ZTrue
MDU6SXNzdWU0NDExNzcwMzI=Impossible to pass arguments to Gym Environment Wrappers2019-05-07T11:20:16Z2019-05-08T17:06:57ZTrue
MDU6SXNzdWU0NDE4NDQxMDk=error when running 6_reinforce_tutorial.ipynb2019-05-08T17:25:39Z2019-05-08T19:35:23ZTrue
MDU6SXNzdWU0NDI2MDc5MzY="Colab 7_SAC_minitaur_tutorial.ipynb ""NameError: name 'video' is not defined"""2019-05-10T08:48:42Z2019-05-10T23:46:35ZTrue
MDU6SXNzdWU0NDMwNTcxMjg=value_pred_loss_coef2019-05-12T02:37:51Z2019-05-14T21:20:12ZTrue
MDU6SXNzdWU0NDM4OTE4NDQ=Run SAC_minitaur_tutorial with ActorDistributionRnnNetwork and num_steps > 22019-05-14T12:39:45Z2019-11-20T14:48:26ZTrue
MDU6SXNzdWU0NDQ1MzE4Mzc=Proper way to write action space2019-05-15T16:24:33Z2019-05-15T16:27:05ZTrue
MDU6SXNzdWU0NDUwMzQxNzA=Shapes of unequal rank when using custom py env in SAC example. ('TFUniformReplayBuffer/ResourceScatterUpdate_3')2019-05-16T15:53:24Z2019-05-22T02:17:44ZTrue
MDU6SXNzdWU0NDUyMjg0MTk=only single action supported2019-05-17T01:50:09ZFalse
MDU6SXNzdWU0NDY3ODk3NTA=DqnAgent.train throws error in TF2.0 environment as Adam.apply_gradients is called with parameter global_step2019-05-21T19:38:02Z2019-05-23T15:56:24ZTrue
MDU6SXNzdWU0NDcwNTY2MjI=The version number is missing (PEP 396)2019-05-22T10:28:56Z2019-05-23T15:49:03ZTrue
MDU6SXNzdWU0NDc5NDA1Nzc="summary() outputs ""QNetwork object has no attribute _check_trainable_weights_consistency"""2019-05-24T01:35:48Z2019-11-04T19:06:21ZTrue
MDU6SXNzdWU0NDgwMTU3Njc=Can't run examples without getting errors. What TF versions are we to use?2019-05-24T07:18:56Z2019-05-30T16:29:14ZTrue
MDU6SXNzdWU0NDg0Mjk2Nzk=Error using parameter train_step_counter according to colab example2019-05-25T06:56:35ZFalse
MDU6SXNzdWU0NDg0NzY0NzI=Is gym.timestep_limit deprecated?2019-05-25T15:45:27Z2019-06-18T16:24:39ZTrue
MDU6SXNzdWU0NDg5MDQyOTk='AtariPreprocessing' object has no attribute 'spec'`2019-05-27T15:09:18ZFalse
MDU6SXNzdWU0NDkyMzY0OTg=How to handle illegal actions?2019-05-28T12:19:02Z2019-05-30T16:26:41ZTrue
MDU6SXNzdWU0NTA5MDIwNjE=How to seed environments belonging to a ParallelPyEnvironment2019-05-31T17:23:36Z2019-06-04T00:46:37ZTrue
MDU6SXNzdWU0NTExMjgwMjU=Add Python type hints2019-06-01T21:49:38ZFalse
MDU6SXNzdWU0NTExNTE0NjQ=Update TFUniformReplayBuffer to use TF2.02019-06-02T04:22:55Z2019-06-03T17:00:00ZTrue
MDU6SXNzdWU0NTE0ODMwNjY=Custom tf.Environment parallelization2019-06-03T13:26:41Z2019-06-03T16:39:10ZTrue
MDU6SXNzdWU0NTE4MjY1NzE=Getting generic methods and attributes from envs in a ParallelPyEnvironment2019-06-04T06:52:52ZFalse
MDU6SXNzdWU0NTMyMDg2NTA=TD3 support for discrete action space2019-06-06T19:50:07Z2019-06-18T20:16:30ZTrue
MDU6SXNzdWU0NTM0NTg2Nzg=DQN Tutorial has import errors2019-06-07T11:06:20Z2019-06-09T09:28:08ZTrue
MDU6SXNzdWU0NTM3NTg5NDI=Is this way of masking TD error/loss correct?2019-06-08T05:32:34Z2019-06-18T19:22:57ZTrue
MDU6SXNzdWU0NTQ0MzExNDU=Error loading DqnAgent saved model.2019-06-11T00:45:42Z2019-09-10T18:34:37ZTrue
MDU6SXNzdWU0NTQ3MzYzOTc=Passing custom arguments to suite_gym.load()2019-06-11T14:48:54Z2019-06-11T18:48:46ZTrue
MDU6SXNzdWU0NTYyNTY1MDY=Request: Exploration by Random Network Distillation (modified PPO)2019-06-14T13:42:59ZFalse
MDU6SXNzdWU0NTY1ODUzNTA=ModuleNotFoundError: No module named 'tf_agents'2019-06-15T23:34:11ZFalse
MDU6SXNzdWU0NTY3NDIzMDg=tf_agent.policy.action(time_step) under TF2 throws exception2019-06-17T04:54:56Z2019-06-18T20:14:27ZTrue
MDU6SXNzdWU0NTgwNTc1MDA=How to create a continuous enviornment2019-06-19T14:45:18Z2019-06-27T00:32:16ZTrue
MDU6SXNzdWU0NTkyMTM3OTE=Add policy saving to 7_SAC_minitaur_tutorial.ipynb2019-06-21T14:02:09Z2019-09-10T18:34:09ZTrue
MDU6SXNzdWU0NTk1Nzg0Mzk=Dict space from gym support2019-06-23T13:50:30ZFalse
MDU6SXNzdWU0NjA4MDI3MDk=How can I access an underlying network?2019-06-26T07:18:42Z2019-06-26T16:13:50ZTrue
MDU6SXNzdWU0NjE5NDkzNzE=TypeError: __init__() got an unexpected keyword argument 'preprocessing_layers'2019-06-28T09:41:04ZFalse
MDU6SXNzdWU0NjM2NjUyNDM=Error in environment validation2019-07-03T10:10:27Z2019-07-04T04:48:08ZTrue
MDU6SXNzdWU0NjM4NDYwODY=Using TF-agents as a dependency2019-07-03T16:27:23Z2019-07-03T20:00:07ZTrue
MDU6SXNzdWU0NjM4NTIwMjY=train_eval.py for PPO is stuck2019-07-03T16:41:49Z2019-09-10T18:44:27ZTrue
MDU6SXNzdWU0NjUzMDA0MTA=Wrong td_target calculation for next_time_step.is_last()2019-07-08T14:46:01Z2019-07-09T21:45:22ZTrue
MDU6SXNzdWU0NjYwNjg4MzY=how to print log_probability in tf agents2019-07-10T02:47:09Z2019-09-10T20:34:42ZTrue
MDU6SXNzdWU0Njc3NjAyMzc=SAC should only apply gradients to trainable variables2019-07-13T19:51:57Z2019-07-30T21:53:51ZTrue
MDU6SXNzdWU0Njg1NjMzNjc=tf-agents-nightly not automatically updated nightly2019-07-16T10:03:08Z2019-12-02T13:50:26ZTrue
MDU6SXNzdWU0NjkwNTcyMzY=Correct way of setting Atari environment for PPO?2019-07-17T08:29:38Z2019-07-24T11:32:28ZTrue
MDU6SXNzdWU0NjkxOTg0NzA=Policy returning always the same action2019-07-17T13:36:58Z2019-07-18T15:10:35ZTrue
MDU6SXNzdWU0NjkyNTgzMzE=How to see model topology / log graph in TensorBoard2019-07-17T14:44:13Z2019-07-17T20:10:50ZTrue
MDU6SXNzdWU0Njk1MTAyMTY=ImageData' object has no attribute 'data'2019-07-18T00:20:12Z2019-07-18T01:19:41ZTrue
MDU6SXNzdWU0Njk2NTUxMTU=Reward Normalizer not updated in PPO?2019-07-18T09:08:56ZFalse
MDU6SXNzdWU0NzA1MDI0MDc=trouble converting a custom PyEnvironment into TFPyEnvironment2019-07-19T19:33:10Z2019-07-19T19:46:14ZTrue
MDU6SXNzdWU0NzA2MjEwNzk=Unable to serve saved model, missing signature_name2019-07-20T01:37:49Z2019-07-24T01:36:54ZTrue
MDU6SXNzdWU0NzA5OTIxNjY=Problem with GPU when running tf_agent.initialize()2019-07-22T09:34:30Z2019-07-22T10:53:11ZTrue
MDU6SXNzdWU0NzE0Njg5MDc=Unable to use two lstm layers with TD3 and DynamicStepDriver2019-07-23T03:28:57Z2019-08-13T22:15:24ZTrue
MDU6SXNzdWU0NzIyMjk3MDU=AtariWrapper outputs warning2019-07-24T11:27:26Z2020-03-26T18:36:11ZTrue
MDU6SXNzdWU0NzQ5MTEyODc=How to use tensor_spec to get embedding2019-07-31T03:02:19Z2019-09-10T20:41:45ZTrue
MDU6SXNzdWU0NzY0ODc0OTA=DQN tutorial: collect_steps_per_iteration = 1 ? dynamic_step_driver ?2019-08-03T20:39:08Z2019-08-05T17:04:50ZTrue
MDU6SXNzdWU0NzkxNTUyNDg=How to create multi-action ActorDistributionNetwork for PPO2019-08-09T20:31:23Z2019-10-03T20:42:23ZTrue
MDU6SXNzdWU0Nzk1OTgyMjQ=Learning the opposite behavior2019-08-12T11:36:07Z2019-09-10T18:23:07ZTrue
MDU6SXNzdWU0Nzk3MDQ4MDY=Untracked object tensor error in PolicySaver.save2019-08-12T15:18:33Z2019-09-23T23:12:55ZTrue
MDU6SXNzdWU0ODA3NjM4Nzk=Reward Assignments After One Epoch2019-08-14T16:04:31Z2019-09-10T18:27:41ZTrue
MDU6SXNzdWU0ODE5MjY2NzQ=Tutorial request - How does saving work?2019-08-17T21:10:54ZFalse
MDU6SXNzdWU0ODM4MTQwNDA=Can not develop on case-insensitive file system (Mac OS)2019-08-22T07:18:05ZFalse
MDU6SXNzdWU0ODM4NTAwMTM=module 'tensorflow' has no attribute 'TypeSpec'2019-08-22T08:39:27Z2019-08-22T08:54:53ZTrue
MDU6SXNzdWU0ODQzMDE5NDY=Network._build assumes input array spec is bounded2019-08-23T02:43:55ZFalse
MDU6SXNzdWU0ODQ5NDUyMjU=Loaded policy can't be used for prediction (TimeStep does not match SavedModel's expected arguments)2019-08-25T15:43:24Z2019-08-26T15:17:53ZTrue
MDU6SXNzdWU0ODUzMTM4MzA=ParallelPyEnvironment fails silently (threads do not start)2019-08-26T16:03:52Z2019-08-26T16:10:30ZTrue
MDU6SXNzdWU0ODYwODI0OTg=Understand SAC v1 replay buffer abnormality2019-08-27T23:26:25Z2019-08-28T21:40:38ZTrue
MDU6SXNzdWU0ODYwODc1NzA=Layer weights initialization2019-08-27T23:44:05Z2019-08-28T18:15:19ZTrue
MDU6SXNzdWU0ODYxMjQzNTY=Rename actor_fc_layers to policy_fc_layers?2019-08-28T02:20:49Z2019-08-28T20:10:17ZTrue
MDU6SXNzdWU0ODY4NjA1NjM=ActorDistributionNetwork crashes when action_spec is not a tensor_spec2019-08-29T10:00:17Z2019-09-10T18:11:13ZTrue
MDU6SXNzdWU0ODY5NDMzOTI=Using the add_batch method from PyUniformReplayBuffer throws an IndexError: tuple index out of range.2019-08-29T12:59:34ZFalse
MDU6SXNzdWU0ODc4MjA3OTM=Trouble testing a environment using utils.validate_py_environment2019-08-31T23:24:50Z2019-09-01T15:13:07ZTrue
MDU6SXNzdWU0ODg4MTg3NzY=Alpha loss in (EC-)SAC different from the original paper2019-09-03T20:55:47Z2019-09-03T21:06:57ZTrue
MDU6SXNzdWU0OTEwNDQwNTU=ActionRepeat wrapper's step() method skips the first time step of each trajectory2019-09-09T11:40:20Z2019-09-14T00:11:24ZTrue
MDU6SXNzdWU0OTEwNzQyMzU=Use gym space's dtype when converting the space to spec in GymWrapper.2019-09-09T12:46:00Z2019-09-14T00:59:20ZTrue
MDU6SXNzdWU0OTExNzI0Nzk=Missing outer_dim in categorical_q_policy action2019-09-09T15:36:46ZFalse
MDU6SXNzdWU0OTMxMTI3ODA=Plot TF-Agents networks2019-09-13T02:50:53ZFalse
MDU6SXNzdWU1MDA4NTMyOTg=Problem when running DQN and Reinforce Colab notebooks2019-10-01T12:09:39Z2019-10-01T12:25:15ZTrue
MDU6SXNzdWU1MDEwODA2ODQ=module 'gast' has no attribute 'Ellipsis'2019-10-01T18:51:02Z2019-10-02T07:46:29ZTrue
MDU6SXNzdWU1MDIyODY0MjE=Dimension error in PPO train() with multiple actions2019-10-03T20:40:35ZFalse
MDU6SXNzdWU1MDMwNzkxNzU=TF-Agents Environments Tutorial.ipynb has random_uniform error2019-10-06T10:08:12Z2019-10-08T15:31:46ZTrue
MDU6SXNzdWU1MDQyMjY0NzM=Method copy for weights of Network2019-10-08T19:13:31Z2019-10-08T20:14:25ZTrue
MDU6SXNzdWU1MDQ3OTY2MjE=ActorDistributionNetwork with bounded array_specs2019-10-09T17:43:32ZFalse
MDU6SXNzdWU1MDUyNDg1Mzc=Parallel training fails silently2019-10-10T12:48:05ZFalse
MDU6SXNzdWU1MDUzODE0MjA=Second Order gradient is not working2019-10-10T16:30:21ZFalse
MDU6SXNzdWU1MDUzOTE5NTc=Post trained model Rewards2019-10-10T16:51:58Z2019-10-11T15:31:24ZTrue
MDU6SXNzdWU1MDYxMjEzMzQ=running TF-Agents Policies Tutorial.ipynb crash at actor_network=action_net parameter2019-10-12T02:57:08Z2019-10-12T09:19:31ZTrue
MDU6SXNzdWU1MDgxMTM3MTk=error initializing AverageEpisodeLengthMetric instance2019-10-16T21:39:21Z2019-10-17T00:03:55ZTrue
MDU6SXNzdWU1MTA0Nzk2MTk=Why using replay_buffer.clear() in PPO? And a bug without it2019-10-22T07:34:09Z2019-10-22T17:29:02ZTrue
MDU6SXNzdWU1MTA1ODEzNjk=save replay buffer2019-10-22T11:04:03Z2019-10-23T15:54:56ZTrue
MDU6SXNzdWU1MTA4NzMwMzY=TF agents & TF 2.0 contrib dependency for current pip package.2019-10-22T19:56:25Z2019-12-11T17:04:31ZTrue
MDU6SXNzdWU1MTIwOTIyNTY=Does this lib provide fully parallel ability?2019-10-24T17:55:12Z2019-10-28T18:22:18ZTrue
MDU6SXNzdWU1MTI2MTQ5MDI=Using tf_agents with Habitat environment2019-10-25T16:45:06Z2019-11-04T22:15:36ZTrue
MDU6SXNzdWU1MTc0Mzg5Mzc=Saving and loading tf_agents2019-11-04T22:23:50Z2019-11-04T22:28:20ZTrue
MDU6SXNzdWU1MTc2NDgxNzg=Structures do not have the same nested Structures2019-11-05T09:24:34Z2019-11-08T16:37:13ZTrue
MDU6SXNzdWU1MTgxOTA5MzM=Why use the tf-nightly?2019-11-06T02:07:35Z2020-02-04T23:52:23ZTrue
MDU6SXNzdWU1MjA2ODgwNjI=I get error when creating TFUniformReplayBuffer2019-11-11T00:59:33Z2019-11-13T02:41:11ZTrue
MDU6SXNzdWU1MjEyNTQyNzA=Error when using loaded trained model2019-11-12T00:06:04ZFalse
MDU6SXNzdWU1MjEyNjkxMDg=how can I output action containing joint decisions2019-11-12T01:04:18Z2019-11-12T01:14:17ZTrue
MDU6SXNzdWU1MjE2NDkxOTY=Error when trying to use action() on saved PPO policy that used RNN model2019-11-12T16:14:43Z2019-11-14T23:54:58ZTrue
MDU6SXNzdWU1MjE5MjE1NTc=How to define actor network yielding action of joint distribution2019-11-13T02:41:08ZFalse
MDU6SXNzdWU1MjM3NTExMzA=Colab 1_dqn_tutorial error on env.render() 2019-11-15T23:52:29Z2019-11-30T19:07:08ZTrue
MDU6SXNzdWU1MjM4MTAwMDU=The collected trajectories do not follow the spec.2019-11-16T07:45:09Z2019-11-16T10:09:25ZTrue
MDU6SXNzdWU1MjM4MjI5NTA=network.create_variables() clogs all GPU memory2019-11-16T09:41:06ZFalse
MDU6SXNzdWU1MjQ3NzA2ODE=EpsilonGreedyPolicy Error2019-11-19T04:45:22ZFalse
MDU6SXNzdWU1MjUxNTQ2NTA=DQN Agent: Should training=True be passed to q_network.__call__?2019-11-19T17:27:15Z2020-02-04T23:54:17ZTrue
MDU6SXNzdWU1MjY5MDIwNTM=Example observation_and_action_constraint_splitter2019-11-21T23:37:23ZFalse
MDU6SXNzdWU1MjcxODk3OTY=Driver Loop InvalidArgumentError with batched env + parallel environments2019-11-22T13:16:50Z2019-12-01T12:29:10ZTrue
MDU6SXNzdWU1Mjc3NTAxMTM=How to implement different deep Q models2019-11-24T20:14:58ZFalse
MDU6SXNzdWU1Mjk5NDI0Njg=DQN Agent Issue With Custom Environment2019-11-28T14:25:08Z2020-01-22T19:14:37ZTrue
MDU6SXNzdWU1MzA2MjAxNDc=Issue with test ddpg 2019-11-30T19:25:59Z2019-12-10T23:56:04ZTrue
MDU6SXNzdWU1MzA4MDE4Mzc=Issue on running ddpg agent with parallel environment2019-12-01T21:48:47Z2019-12-01T21:52:54ZTrue
MDU6SXNzdWU1MzEwMjE2OTg=TFPyEnvironment does not call the actual env._reset()2019-12-02T10:08:18Z2019-12-05T01:54:13ZTrue
MDU6SXNzdWU1MzIzMDQyMDA=[Question] call() in ddpg critic network2019-12-03T22:17:33Z2019-12-11T00:07:42ZTrue
MDU6SXNzdWU1MzI3MzQ3OTA=How do I adjust ddpg example to fit my environment?2019-12-04T14:52:17Z2020-02-13T19:08:52ZTrue
MDU6SXNzdWU1MzQ5NTQ5MDk=ParallelPyEnvironments do not work on Windows2019-12-09T14:07:55Z2019-12-16T19:05:11ZTrue
MDU6SXNzdWU1MzUyODkzMTA=Clarification on defining discrete, nominal actions2019-12-09T20:38:59Z2019-12-10T20:35:43ZTrue
MDU6SXNzdWU1MzUzNTMwNjE=A novice question about shape2019-12-09T21:37:14Z2020-01-06T19:26:13ZTrue
MDU6SXNzdWU1MzU3NjY0NDI=Can we expect a release any time soon ?2019-12-10T14:29:40Z2019-12-10T15:29:23ZTrue
MDU6SXNzdWU1MzY2NjA4Njg=TRAIN SAC AGENT V2 EXAMPLE ON GOOGLE_CLOUD WITH PYTHON 3.5 TENSORFLOW-GPU 2.02019-12-11T22:59:07Z2020-01-21T00:35:56ZTrue
MDU6SXNzdWU1MzcxMDY1NjM=Can the call to 'tf.compat.v1.enable_v2_behavior()' be avoided ?2019-12-12T17:09:17Z2019-12-12T17:25:09ZTrue
MDU6SXNzdWU1MzkxNDA2NDE=is TFUniformReplayBuffer thread safe?2019-12-17T15:31:54Z2020-01-22T16:36:10ZTrue
MDU6SXNzdWU1NDAwODAxOTY=Wrapping a loaded policy model with PyTFPolicy2019-12-19T05:04:49Z2020-01-22T17:29:29ZTrue
MDU6SXNzdWU1NDE0OTI5NDI=tf-agents SAC 10x slower than stable-baselines on same hardware2019-12-22T19:35:19ZFalse
MDU6SXNzdWU1NDIzMjcyMDA=Epsilon greedy policy error when generating new action from Dict Action space. 2019-12-25T11:37:26ZFalse
MDU6SXNzdWU1NDQxMDQyMzc=Issue with using the combination of TimeLimit wrapper, ParallelPyEnvironment and TfPyEnvironment.2019-12-31T06:38:07Z2020-01-20T07:03:45ZTrue
MDU6SXNzdWU1NDQ0MTgwNzQ=is it possible to convert tf-agents to tf-lite and run on android device2020-01-02T00:58:56ZFalse
MDU6SXNzdWU1NDU1NDYyNjc=dqn_tutorial - initial_collect_steps is defined to be 1000 but not used anywhere...2020-01-06T05:24:18ZFalse
MDU6SXNzdWU1NDc0OTkzODI=ParallelPyEnvironment doesn't work when subprocess calling a tensorflow model2020-01-09T14:06:38ZFalse
MDU6SXNzdWU1NTA5ODgwOTY=Problem when running dqn on FrozenLake 2020-01-16T18:46:28ZFalse
MDU6SXNzdWU1NTIxNzcyOTM=concatenation layer and replay buffers2020-01-20T09:31:55Z2020-01-22T16:18:14ZTrue
MDU6SXNzdWU1NTI1NjUxOTg=TRAIN TF-AGENTS WITH MULTIPLE GPUs2020-01-21T00:36:50ZFalse
MDU6SXNzdWU1NTMzMDYzODI=we need install x11-utils for tutorials/1_dqn_tutorial.ipynb2020-01-22T04:48:41Z2020-01-29T18:04:41ZTrue
MDU6SXNzdWU1NTMzMzg0MTI=Somthing wrong in tutorials/4_drivers_tutorial.ipynb2020-01-22T06:32:38Z2020-01-31T20:04:35ZTrue
MDU6SXNzdWU1NTQwMTMwMjQ=observation_and_action_constraint_splitter behavior2020-01-23T08:36:04ZFalse
MDU6SXNzdWU1NTU3ODI2Nzg=suite_gym.load() default for max_episode_steps2020-01-27T19:26:09Z2020-02-05T17:56:02ZTrue
MDU6SXNzdWU1NTc1NTE0NDA=Decrease learning rate2020-01-30T15:00:13Z2020-02-05T00:09:10ZTrue
MDU6SXNzdWU1NTgwODYwMTY=Policy's action with an action mask not respecting its min/max bounds2020-01-31T11:50:06ZFalse
MDU6SXNzdWU1NTgyMTkyMjc=Error in EncodingNetwork when specifying only 1 convolution layer2020-01-31T16:03:06Z2020-01-31T16:41:49ZTrue
MDU6SXNzdWU1NTg1MjQ2MTY=DQN loss calculation error when using Dict Action space2020-02-01T11:07:55ZFalse
MDU6SXNzdWU1NTkzMzU1MDQ=Breaking of episodic replay buffer on tf-nightly2020-02-03T21:09:39ZFalse
MDU6SXNzdWU1NTk4NDA4NDY=Broken Tutorials - DQN2020-02-04T16:40:14Z2020-02-05T22:04:44ZTrue
MDU6SXNzdWU1NjAzNTAwMDg=Epsilon Decay in DQN2020-02-05T12:47:48Z2020-03-02T17:08:28ZTrue
MDU6SXNzdWU1NjEzOTA1Nzk=utils.validate_py_environment() should support environment with action constraints2020-02-07T02:57:21ZFalse
MDU6SXNzdWU1NjE4MDI5MDk=Why ParallelPyEnvironment cant work with RandomTFPolicy?2020-02-07T18:34:10ZFalse
MDU6SXNzdWU1NjQxODIyODY=Gym Wrapper Converts MultiBinary Space to int8 Data Type Incompatible With Many TF Ops2020-02-12T18:22:34Z2020-02-12T21:07:47ZTrue
MDU6SXNzdWU1NjUzNTU2NzA=PPOAgent without value network2020-02-14T14:20:12Z2020-02-14T21:55:38ZTrue
MDU6SXNzdWU1NjYyMjAwMjU=utils.validate_py_environment() does not support environments with batched actions2020-02-17T10:57:48ZFalse
MDU6SXNzdWU1NjcxOTg3OTM=Why does the action spec for a DqnAgent require a minimum of 0?2020-02-18T22:30:58Z2020-02-18T22:34:26ZTrue
MDU6SXNzdWU1NjkyMjU3OTg=There should only be one CountingEnv2020-02-21T22:57:13Z2020-02-26T17:15:32ZTrue
MDU6SXNzdWU1NzA3MTQzNzk=C51/Rainbow tutorial not working on Colab2020-02-25T17:11:09ZFalse
MDU6SXNzdWU1NzA3NTczMDc=add *args to .reset method of PyEnvironment2020-02-25T18:42:08Z2020-03-02T15:23:47ZTrue
MDU6SXNzdWU1NzE3ODQ0Mzc=Why the DQN's training loss become bigger and bigger after longer training iterations in docs/tutorials/1_dqn_tutorial.ipynb?2020-02-27T02:46:02Z2020-02-27T16:15:06ZTrue
MDU6SXNzdWU1NzIwNTcxMjM=Different layers saved under the same name2020-02-27T12:41:07Z2020-02-27T23:48:43ZTrue
MDU6SXNzdWU1NzI0NzExODI=Time Step causing memory leak2020-02-28T01:54:45Z2020-02-28T07:52:50ZTrue
MDU6SXNzdWU1NzI2MjA4MjU=policy.action MEMORY LEAK2020-02-28T09:05:33Z2020-03-05T07:39:02ZTrue
MDU6SXNzdWU1NzM1OTE5MjM=action constraints in PPO2020-03-01T18:26:31ZFalse
MDU6SXNzdWU1NzUxMTY3MDM=TF-Agents 0.3.0 compatibility with TensorFlow 2.1.02020-03-04T03:44:24Z2020-03-23T16:45:33ZTrue
MDU6SXNzdWU1NzUxMTg0MTM=How to use agents/tf_agents/bandits/agents/mixture_agent.py? is there any example?2020-03-04T03:50:10Z2020-03-05T01:34:09ZTrue
MDU6SXNzdWU1NzYwNTMzNzY=Does DynamicStepDriver take a Reference of the tf_env?2020-03-05T07:42:20Z2020-03-05T18:35:44ZTrue
MDU6SXNzdWU1NzY5Nzg1Njc=Possible flaws in PPO implementation2020-03-06T14:47:37Z2020-04-21T23:25:05ZTrue
MDU6SXNzdWU1NzczNTgzMDU=Shape Error of DQN2020-03-07T16:31:53Z2020-03-08T08:51:43ZTrue
MDU6SXNzdWU1Nzc2ODMxMDE=Multiple activation function for ActorDistributionNetwork2020-03-09T06:16:13Z2020-03-09T15:10:00ZTrue
MDU6SXNzdWU1ODI3Njg0OTc=Network only supports action_specs with shape in [(), (1,)])   In call to configurable 'QNetwork' 2020-03-17T05:39:42ZFalse
MDU6SXNzdWU1ODM5MjY5NDg=Multiple OBservation with Actor Critic2020-03-18T18:34:44Z2020-03-19T00:12:26ZTrue
MDU6SXNzdWU1ODQ2NDYxMzg=Categorical Q Network Performing Worse than Q Network2020-03-19T19:26:18ZFalse
MDU6SXNzdWU1ODQ4MjI4NjA=FrameStack for a PyEnvironment2020-03-20T03:40:37Z2020-03-20T16:15:50ZTrue
MDU6SXNzdWU1ODU5MjgzMjE=Correct Way to Implement Frame Stacking2020-03-23T04:52:35ZFalse
MDU6SXNzdWU1ODc0Njc1Mjg=Question about Collect Driver and Parallel Environment2020-03-25T06:13:59ZFalse
MDU6SXNzdWU1ODgwODIxNzE=Parallel Py Environment Performing Considerably Worse For More Environments2020-03-26T01:02:09ZFalse
MDU6SXNzdWU1ODg0MjM2NDg=Unable to use ActorDistributionRnnNetwork with SAC agent2020-03-26T13:31:47ZFalse
MDU6SXNzdWU1ODkwNzM5MDM=Question about Custom PYEnvironment regarding two  inputs2020-03-27T11:39:21Z2020-05-02T12:20:08ZTrue
MDU6SXNzdWU1OTA4Njk5ODU=How does epsilon_greedy work for the DQN agent2020-03-31T07:37:37Z2020-04-08T15:46:04ZTrue
MDU6SXNzdWU1OTI4MDQ2MjE=Incompatibilty of Tensorflow version 1.15 with tf-agents version 0.3.02020-04-02T17:27:29Z2020-05-05T15:32:14ZTrue
MDU6SXNzdWU1OTI5NTYzOTE=ModuleNotFoundError: No module named 'tf_agents.bandits.specs'2020-04-02T22:29:55Z2020-04-29T22:35:14ZTrue
MDU6SXNzdWU1OTc1NDkwNjM=Support scalar action spaces in the normal projection network2020-04-09T20:58:28ZFalse
MDU6SXNzdWU1OTgwMjM4MDg=Custom tf metrics for game variables?2020-04-10T18:19:26Z2020-04-18T04:08:55ZTrue
MDU6SXNzdWU1OTgxNDUzMTU=How to use agents/tf_agents/behavioral cloning? is there any example?2020-04-10T23:55:36ZFalse
MDU6SXNzdWU1OTkxNTExODE=Function to Make Predictions From Policy2020-04-13T21:36:33ZFalse
MDU6SXNzdWU1OTk2NDY3MDQ=Multiagent Support2020-04-14T15:09:23ZFalse
MDU6SXNzdWU2MDA0NjE0MDE=Parallel Py Environment and Use of Policy in Environment Crashing2020-04-15T17:17:37Z2020-04-22T04:17:57ZTrue
MDU6SXNzdWU2MDIxNjY4NjM=[Feature request] Metrics documentation or tutorial to assist with hyperparameter selection/tuning2020-04-17T18:48:40ZFalse
MDU6SXNzdWU2MDI1MzE1Mjc=Linking API documentation in the README2020-04-18T19:14:07Z2020-05-05T15:31:40ZTrue
MDU6SXNzdWU2MDQ0NDEzNzE=Policies Breaking in Parallel Py Environment (Possible Memory Leak??)2020-04-22T04:18:26Z2020-04-22T18:36:56ZTrue
MDU6SXNzdWU2MDUzMDE3MDM=PPO value clipping seems to have no effect2020-04-23T07:22:16Z2020-04-29T22:00:28ZTrue
MDU6SXNzdWU2MDY3NzIzOTI=Create encoder and decoder actor networks2020-04-25T13:42:39ZFalse
MDU6SXNzdWU2MDY4MjczNjI=Speeding up Prediction Time of a Policy2020-04-25T18:23:38ZFalse
MDU6SXNzdWU2MDkwNzk1NzE=Function copy(self) not defined in class policies.py_policy.PyPolicy2020-04-29T13:57:10Z2020-05-04T21:01:08ZTrue
MDU6SXNzdWU2MDk2NjcyODE=Do the metrics in the `tf_metrics`-module support batching by design?2020-04-30T07:30:25Z2020-05-02T06:47:18ZTrue
MDU6SXNzdWU2MTAwODY0MDI=PPOAgent._train increments the train_step_counter by num_epochs rather than 12020-04-30T14:43:33Z2020-05-01T20:10:34ZTrue
MDU6SXNzdWU2MTA2OTk2NjI=Intel-tensorflow with TF-Agents not working2020-05-01T11:11:22ZFalse
MDU6SXNzdWU2MTExNzQ2NzE="metric_utils.eager_compute not evaluating ""fast"""2020-05-02T12:18:16ZFalse
MDU6SXNzdWU2MTEzMzkzNTA=1_dqn_tutorial.ipynb compute_avg_return should be in batch2020-05-03T05:45:07ZFalse
MDU6SXNzdWU2MTE0MjAzOTM=Question: Is the replay buffer a priority replay buffer?2020-05-03T14:00:02Z2020-05-06T00:49:48ZTrue
MDU6SXNzdWU2MTE0NTE5ODQ=AttributeError: 'tuple' object has no attribute 'rank'2020-05-03T16:32:24ZFalse
MDU6SXNzdWU2MTMzNzU5Njk=AttributeError: 'dict' object has no attribute 'shape'2020-05-06T14:43:02Z2020-05-14T23:25:49ZTrue
MDU6SXNzdWU2MTM5MTk2MDI=`AffineScalar` bijector is deprecated2020-05-07T09:44:55ZFalse
MDU6SXNzdWU2MTQwMTM3NzI=Where can I set a random seed in order to obtain the same actions, states and rewards?2020-05-07T12:21:01ZFalse
MDU6SXNzdWU2MTQ4NTY5MTA=TensorFlow 2.2 Support.2020-05-08T16:47:00ZFalse
MDU6SXNzdWU2MTQ5NjcwODI=AverageReturn appears incorrect in drivers tutorial 2020-05-08T20:23:48Z2020-05-08T20:30:51ZTrue
MDU6SXNzdWU2MTUwMTU1MTI=No module name 'tf_agents.typing' on latest nightly2020-05-08T22:18:00ZFalse
MDU6SXNzdWU2MTUxMzU0OTE=Unable to use ChosenActionHistogram with DynamicStepDriver2020-05-09T09:37:48ZFalse
MDU6SXNzdWU2MTUzOTQ2NDQ=tensorflow environment2020-05-10T13:43:10ZFalse
MDU6SXNzdWU2MTYwNjE2NzU="Bellman optimality equation shown as LaTeX source in ""Intro to RL"""2020-05-11T17:42:41ZFalse
MDU6SXNzdWU2MTY4ODE1NDc=Error with nested spec2020-05-12T18:52:24Z2020-05-12T19:26:33ZTrue
MDU6SXNzdWU2MTc0MDgxOTA=Better warnings for incorrect batch size in replay buffer2020-05-13T12:31:01ZFalse
MDU6SXNzdWU2MTgyNDAwNTE=Questions about input of policy.action2020-05-14T13:39:19ZFalse
