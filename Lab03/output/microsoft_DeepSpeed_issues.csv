idtitlecreatedAtclosedAtclosed
MDU6SXNzdWU1NTk1ODY0NTc=Distributed unit tests.2020-02-04T09:21:21Z2020-02-04T17:56:57ZTrue
MDU6SXNzdWU1NTk5MDk3MDU=add allreduce test2020-02-04T18:48:23Z2020-02-04T19:26:14ZTrue
MDU6SXNzdWU1NjAwMDk4MDQ=Install script assumes shared filesystem for multi-node installs2020-02-04T22:11:22Z2020-02-04T22:28:34ZTrue
MDU6SXNzdWU1NjA1Njk0MDg=Load model checkpoint without loading the optimizer states.2020-02-05T19:06:14Z2020-03-10T23:52:03ZTrue
MDU6SXNzdWU1NjA1NzQwMDM=Make optimizer field optional in JSON config2020-02-05T19:15:16Z2020-02-06T08:04:52ZTrue
MDU6SXNzdWU1NjA1OTE0Mzc=batch config issue?2020-02-05T19:49:41Z2020-02-07T21:11:08ZTrue
MDU6SXNzdWU1NjA2MzExNDQ=Installation documentation needed2020-02-05T21:11:42Z2020-02-09T06:00:25ZTrue
MDU6SXNzdWU1NjA2MzE1MjI=Megatron tutorial2020-02-05T21:12:30Z2020-02-07T01:58:56ZTrue
MDU6SXNzdWU1NjA2MzIyNTM=Azure tutorial + documentation2020-02-05T21:14:03Z2020-02-08T01:21:42ZTrue
MDU6SXNzdWU1NjA2MzI1NzM=Port core API documentation2020-02-05T21:14:43Z2020-02-08T00:37:12ZTrue
MDU6SXNzdWU1NjA2MzM0NTQ=Port DeepSpeed overview documentation2020-02-05T21:16:34Z2020-02-07T02:03:19ZTrue
MDU6SXNzdWU1NjA2MzU1NjQ=Write CIFAR10 tutorial2020-02-05T21:20:53Z2020-02-06T23:47:08ZTrue
MDU6SXNzdWU1NjA2NTkxNjg=Table of contents in README.md2020-02-05T22:10:40Z2020-02-07T02:02:46ZTrue
MDU6SXNzdWU1NjA4NjM2OTA=Config and core arguments API docstrings2020-02-06T08:51:55Z2020-02-06T21:14:23ZTrue
MDU6SXNzdWU1NjIxMTg1NDM=Install details2020-02-09T04:09:02Z2020-02-09T19:11:38ZTrue
MDU6SXNzdWU1NjI4MzcxNTk=Turing NLG2020-02-10T21:24:52Z2020-02-11T21:28:51ZTrue
MDU6SXNzdWU1NjMwNjg3OTk=DDLRUN + DeepSpeed on SUMMIT2020-02-11T09:20:54Z2020-02-28T10:34:43ZTrue
MDU6SXNzdWU1NjMyMDMxOTM=train_batch_size + dataset +  actual batch size2020-02-11T13:22:28Z2020-02-11T21:22:08ZTrue
MDU6SXNzdWU1NjMyMjI5MDQ=pytorch gradient checkpointing is much better than deepspeed !2020-02-11T13:55:44Z2020-02-12T04:50:57ZTrue
MDU6SXNzdWU1NjM2MTQ5MDk=Optimization for a Single GPU2020-02-11T23:43:55Z2020-02-12T13:51:04ZTrue
MDU6SXNzdWU1NjM3NjI1NDM=Error while initializing multiple models2020-02-12T05:26:37Z2020-02-15T01:55:46ZTrue
MDU6SXNzdWU1NjQxNzQ0OTA=Undefined name: f --> bare except2020-02-12T18:08:27Z2020-02-12T20:21:37ZTrue
MDU6SXNzdWU1NjQzMzgxNTM=Fix cifar tutorial links2020-02-12T23:17:32Z2020-02-12T23:57:59ZTrue
MDU6SXNzdWU1NjQ1NTM1ODY=Pip install support2020-02-13T09:35:25Z2020-03-23T02:04:21ZTrue
MDU6SXNzdWU1NjQ2OTA5OTE=Support old and new apex optimizer fusion2020-02-13T13:42:09Z2020-02-22T05:51:55ZTrue
MDU6SXNzdWU1NjQ5MDQ5MzI=NoneType has no attribute to2020-02-13T19:34:42Z2020-02-15T01:04:16ZTrue
MDU6SXNzdWU1NjU0OTg1MDc=Update default deepspeed config2020-02-14T18:54:03Z2020-03-03T22:38:42ZTrue
MDU6SXNzdWU1NjU0OTkwMTY=Allow DeepSpeed init to accept dictionary instead of args2020-02-14T18:55:10ZFalse
MDU6SXNzdWU1NjU2MTEwMzA=Catch spawned process failures and terminate2020-02-14T23:47:07ZFalse
MDU6SXNzdWU1NjU2Mzc1OTQ=Detect split brain issues w.r.t. user code and deepspeed batch sizes2020-02-15T01:32:55ZFalse
MDU6SXNzdWU1NjU2NDUxMzg=Detect if init distributed is needed2020-02-15T02:41:49Z2020-02-26T23:07:50ZTrue
MDU6SXNzdWU1NjU3NjE2NTA=Unable to detect local GPU Resources2020-02-15T15:19:32Z2020-02-17T18:27:05ZTrue
MDU6SXNzdWU1NjU4Nzc3MDM=When is T-NLG open source?2020-02-16T09:40:36Z2020-02-24T18:26:54ZTrue
MDU6SXNzdWU1NjU5ODU4MzA=ZeRO optimizer LAMB compatibility2020-02-16T23:00:11ZFalse
MDU6SXNzdWU1NjY5NTY1NTY=Support configuration for general devices and backends.2020-02-18T15:07:52ZFalse
MDU6SXNzdWU1Njc4MTY1OTQ=ZeRO with non-zero loss scale crashes2020-02-19T20:10:15Z2020-03-25T16:34:27ZTrue
MDU6SXNzdWU1Njc5OTYzNTc=Propagate PYTHONPATH like NCCL env variables2020-02-20T01:42:11Z2020-02-20T06:08:49ZTrue
MDU6SXNzdWU1Njg0ODk0NDY=Local Install Issue - Apex2020-02-20T18:30:04Z2020-02-20T22:18:20ZTrue
MDU6SXNzdWU1Njg0OTMwMDE=Conda Environment Install Issue2020-02-20T18:37:18ZFalse
MDU6SXNzdWU1Njg1ODc5Njg=_init_distributed does not use dist_init_required2020-02-20T21:37:28Z2020-02-24T14:33:58ZTrue
MDU6SXNzdWU1Njg2OTkzMDg=How does DeepSpeed implement multi-machine model parallelism?2020-02-21T02:36:56Z2020-02-26T18:55:50ZTrue
MDU6SXNzdWU1NjkyNDIxNjU=DeepSpeed using DistributedSampler with model parallelism2020-02-21T23:58:28ZFalse
MDU6SXNzdWU1Njk1NDczMjg=Initialization of two nn.Modules (e.g. generator and discriminator)2020-02-23T19:47:08Z2020-02-24T14:33:26ZTrue
MDU6SXNzdWU1Njk4Njc4Mzk=max_grad_norm is ignored in FP16 training2020-02-24T13:47:38ZFalse
MDU6SXNzdWU1NzExOTM4NzQ=FP32 Mode for ZeRO2020-02-26T09:35:00Z2020-03-11T06:15:04ZTrue
MDU6SXNzdWU1NzMwNTU3NDY=[Question]: Does DeepSpeed only for GPU clusters?2020-02-28T23:09:14Z2020-02-28T23:48:30ZTrue
MDU6SXNzdWU1NzMyNjU2NDA=training the  20 and 8 billion model failed on SUMMIT2020-02-29T10:00:56Z2020-02-29T18:20:10ZTrue
MDU6SXNzdWU1NzM1ODc1Njg=Following CIFAR Tutorial but Code Forcing RANK variable2020-03-01T17:55:56Z2020-03-05T01:16:49ZTrue
MDU6SXNzdWU1NzUxNTg1Nzg=TypeError: FP16_DeepSpeedZeroOptimizer is not an Optimizer2020-03-04T06:02:14Z2020-03-10T21:03:24ZTrue
MDU6SXNzdWU1NzU5NDc2NTc=README : Missing list of supported Schedulers2020-03-05T02:21:02Z2020-03-10T13:37:02ZTrue
MDU6SXNzdWU1NzcxNzg4MTg=deepspeed_light.py bug: 'global_step' should be 'global_steps' in _load_checkpoint()2020-03-06T21:14:33Z2020-05-06T17:09:20ZTrue
MDU6SXNzdWU1NzcxODEzNTU=Can you support automatic mixed precision (amp) in zero optimizer?2020-03-06T21:20:17ZFalse
MDU6SXNzdWU1NzcxODY5ODc=Using zero optimizer with torch.optim.lr_scheduler will hit error: Zero is not an Optimizer2020-03-06T21:32:55Z2020-04-13T18:49:01ZTrue
MDU6SXNzdWU1Nzg4MjMxNTA=lr_scheduler unit tests2020-03-10T19:53:10ZFalse
MDU6SXNzdWU1Nzg5NDE5NTU=Zero Optimizer crashes if a param_group has fewer number of elements than the data parallel size2020-03-11T00:48:37Z2020-04-24T16:24:16ZTrue
MDU6SXNzdWU1Nzg5NzExOTE=DeepSpeed install in Mac2020-03-11T02:38:53Z2020-03-12T05:48:30ZTrue
MDU6SXNzdWU1Nzk0NTYzNDI=What is the effective batch size?2020-03-11T18:17:02Z2020-03-13T10:30:17ZTrue
MDU6SXNzdWU1ODAxNTkzMzE=Example one-liner input for GPT2?2020-03-12T19:05:53Z2020-04-05T10:04:28ZTrue
MDU6SXNzdWU1ODA3MjgwNjk=Getting started guide is missing critical text about add_config_args2020-03-13T16:57:58ZFalse
MDU6SXNzdWU1ODA4MDIwMjE=zero optimizer static loss scale is not working2020-03-13T19:39:20Z2020-03-25T16:34:28ZTrue
MDU6SXNzdWU1ODE3NzgzNTA=Loss didn't go down when using zero optimizer with BERT pretrain2020-03-15T18:40:34Z2020-03-15T21:11:05ZTrue
MDU6SXNzdWU1ODM1NDcwMjA=DeepSpeed assumes model returns just one variable: loss2020-03-18T08:13:43Z2020-03-27T20:57:30ZTrue
MDU6SXNzdWU1ODQwMTMxODY=Which batch size to use with DataLoader2020-03-18T21:26:14Z2020-03-22T23:57:41ZTrue
MDU6SXNzdWU1ODQwNTc3NDg=ZeRO & Custom Optmizer (RangerLars)2020-03-18T23:15:21Z2020-03-23T08:32:35ZTrue
MDU6SXNzdWU1ODQyMzQ5MzI=DeepSpeed GPU 0 parameter server OOM with 8xV1002020-03-19T08:09:54Z2020-04-08T10:52:46ZTrue
MDU6SXNzdWU1ODQyODExMDA=GPU experiments - e.g. GTX 2080Ti2020-03-19T09:37:37ZFalse
MDU6SXNzdWU1ODQ2MDA3Mjk=bug about Zero-optimizer2020-03-19T18:06:39Z2020-03-19T23:11:08ZTrue
MDU6SXNzdWU1ODQ2MTA0Nzc=Handle None in loss scaling2020-03-19T18:23:51Z2020-03-27T20:57:30ZTrue
MDU6SXNzdWU1ODQ2MTA4NTk=Export PYTHON prefixed environment variables2020-03-19T18:24:32Z2020-03-23T01:50:20ZTrue
MDU6SXNzdWU1ODQ5MDU0MjQ=Make FP16_DeepSpeedZeroOptimizer a sub-class of torch.optim.Optimizer?2020-03-20T08:08:51Z2020-03-31T17:50:30ZTrue
MDU6SXNzdWU1ODU3Mzc5OTg=DeepSpeed FusedLamb vs Apex FusedLamb2020-03-22T14:58:48Z2020-03-23T00:24:28ZTrue
MDU6SXNzdWU1ODY4MTYxNDY=DeepSpeedDataLoader's len is not caculated properly2020-03-24T09:25:33Z2020-04-13T18:50:51ZTrue
MDU6SXNzdWU1ODY4ODUyMzE=some questions about the source code2020-03-24T11:20:15Z2020-04-03T20:49:09ZTrue
MDU6SXNzdWU1ODgzMDA4NDg=Does deepspeed support torch 1.3.1 version ?2020-03-26T10:09:39Z2020-03-26T13:18:28ZTrue
MDU6SXNzdWU1ODg3MzA2NDA=Need help running DeepSpeed distributed on-premises with containers2020-03-26T21:08:50Z2020-04-28T18:27:25ZTrue
MDU6SXNzdWU1ODkzMzg1OTA=DeepSpeed Examples Should Use DeepSpeed Launcher2020-03-27T18:55:55Z2020-03-27T20:22:25ZTrue
MDU6SXNzdWU1ODk0Njg2MDI=Ensure DeepSpeedDataLoader matches torch DataLoader semantics2020-03-28T00:03:58ZFalse
MDU6SXNzdWU1OTEwNTUwNzI=Megatron-LM pretrain_bert with deepspeed2020-03-31T12:10:30Z2020-03-31T12:23:53ZTrue
MDU6SXNzdWU1OTIwNDk3NTg=Add representation method to optimizer like pytorch optimizer2020-04-01T16:34:21Z2020-04-03T15:57:19ZTrue
MDU6SXNzdWU1OTM0NDI5OTY=Converge issue when using DeepSpeed FusedLamb2020-04-03T14:58:01Z2020-04-06T05:16:29ZTrue
MDU6SXNzdWU1OTM2MzA2MTc=Mixed Precision Training Support2020-04-03T20:44:30ZFalse
MDU6SXNzdWU1OTc0NjMzMTI=A summary of primitives provided by DeepSpeed to reduce memory?2020-04-09T18:13:11Z2020-04-20T17:54:41ZTrue
MDU6SXNzdWU1OTgxNzc0MDM=multiple deepspeed runs in a single machine2020-04-11T03:11:55Z2020-04-20T14:29:30ZTrue
MDU6SXNzdWU2MDE3ODA3Mjk="1.5B gpt2 training with zero enabled got ""CUDA out of memory"" on Nividia V100 (32GB)"2020-04-17T08:00:16Z2020-04-17T19:23:49ZTrue
MDU6SXNzdWU2MDI1Nzc1ODU=ZeRO Optimizer: Redundant data copies2020-04-18T23:47:22Z2020-04-27T16:41:56ZTrue
MDU6SXNzdWU2MDI1ODg0OTI=detect redundant tensors in param group2020-04-19T00:57:18ZFalse
MDU6SXNzdWU2MDMwNzU4NjU=How to use other optimizers not in deepspeed？2020-04-20T09:14:02Z2020-04-20T14:28:41ZTrue
MDU6SXNzdWU2MDM0OTg0OTg=setup.py fails2020-04-20T20:01:57Z2020-04-20T22:21:31ZTrue
MDU6SXNzdWU2MDQ1OTI4NDQ=Megatron-LM BERT2020-04-22T09:13:48Z2020-04-29T05:11:31ZTrue
MDU6SXNzdWU2MTE1NDQwMDM=SLURM support2020-05-04T00:29:48Z2020-05-12T16:50:26ZTrue
MDU6SXNzdWU2MTMzNDM0NDQ=pytorch tpu xla2020-05-06T14:00:20Z2020-05-06T14:27:26ZTrue
MDU6SXNzdWU2MTM0NzY0ODU=Support dynamic loss scaling parameters in FP16 optimizers2020-05-06T17:08:34Z2020-05-11T19:36:07ZTrue
