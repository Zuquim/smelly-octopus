idtitlecreatedAtclosedAtclosed
MDU6SXNzdWU1NDU5MzczOTg=SQuAD vocab file for ALBERT2019-10-24T15:04:15Z2020-01-07T22:50:18ZTrue
MDU6SXNzdWU1NDU5MzczNjc=Index Out of Range Error in tokenization using TF Hub for Pretrained Albert Models2019-10-24T16:14:28Z2020-01-08T00:59:53ZTrue
MDU6SXNzdWU1NDU5MzczMzE=[ALBERT] Pretraining2019-10-24T16:42:23Z2020-01-08T01:02:53ZTrue
MDU6SXNzdWU1NDU5MzcyODU=[ALBERT] Checkpoints for pretrained models2019-10-27T13:33:05Z2020-01-25T18:06:28ZTrue
MDU6SXNzdWU1NDU5MzcyNTc=[ALBERT] --random_next_sentence option in create_pretraining_data.py2019-10-27T14:32:41ZFalse
MDU6SXNzdWU1NDU5MzcyMzI=[ALBERT] Tokenization crashes while trying to finetune classifier with TF Hub model2019-10-28T08:44:51ZFalse
MDU6SXNzdWU1NDU5MzcxOTk=[ALBERT]: LookupError: gradient registry has no entry for: AddV2 2019-10-29T20:35:19Z2020-01-08T23:07:53ZTrue
MDU6SXNzdWU1NDU5MzcxNjk=[ALBERT]Has anyone reproduced ALBERT a scores on GLUE dataset?2019-10-30T00:43:59Z2020-01-21T10:06:09ZTrue
MDU6SXNzdWU1NDU5MzcxNDY=[ALBERT]: In run_squad_sp, convert_examples_to_features gives error in case sentence piece model is not provided.2019-10-31T09:40:58ZFalse
MDU6SXNzdWU1NDU5MzcxMTM=[ALBERT] Fine-tune SQUAD, got error in `convert_examples_to_features()`2019-11-01T13:39:32ZFalse
MDU6SXNzdWU1NDU5MzcwODY="[ALBERT]error ""expected str instance, bytes found"" to error ""list index out of range"""2019-11-03T17:13:40Z2020-01-08T23:00:36ZTrue
MDU6SXNzdWU1NDU5MzcwNTk=Any plans on multilangual albert models release?2019-11-03T23:31:41Z2020-01-08T21:51:32ZTrue
MDU6SXNzdWU1NDU5MzY5MzA=LookupError: No gradient defined for operation 'module_apply_tokens/bert/encoder/transformer/group_0_11/layer_11/inner_group_0/ffn_1/intermediate/output/dense/einsum/Ein$um' (op type: Einsum)2019-11-04T02:09:23Z2020-01-08T21:57:23ZTrue
MDU6SXNzdWU1NDU5MzY4Nzk=How to generate vocab.txt file?2019-11-04T06:40:00Z2020-01-08T22:58:24ZTrue
MDU6SXNzdWU1NDU5MzY4MzQ=KeyError: '[CLS]' in tokenization.py2019-11-04T20:49:03Z2020-01-25T18:32:04ZTrue
MDU6SXNzdWU1NDU5MDU4NDc=Multilingual ALBERT 2019-11-05T16:58:14Z2020-01-08T21:38:41ZTrue
MDU6SXNzdWU1NDU5MDU4MTU=[ALBERT] Failed to load TF-Hub model on Google Colab2019-11-08T14:14:47Z2020-01-08T22:57:15ZTrue
MDU6SXNzdWU1NDU5MDU3ODg=[ALBERT] albert-xlarge V2 seems to have a different behavior than the other models2019-11-08T16:36:11ZFalse
MDU6SXNzdWU1NDU5MDU3NDc=[ALBERT] FullTokenizer inconsistency - do_lower_case ignored when spm_model_file is specified2019-11-08T20:46:13ZFalse
MDU6SXNzdWU1NDU5MDU3MTY=Finetune ALBERT using pretained model2019-11-09T14:23:22Z2020-01-08T22:40:53ZTrue
MDU6SXNzdWU1NDU5MDU2NzM=Fine-tuning Albert large2019-11-10T19:32:13ZFalse
MDU6SXNzdWU1NDU5MDU2NDc=[ALBERT] question : unnecessary(or code error) in tokenization.py?2019-11-13T09:46:53ZFalse
MDU6SXNzdWU1NDU5MDU2MTg=[ALBERT] How many value of 'dupe_factor'  is good?2019-11-14T04:44:07Z2020-02-07T09:55:14ZTrue
MDU6SXNzdWU1NDU5MDA0MTg=[ALBERT] How to know memory consumption on GPU/TPU of a model?2019-11-15T14:25:05ZFalse
MDU6SXNzdWU1NDU5MDAzNjc=tensorflow.python.framework.errors_impl.NotFoundError: Graph ops missing from the python registry ({'Einsum'}) are also absent from the c++ registry2019-11-15T21:56:53Z2020-01-08T22:39:29ZTrue
MDU6SXNzdWU1NDU5MDAzMDk=[UNK] token in v2 models2019-11-16T13:57:34ZFalse
MDU6SXNzdWU1NDU5MDAxOTk=Incorrect English alphabet in line 402 in create_pretraining_data.py2019-11-16T17:12:49ZFalse
MDU6SXNzdWU1NDU4OTk0NjQ=[ALBERT] Pre-training on TPU Pod2019-11-19T03:27:46ZFalse
MDU6SXNzdWU1NDU4OTkzNzI=[ALBERT] TFHub assets/albert_config.json has 0 for dropouts2019-11-19T15:44:18ZFalse
MDU6SXNzdWU1NDU4OTkyNjI=Training time2019-11-21T14:41:33ZFalse
MDU6SXNzdWU1NDU4OTkxOTk=[ALBERT] Same amount of TPU memory consumption compared to BERT2019-11-25T11:31:14ZFalse
MDU6SXNzdWU1NDU4OTkxMDQ=[ALBERT] what are the parameters setting for training data generation ?2019-11-25T17:09:24ZFalse
MDU6SXNzdWU1NDU4OTkwMzY=[ALBERT] : Gradient for bert/embeddings/LayerNorm/gamma:0 is NaN : Tensor had NaN values          [[node CheckNumerics_4 (defined at usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]  2019-11-26T08:30:42ZFalse
MDU6SXNzdWU1NDU4OTg5NjI=How to train and test the ALBERT model.2019-11-28T12:03:57Z2020-01-08T22:31:45ZTrue
MDU6SXNzdWU1NDU4OTg3NTY=Model for SQuAD Question Answering2019-11-30T05:01:56Z2020-01-08T22:29:44ZTrue
MDU6SXNzdWU1NDU4OTg2Njc=[ALBERT] albert-xxlarge v1 `albert_config.josn/30k-clean.model` file2019-11-30T14:56:54Z2020-01-25T18:08:41ZTrue
MDU6SXNzdWU1NDU4OTcyNjE=Issues with codes in Google Colab-Tensorflow-2.0.02019-12-01T05:47:54Z2020-01-08T22:27:24ZTrue
MDU6SXNzdWU1MzQ1MTc3NzQ=in run_squad_sp.py what is the file format for train_feature_file?2019-12-08T09:53:55Z2019-12-12T06:04:26ZTrue
MDU6SXNzdWU1MzQ2MDM0OTk=layer norm after embedding layer2019-12-08T21:20:28Z2020-02-06T19:30:52ZTrue
MDU6SXNzdWU1MzQ4MTEwOTA=Multilingual Albert2019-12-09T09:51:23ZFalse
MDU6SXNzdWU1MzQ4NDQzMzg=squad error2019-12-09T10:46:45Z2019-12-19T10:23:52ZTrue
MDU6SXNzdWU1MzY1Mjk3MDA=Training from scratch on TPU2019-12-11T18:01:07ZFalse
MDU6SXNzdWU1MzY1NjcwMDc=Failed precondition: Error while reading resource variable2019-12-11T19:23:30ZFalse
MDU6SXNzdWU1MzY4NDk2OTQ=Is it possible to reload a pretrained tfhub albert in tf2.0?2019-12-12T09:16:40Z2020-01-08T21:49:33ZTrue
MDU6SXNzdWU1Mzg1Mjg2MDE=How to continue train ALBERT from the modelreleased on the tfhub ?2019-12-16T16:46:41Z2020-01-08T22:24:35ZTrue
MDU6SXNzdWU1Mzg2Mzc2MjM=Significantly lower than expected eval accuracy on MNLI2019-12-16T20:31:37Z2019-12-26T22:59:47ZTrue
MDU6SXNzdWU1Mzg4MTI1MDY=Will you release Chinese albert models ?2019-12-17T03:35:27Z2020-01-08T21:51:14ZTrue
MDU6SXNzdWU1Mzg4NjI2OTQ=export_dir in run_pretraining.py is necessary?2019-12-17T06:20:17Z2019-12-25T00:24:06ZTrue
MDU6SXNzdWU1Mzg4NzUyODY=where to download non-tfhub pretrained models?2019-12-17T06:56:36Z2020-01-08T21:52:05ZTrue
MDU6SXNzdWU1Mzk1MTAyOTg="""no dropout"" on v2 models"2019-12-18T07:39:16Z2019-12-26T19:19:20ZTrue
MDU6SXNzdWU1Mzk5NDYyMDc=Empty predictions in SQUAD2.0 fine tuning2019-12-18T21:56:26Z2019-12-26T19:18:29ZTrue
MDU6SXNzdWU1Mzk5NjU0MzY=How to use hub.KerasLayer() with pretrained ALBERT?2019-12-18T22:42:42Z2020-01-08T21:53:34ZTrue
MDU6SXNzdWU1NDAwMjQ5NTQ=No decreasing loss when pre-train for xxlarge2019-12-19T01:49:19Z2020-02-06T19:39:12ZTrue
MDU6SXNzdWU1NDAxMTQzMjE=The public score in readme comes from dev set or test set?2019-12-19T06:47:14Z2019-12-20T04:29:33ZTrue
MDU6SXNzdWU1NDAxMjM5NjY=LookupError: No gradient defined for operation 'module_apply_tokens/bert/encoder/transformer/group_0_11/layer_11/inner_group_0/ffn_1/intermediate/output/dense/einsum/Ein$um' (op type: Einsum)2019-12-19T07:10:46Z2020-01-08T21:54:36ZTrue
MDU6SXNzdWU1NDA3MjY4MjU=what is 他和2019-12-20T03:23:52Z2019-12-20T03:24:25ZTrue
MDU6SXNzdWU1NDEzMjU1ODc=Could not find 'albert_config.json' in non-tfhub pretrained models2019-12-21T13:12:05Z2019-12-26T19:17:55ZTrue
MDU6SXNzdWU1NDIwMDA4Mjk=Necessary readings to understand concepts used In ALBERT2019-12-24T05:49:10Z2020-01-25T17:27:52ZTrue
MDU6SXNzdWU1NDI3ODMxMjc=learning rate in optimization.py is so confusing2019-12-27T07:43:01Z2020-01-25T17:31:47ZTrue
MDU6SXNzdWU1NDMyNjgwNjg="Where can I find the ""spm_model_file"" when run run_squad_v2.py"2019-12-28T18:50:50Z2020-02-06T19:48:13ZTrue
MDU6SXNzdWU1NDM3NjM4MDI=max_predictions_per_seq=20 for seq_len=5122019-12-30T06:32:02Z2020-01-06T15:54:56ZTrue
MDU6SXNzdWU1NDQwNjg0ODM=It seems that ALBERT didn't use sentencepiece in Chinese, but BERT's tokenizer?2019-12-31T02:28:39ZFalse
MDU6SXNzdWU1NDQxMTcwMzk=do u have some contrastive datas between chinese Albert with other chinese pretraining models?2019-12-31T07:42:34ZFalse
MDU6SXNzdWU1NDQyNTc3MDY=Finetune on RACE2019-12-31T19:41:58Z2020-01-04T03:23:37ZTrue
MDU6SXNzdWU1NDQzNTgzMzA=impossible to reproduce glue result for mnli-m. did i do something wrong?2020-01-01T13:38:36Z2020-03-27T23:46:07ZTrue
MDU6SXNzdWU1NDQ0NDk1MzM=SentencePiece Models for Chinese Models Missing?2020-01-02T05:17:30ZFalse
MDU6SXNzdWU1NDQ3Mjc4Nzc=`run_classifier_with_tfhub.py` is missing.2020-01-02T20:11:08Z2020-01-06T17:23:49ZTrue
MDU6SXNzdWU1NDQ4NTkyOTk=Missing some weights parameters of the released albert_base_v22020-01-03T05:46:01Z2020-01-08T22:20:44ZTrue
MDU6SXNzdWU1NDUxMjE0MjQ=tokenization encode_pieces error2020-01-03T19:11:56ZFalse
MDU6SXNzdWU1NDUyNDU0NTY=_best_trial_info2020-01-04T05:47:49Z2020-01-06T00:24:57ZTrue
MDU6SXNzdWU1NDUzNzk5OTE=race score using run_race.py with albert base v2 is so much higher than score in table(66.8)2020-01-05T06:44:22ZFalse
MDU6SXNzdWU1NDU1NDIwODg=Multi GPU support2020-01-06T05:09:14Z2020-01-25T18:00:20ZTrue
MDU6SXNzdWU1NDY5MTI3NTQ=Unknown signature default2020-01-08T14:54:17Z2020-01-08T21:58:18ZTrue
MDU6SXNzdWU1NDY5NTUyOTg=[ALBERT] How to deal with `Model diverged with loss = NaN` when training from scratch?2020-01-08T16:06:04ZFalse
MDU6SXNzdWU1NDcwNDExODc=KerasLayer of albert from tensorflow hub has no trainable parameters in tensorflow 22020-01-08T18:52:09Z2020-01-25T19:06:32ZTrue
MDU6SXNzdWU1NDcwOTQyODk=Problems tokenizing Norwegian/Danish characters2020-01-08T20:49:23Z2020-01-09T12:34:59ZTrue
MDU6SXNzdWU1NDc1MDYzOTc=When fine-tuning ALBERT on SQUAD 1.1 - TypeError: Expected binary or unicode string, got None2020-01-09T14:18:57ZFalse
MDU6SXNzdWU1NDc4NzgxODg=[ALBERT]Error during fine-turning.2020-01-10T05:44:31Z2020-03-27T23:48:24ZTrue
MDU6SXNzdWU1NDc4ODUxNzM=[ALBERT] hub module on tpu2020-01-10T06:09:03Z2020-02-06T19:27:02ZTrue
MDU6SXNzdWU1NDc5NTAyNjU=Resource exhausted: OOM when allocating tensor with shape[32,512,3072] and type float on2020-01-10T09:02:00Z2020-01-15T03:48:52ZTrue
MDU6SXNzdWU1NDgxNTMzODc=Bad eval results on RTE and CoLA2020-01-10T15:50:35Z2020-01-10T16:33:45ZTrue
MDU6SXNzdWU1NDgzNDIwNjc=Adding new custom tokens in vocab during the fine-tuning pre-trained model2020-01-10T23:42:37Z2020-02-06T19:20:40ZTrue
MDU6SXNzdWU1NDgzNTY1NjM=Hyper-params settings for MNLI fine-tuning using Albert-v22020-01-11T00:56:54ZFalse
MDU6SXNzdWU1NDg3MTcxNjE=tensorflow.python.framework.errors_impl.FailedPreconditionError2020-01-13T05:43:05ZFalse
MDU6SXNzdWU1NDg3MjMzOTA=How can i run a pretrained model for prediction on new data?2020-01-13T06:05:00ZFalse
MDU6SXNzdWU1NDg3Njk0NDY=ALBERT vocabulary and BERT vocabulary2020-01-13T08:12:37Z2020-02-06T19:20:00ZTrue
MDU6SXNzdWU1NDkxMTA3OTk=ALBERT serving (`tf.contrib.resampler` should be done)2020-01-13T18:39:15Z2020-01-31T05:48:47ZTrue
MDU6SXNzdWU1NDkzNzU4ODc=albert fine-tuning about init_checkpoint in run_classifier.py2020-01-14T07:09:08Z2020-01-25T18:36:26ZTrue
MDU6SXNzdWU1NTAxNTQ4OTM=pre-training ALBERT with fp16 and other optimizations2020-01-15T12:24:03ZFalse
MDU6SXNzdWU1NTA3NzEzMzU=tensorflow.python.framework.errors_impl.InvalidArgumentError2020-01-16T12:17:12ZFalse
MDU6SXNzdWU1NTEyMzE3MDE=Has anyone reproduced SQuAD 1.1 score(90.2/83.2) on albert-base V2?2020-01-17T06:37:39Z2020-01-20T02:35:51ZTrue
MDU6SXNzdWU1NTMzMzU5ODk=How do I set the random seed to reproduce the same result each time?2020-01-22T06:25:34ZFalse
MDU6SXNzdWU1NTQ2Nzc2NTI=Squad v2 does not seem to train.2020-01-24T11:15:18ZFalse
MDU6SXNzdWU1NTUwNTQ1MTM=Extract raw tensor of 'pooled_output' token2020-01-25T07:29:18Z2020-01-26T11:20:16ZTrue
MDU6SXNzdWU1NTUxNjg3ODI=Exceeding Memory2020-01-26T02:38:17ZFalse
MDU6SXNzdWU1NTY4MDQ0NTU=tensorflow_estimator version incompatibility?2020-01-29T11:34:29Z2020-02-13T07:22:58ZTrue
MDU6SXNzdWU1NTc5MDE0MTA=Need to upgrade the model published on tfhub.dev to the newer SavedModel format2020-01-31T04:13:58ZFalse
MDU6SXNzdWU1NTk3NDU0OTg=ValueError: Shape of variable bert/pooler/dense/bias:0 ((128,)) doesn't match with shape of tensor bert/pooler/dense/bias ([768]) from checkpoint reader.2020-02-04T14:12:14ZFalse
MDU6SXNzdWU1NjA0NDE2Mzc=How can I view model weights ?2020-02-05T15:20:13Z2020-02-12T16:41:19ZTrue
MDU6SXNzdWU1NjEyMzA1MzE=Provide the parameters used for the sentencepiece 2020-02-06T19:47:12Z2020-02-07T16:25:20ZTrue
MDU6SXNzdWU1NjIzMTI0ODA=a question about `num_attention_heads`2020-02-10T04:09:15Z2020-03-27T23:35:59ZTrue
MDU6SXNzdWU1NjIzNDI3MTk=A question about `hidden_act`2020-02-10T05:58:24Z2020-03-27T23:32:41ZTrue
MDU6SXNzdWU1NjI5NzU0ODE=The latest code giving error while prediction2020-02-11T04:45:25Z2020-02-17T16:25:24ZTrue
MDU6SXNzdWU1NjUwNzU3MjM=Chinese models  is V1 or v22020-02-14T03:09:41Z2020-02-26T04:15:50ZTrue
MDU6SXNzdWU1NjU4MjYzNTA=Can ALBERT run NER?2020-02-15T23:56:03ZFalse
MDU6SXNzdWU1NjYyMzE3Mjk=Be careful if you use foreign language with spm model when you create pretraining data2020-02-17T11:18:50ZFalse
MDU6SXNzdWU1NjY0MTMxNjE=install with pip?2020-02-17T16:40:42ZFalse
MDU6SXNzdWU1Njc3OTE5OTg=Anyone try chinese NER task?2020-02-19T19:27:54ZFalse
MDU6SXNzdWU1Njg3ODgwOTE=I'm going to pretrain Albert on my own data. What am I going to do?What format is the training data?2020-02-21T07:34:30Z2020-02-24T01:03:47ZTrue
MDU6SXNzdWU1Njg4MTAyMjY=When I ran the file ‘run_pretraining.py’, there was a problem2020-02-21T08:30:54Z2020-03-10T08:10:08ZTrue
MDU6SXNzdWU1NjkxOTk4NDM=Ran out of memory in memory space hbm on RACE xlarge v3 on TPU v2-82020-02-21T21:44:02ZFalse
MDU6SXNzdWU1Njk4MjYwNjI=Running run_classifier.py error2020-02-24T12:30:51ZFalse
MDU6SXNzdWU1NzEwNTgzODE=Does Albert fine tuning support parallelism？2020-02-26T04:18:00ZFalse
MDU6SXNzdWU1NzMyODgzMjE=When I run run_pretraining.py on the GPU, I get the following error, but run_pretraining_test.py is normal.2020-02-29T12:47:10Z2020-03-16T03:07:53ZTrue
MDU6SXNzdWU1NzQyODgwNzg=File system scheme '[local]' not implemented (fine tuning Albert on TPU)2020-03-02T22:23:08Z2020-03-27T23:31:10ZTrue
MDU6SXNzdWU1NzUwNTA2Njc=[Question] Is there any fine-tuned ALBERT model on GLUE tasks available?2020-03-04T00:39:21ZFalse
MDU6SXNzdWU1NzUyMTAwNDY="[Question] About ""embedding_hidden_mapping_in"" "2020-03-04T08:07:07Z2020-03-06T06:37:35ZTrue
MDU6SXNzdWU1NzYxMTI4MDI=Uneven sampling if number of tfrecords > cycle_length2020-03-05T09:32:24ZFalse
MDU6SXNzdWU1NzYzNTg4Nzg=Getting huge number of training steps2020-03-05T15:59:45ZFalse
MDU6SXNzdWU1NzcyMzY5Mjg=Pretraining iterations_per_loop and batch_size2020-03-07T00:06:10ZFalse
MDU6SXNzdWU1NzczMzIyNjI=Wrong configuration of albert xlarge2020-03-07T13:04:56ZFalse
MDU6SXNzdWU1Nzk4MjM4MDA=How to extract the word embedding2020-03-12T10:03:45ZFalse
MDU6SXNzdWU1ODc1Mzk1OTQ=TPU training problem2020-03-25T08:53:16Z2020-03-26T18:03:10ZTrue
MDU6SXNzdWU1ODc2MDY4MjE=run_pretraining.py doesn't read all input files while training on desktop, global step doesn't work like epoch logic2020-03-25T10:49:10ZFalse
MDU6SXNzdWU1OTA1MzA2MDc=GPU not used when fine-tuning on SQuAD 2.02020-03-30T19:20:16ZFalse
MDU6SXNzdWU1OTE3MDYzMTU="tokenization.py - WordpieceTokenizer uses ""##"", but Sentencepiece vocab generates ""▁"""2020-04-01T07:58:29ZFalse
MDU6SXNzdWU1OTU0NDI4MDg=albert large_v22020-04-06T21:33:10ZFalse
MDU6SXNzdWU1OTYwNTY1MjM=ALBERT-xxlarge V2 training on TPU V3-512 extremely slow2020-04-07T18:10:53ZFalse
MDU6SXNzdWU1OTYzMDkxMDQ=ModuleNotFoundError: No module named 'albert'2020-04-08T04:59:29Z2020-04-11T03:36:09ZTrue
MDU6SXNzdWU1OTY1ODcyNzc=[question] Cross-layer parameter sharing2020-04-08T13:39:35ZFalse
MDU6SXNzdWU1OTY3OTM4NTE=ValueError: Signature 'serving_default' is missing from meta graph.2020-04-08T19:05:47ZFalse
MDU6SXNzdWU1OTcxMDE1MjM=Training without SOP Loss2020-04-09T08:08:37ZFalse
MDU6SXNzdWU1OTcyNjMzNTQ=Extremely large RAM consumption by create_pretraining.py2020-04-09T12:57:37ZFalse
MDU6SXNzdWU1OTk0NTQ1OTY=get Matthew_corr = 0 in eval on CoLA 2020-04-14T10:13:09ZFalse
MDU6SXNzdWU1OTk2OTc1OTI=different parameter position with api doc2020-04-14T16:22:53ZFalse
MDU6SXNzdWU2MDE2NTI1Mjg=model isn't learning 2020-04-17T02:12:31ZFalse
MDU6SXNzdWU2MDE2NzcxNjg=Description problem of attention_mask.2020-04-17T03:33:22ZFalse
MDU6SXNzdWU2MTM0NDY2MTU=Where is the function of Factorized Embedding Parameterization?2020-05-06T16:21:06ZFalse
MDU6SXNzdWU2MTQ0ODA2MTc=fine_tune albert with run_classifier.py on GPU but the GPU-util is 0,why and how?2020-05-08T03:37:33ZFalse
